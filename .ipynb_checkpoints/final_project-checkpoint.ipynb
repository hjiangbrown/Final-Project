{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c6996b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import xgboost\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statistics\n",
    "import shap\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import plot_tree\n",
    "import graphviz\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78c9bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the csv\n",
    "df_dating=pd.read_csv('Speed Dating Data.csv',encoding = \"ISO-8859-1\")\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 500\n",
    "df_dating=df_dating.loc[:, 'iid':'amb3_s']\n",
    "df_dating=df_dating.drop(['field','career' ,'from','idg','id','partner','undergra','dec','dec_o'],axis='columns') #dropping identifiers and string columns and decision variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35a2895e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iid</th>\n",
       "      <th>gender</th>\n",
       "      <th>condtn</th>\n",
       "      <th>wave</th>\n",
       "      <th>round</th>\n",
       "      <th>position</th>\n",
       "      <th>positin1</th>\n",
       "      <th>order</th>\n",
       "      <th>pid</th>\n",
       "      <th>match</th>\n",
       "      <th>int_corr</th>\n",
       "      <th>samerace</th>\n",
       "      <th>age_o</th>\n",
       "      <th>race_o</th>\n",
       "      <th>pf_o_att</th>\n",
       "      <th>pf_o_sin</th>\n",
       "      <th>pf_o_int</th>\n",
       "      <th>pf_o_fun</th>\n",
       "      <th>pf_o_amb</th>\n",
       "      <th>pf_o_sha</th>\n",
       "      <th>attr_o</th>\n",
       "      <th>sinc_o</th>\n",
       "      <th>intel_o</th>\n",
       "      <th>fun_o</th>\n",
       "      <th>amb_o</th>\n",
       "      <th>shar_o</th>\n",
       "      <th>like_o</th>\n",
       "      <th>prob_o</th>\n",
       "      <th>met_o</th>\n",
       "      <th>age</th>\n",
       "      <th>field_cd</th>\n",
       "      <th>mn_sat</th>\n",
       "      <th>tuition</th>\n",
       "      <th>race</th>\n",
       "      <th>imprace</th>\n",
       "      <th>imprelig</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>income</th>\n",
       "      <th>goal</th>\n",
       "      <th>date</th>\n",
       "      <th>go_out</th>\n",
       "      <th>career_c</th>\n",
       "      <th>sports</th>\n",
       "      <th>tvsports</th>\n",
       "      <th>exercise</th>\n",
       "      <th>dining</th>\n",
       "      <th>museums</th>\n",
       "      <th>art</th>\n",
       "      <th>hiking</th>\n",
       "      <th>gaming</th>\n",
       "      <th>clubbing</th>\n",
       "      <th>reading</th>\n",
       "      <th>tv</th>\n",
       "      <th>theater</th>\n",
       "      <th>movies</th>\n",
       "      <th>concerts</th>\n",
       "      <th>music</th>\n",
       "      <th>shopping</th>\n",
       "      <th>yoga</th>\n",
       "      <th>exphappy</th>\n",
       "      <th>expnum</th>\n",
       "      <th>attr1_1</th>\n",
       "      <th>sinc1_1</th>\n",
       "      <th>intel1_1</th>\n",
       "      <th>fun1_1</th>\n",
       "      <th>amb1_1</th>\n",
       "      <th>shar1_1</th>\n",
       "      <th>attr4_1</th>\n",
       "      <th>sinc4_1</th>\n",
       "      <th>intel4_1</th>\n",
       "      <th>fun4_1</th>\n",
       "      <th>amb4_1</th>\n",
       "      <th>shar4_1</th>\n",
       "      <th>attr2_1</th>\n",
       "      <th>sinc2_1</th>\n",
       "      <th>intel2_1</th>\n",
       "      <th>fun2_1</th>\n",
       "      <th>amb2_1</th>\n",
       "      <th>shar2_1</th>\n",
       "      <th>attr3_1</th>\n",
       "      <th>sinc3_1</th>\n",
       "      <th>fun3_1</th>\n",
       "      <th>intel3_1</th>\n",
       "      <th>amb3_1</th>\n",
       "      <th>attr5_1</th>\n",
       "      <th>sinc5_1</th>\n",
       "      <th>intel5_1</th>\n",
       "      <th>fun5_1</th>\n",
       "      <th>amb5_1</th>\n",
       "      <th>attr</th>\n",
       "      <th>sinc</th>\n",
       "      <th>intel</th>\n",
       "      <th>fun</th>\n",
       "      <th>amb</th>\n",
       "      <th>shar</th>\n",
       "      <th>like</th>\n",
       "      <th>prob</th>\n",
       "      <th>met</th>\n",
       "      <th>match_es</th>\n",
       "      <th>attr1_s</th>\n",
       "      <th>sinc1_s</th>\n",
       "      <th>intel1_s</th>\n",
       "      <th>fun1_s</th>\n",
       "      <th>amb1_s</th>\n",
       "      <th>shar1_s</th>\n",
       "      <th>attr3_s</th>\n",
       "      <th>sinc3_s</th>\n",
       "      <th>intel3_s</th>\n",
       "      <th>fun3_s</th>\n",
       "      <th>amb3_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>60,521</td>\n",
       "      <td>69,487.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>60,521</td>\n",
       "      <td>69,487.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>60,521</td>\n",
       "      <td>69,487.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>60,521</td>\n",
       "      <td>69,487.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>60,521</td>\n",
       "      <td>69,487.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iid  gender  condtn  wave  round  position  positin1  order   pid  match  \\\n",
       "0    1       0       1     1     10         7      -1.0      4  11.0      0   \n",
       "1    1       0       1     1     10         7      -1.0      3  12.0      0   \n",
       "2    1       0       1     1     10         7      -1.0     10  13.0      1   \n",
       "3    1       0       1     1     10         7      -1.0      5  14.0      1   \n",
       "4    1       0       1     1     10         7      -1.0      7  15.0      1   \n",
       "\n",
       "   int_corr  samerace  age_o  race_o  pf_o_att  pf_o_sin  pf_o_int  pf_o_fun  \\\n",
       "0      0.14         0   27.0     2.0      35.0      20.0      20.0      20.0   \n",
       "1      0.54         0   22.0     2.0      60.0       0.0       0.0      40.0   \n",
       "2      0.16         1   22.0     4.0      19.0      18.0      19.0      18.0   \n",
       "3      0.61         0   23.0     2.0      30.0       5.0      15.0      40.0   \n",
       "4      0.21         0   24.0     3.0      30.0      10.0      20.0      10.0   \n",
       "\n",
       "   pf_o_amb  pf_o_sha  attr_o  sinc_o  intel_o  fun_o  amb_o  shar_o  like_o  \\\n",
       "0       0.0       5.0     6.0     8.0      8.0    8.0    8.0     6.0     7.0   \n",
       "1       0.0       0.0     7.0     8.0     10.0    7.0    7.0     5.0     8.0   \n",
       "2      14.0      12.0    10.0    10.0     10.0   10.0   10.0    10.0    10.0   \n",
       "3       5.0       5.0     7.0     8.0      9.0    8.0    9.0     8.0     7.0   \n",
       "4      10.0      20.0     8.0     7.0      9.0    6.0    9.0     7.0     8.0   \n",
       "\n",
       "   prob_o  met_o   age  field_cd mn_sat tuition  race  imprace  imprelig  \\\n",
       "0     4.0    2.0  21.0       1.0     -1      -1   4.0      2.0       4.0   \n",
       "1     4.0    2.0  21.0       1.0     -1      -1   4.0      2.0       4.0   \n",
       "2    10.0    1.0  21.0       1.0     -1      -1   4.0      2.0       4.0   \n",
       "3     7.0    2.0  21.0       1.0     -1      -1   4.0      2.0       4.0   \n",
       "4     6.0    2.0  21.0       1.0     -1      -1   4.0      2.0       4.0   \n",
       "\n",
       "  zipcode     income  goal  date  go_out  career_c  sports  tvsports  \\\n",
       "0  60,521  69,487.00   2.0   7.0     1.0      -1.0     9.0       2.0   \n",
       "1  60,521  69,487.00   2.0   7.0     1.0      -1.0     9.0       2.0   \n",
       "2  60,521  69,487.00   2.0   7.0     1.0      -1.0     9.0       2.0   \n",
       "3  60,521  69,487.00   2.0   7.0     1.0      -1.0     9.0       2.0   \n",
       "4  60,521  69,487.00   2.0   7.0     1.0      -1.0     9.0       2.0   \n",
       "\n",
       "   exercise  dining  museums  art  hiking  gaming  clubbing  reading   tv  \\\n",
       "0       8.0     9.0      1.0  1.0     5.0     1.0       5.0      6.0  9.0   \n",
       "1       8.0     9.0      1.0  1.0     5.0     1.0       5.0      6.0  9.0   \n",
       "2       8.0     9.0      1.0  1.0     5.0     1.0       5.0      6.0  9.0   \n",
       "3       8.0     9.0      1.0  1.0     5.0     1.0       5.0      6.0  9.0   \n",
       "4       8.0     9.0      1.0  1.0     5.0     1.0       5.0      6.0  9.0   \n",
       "\n",
       "   theater  movies  concerts  music  shopping  yoga  exphappy  expnum  \\\n",
       "0      1.0    10.0      10.0    9.0       8.0   1.0       3.0     2.0   \n",
       "1      1.0    10.0      10.0    9.0       8.0   1.0       3.0     2.0   \n",
       "2      1.0    10.0      10.0    9.0       8.0   1.0       3.0     2.0   \n",
       "3      1.0    10.0      10.0    9.0       8.0   1.0       3.0     2.0   \n",
       "4      1.0    10.0      10.0    9.0       8.0   1.0       3.0     2.0   \n",
       "\n",
       "   attr1_1  sinc1_1  intel1_1  fun1_1  amb1_1  shar1_1  attr4_1  sinc4_1  \\\n",
       "0     15.0     20.0      20.0    15.0    15.0     15.0     -1.0     -1.0   \n",
       "1     15.0     20.0      20.0    15.0    15.0     15.0     -1.0     -1.0   \n",
       "2     15.0     20.0      20.0    15.0    15.0     15.0     -1.0     -1.0   \n",
       "3     15.0     20.0      20.0    15.0    15.0     15.0     -1.0     -1.0   \n",
       "4     15.0     20.0      20.0    15.0    15.0     15.0     -1.0     -1.0   \n",
       "\n",
       "   intel4_1  fun4_1  amb4_1  shar4_1  attr2_1  sinc2_1  intel2_1  fun2_1  \\\n",
       "0      -1.0    -1.0    -1.0     -1.0     35.0     20.0      15.0    20.0   \n",
       "1      -1.0    -1.0    -1.0     -1.0     35.0     20.0      15.0    20.0   \n",
       "2      -1.0    -1.0    -1.0     -1.0     35.0     20.0      15.0    20.0   \n",
       "3      -1.0    -1.0    -1.0     -1.0     35.0     20.0      15.0    20.0   \n",
       "4      -1.0    -1.0    -1.0     -1.0     35.0     20.0      15.0    20.0   \n",
       "\n",
       "   amb2_1  shar2_1  attr3_1  sinc3_1  fun3_1  intel3_1  amb3_1  attr5_1  \\\n",
       "0     5.0      5.0      6.0      8.0     8.0       8.0     7.0     -1.0   \n",
       "1     5.0      5.0      6.0      8.0     8.0       8.0     7.0     -1.0   \n",
       "2     5.0      5.0      6.0      8.0     8.0       8.0     7.0     -1.0   \n",
       "3     5.0      5.0      6.0      8.0     8.0       8.0     7.0     -1.0   \n",
       "4     5.0      5.0      6.0      8.0     8.0       8.0     7.0     -1.0   \n",
       "\n",
       "   sinc5_1  intel5_1  fun5_1  amb5_1  attr  sinc  intel  fun  amb  shar  like  \\\n",
       "0     -1.0      -1.0    -1.0    -1.0   6.0   9.0    7.0  7.0  6.0   5.0   7.0   \n",
       "1     -1.0      -1.0    -1.0    -1.0   7.0   8.0    7.0  8.0  5.0   6.0   7.0   \n",
       "2     -1.0      -1.0    -1.0    -1.0   5.0   8.0    9.0  8.0  5.0   7.0   7.0   \n",
       "3     -1.0      -1.0    -1.0    -1.0   7.0   6.0    8.0  7.0  6.0   8.0   7.0   \n",
       "4     -1.0      -1.0    -1.0    -1.0   5.0   6.0    7.0  7.0  6.0   6.0   6.0   \n",
       "\n",
       "   prob  met  match_es  attr1_s  sinc1_s  intel1_s  fun1_s  amb1_s  shar1_s  \\\n",
       "0   6.0  2.0       4.0     -1.0     -1.0      -1.0    -1.0    -1.0     -1.0   \n",
       "1   5.0  1.0       4.0     -1.0     -1.0      -1.0    -1.0    -1.0     -1.0   \n",
       "2  -1.0  1.0       4.0     -1.0     -1.0      -1.0    -1.0    -1.0     -1.0   \n",
       "3   6.0  2.0       4.0     -1.0     -1.0      -1.0    -1.0    -1.0     -1.0   \n",
       "4   6.0  2.0       4.0     -1.0     -1.0      -1.0    -1.0    -1.0     -1.0   \n",
       "\n",
       "   attr3_s  sinc3_s  intel3_s  fun3_s  amb3_s  \n",
       "0     -1.0     -1.0      -1.0    -1.0    -1.0  \n",
       "1     -1.0     -1.0      -1.0    -1.0    -1.0  \n",
       "2     -1.0     -1.0      -1.0    -1.0    -1.0  \n",
       "3     -1.0     -1.0      -1.0    -1.0    -1.0  \n",
       "4     -1.0     -1.0      -1.0    -1.0    -1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make all the columns integers\n",
    "df_dating=df_dating.fillna(-1)\n",
    "df_dating.head()\n",
    "df_dating['iid'].nunique()\n",
    "df_dating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6737c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to make all values ints\n",
    "df_dating['zipcode'] = df_dating['zipcode'].replace(',','', regex=True)\n",
    "df_dating['zipcode'] = df_dating['zipcode'].apply(pd.to_numeric)\n",
    "# do the same for income\n",
    "df_dating['income'] = df_dating['income'].replace(',','', regex=True)\n",
    "df_dating['income'] = df_dating['income'].apply(pd.to_numeric)\n",
    "# do the same for mn_sat\n",
    "df_dating['mn_sat'] = df_dating['mn_sat'].replace(',','', regex=True)\n",
    "df_dating['mn_sat'] = df_dating['mn_sat'].apply(pd.to_numeric)\n",
    "# do the same for tuition\n",
    "df_dating['tuition'] = df_dating['tuition'].replace(',','', regex=True)\n",
    "df_dating['tuition'] = df_dating['tuition'].apply(pd.to_numeric)\n",
    "#remove periods\n",
    "df_dating[\"pid\"] = df_dating[\"pid\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "138b3383",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_zip=df_dating[['iid','zipcode']]\n",
    "part_zip_ls=[]\n",
    "row=0\n",
    "for pid in df_dating['pid']:\n",
    "    try:\n",
    "        part_zip=df_zip.loc[df_zip['iid']== pid,'zipcode'].unique()[0]\n",
    "        part_zip_ls.append(part_zip)\n",
    "        row=row+1\n",
    "#         print('row:',row)\n",
    "#         print(part_zip)\n",
    "    except:\n",
    "        part_zip_ls.append(-1)\n",
    "df_dating['part_zip']=part_zip_ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fe6cfbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iid</th>\n",
       "      <th>gender</th>\n",
       "      <th>condtn</th>\n",
       "      <th>wave</th>\n",
       "      <th>round</th>\n",
       "      <th>position</th>\n",
       "      <th>positin1</th>\n",
       "      <th>order</th>\n",
       "      <th>pid</th>\n",
       "      <th>match</th>\n",
       "      <th>int_corr</th>\n",
       "      <th>samerace</th>\n",
       "      <th>age_o</th>\n",
       "      <th>race_o</th>\n",
       "      <th>pf_o_att</th>\n",
       "      <th>pf_o_sin</th>\n",
       "      <th>pf_o_int</th>\n",
       "      <th>pf_o_fun</th>\n",
       "      <th>pf_o_amb</th>\n",
       "      <th>pf_o_sha</th>\n",
       "      <th>attr_o</th>\n",
       "      <th>sinc_o</th>\n",
       "      <th>intel_o</th>\n",
       "      <th>fun_o</th>\n",
       "      <th>amb_o</th>\n",
       "      <th>shar_o</th>\n",
       "      <th>like_o</th>\n",
       "      <th>prob_o</th>\n",
       "      <th>met_o</th>\n",
       "      <th>age</th>\n",
       "      <th>field_cd</th>\n",
       "      <th>mn_sat</th>\n",
       "      <th>tuition</th>\n",
       "      <th>race</th>\n",
       "      <th>imprace</th>\n",
       "      <th>imprelig</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>income</th>\n",
       "      <th>goal</th>\n",
       "      <th>date</th>\n",
       "      <th>go_out</th>\n",
       "      <th>career_c</th>\n",
       "      <th>sports</th>\n",
       "      <th>tvsports</th>\n",
       "      <th>exercise</th>\n",
       "      <th>dining</th>\n",
       "      <th>museums</th>\n",
       "      <th>art</th>\n",
       "      <th>hiking</th>\n",
       "      <th>gaming</th>\n",
       "      <th>clubbing</th>\n",
       "      <th>reading</th>\n",
       "      <th>tv</th>\n",
       "      <th>theater</th>\n",
       "      <th>movies</th>\n",
       "      <th>concerts</th>\n",
       "      <th>music</th>\n",
       "      <th>shopping</th>\n",
       "      <th>yoga</th>\n",
       "      <th>exphappy</th>\n",
       "      <th>expnum</th>\n",
       "      <th>attr1_1</th>\n",
       "      <th>sinc1_1</th>\n",
       "      <th>intel1_1</th>\n",
       "      <th>fun1_1</th>\n",
       "      <th>amb1_1</th>\n",
       "      <th>shar1_1</th>\n",
       "      <th>attr4_1</th>\n",
       "      <th>sinc4_1</th>\n",
       "      <th>intel4_1</th>\n",
       "      <th>fun4_1</th>\n",
       "      <th>amb4_1</th>\n",
       "      <th>shar4_1</th>\n",
       "      <th>attr2_1</th>\n",
       "      <th>sinc2_1</th>\n",
       "      <th>intel2_1</th>\n",
       "      <th>fun2_1</th>\n",
       "      <th>amb2_1</th>\n",
       "      <th>shar2_1</th>\n",
       "      <th>attr3_1</th>\n",
       "      <th>sinc3_1</th>\n",
       "      <th>fun3_1</th>\n",
       "      <th>intel3_1</th>\n",
       "      <th>amb3_1</th>\n",
       "      <th>attr5_1</th>\n",
       "      <th>sinc5_1</th>\n",
       "      <th>intel5_1</th>\n",
       "      <th>fun5_1</th>\n",
       "      <th>amb5_1</th>\n",
       "      <th>attr</th>\n",
       "      <th>sinc</th>\n",
       "      <th>intel</th>\n",
       "      <th>fun</th>\n",
       "      <th>amb</th>\n",
       "      <th>shar</th>\n",
       "      <th>like</th>\n",
       "      <th>prob</th>\n",
       "      <th>met</th>\n",
       "      <th>match_es</th>\n",
       "      <th>attr1_s</th>\n",
       "      <th>sinc1_s</th>\n",
       "      <th>intel1_s</th>\n",
       "      <th>fun1_s</th>\n",
       "      <th>amb1_s</th>\n",
       "      <th>shar1_s</th>\n",
       "      <th>attr3_s</th>\n",
       "      <th>sinc3_s</th>\n",
       "      <th>intel3_s</th>\n",
       "      <th>fun3_s</th>\n",
       "      <th>amb3_s</th>\n",
       "      <th>part_zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>60521</td>\n",
       "      <td>69487.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>60521</td>\n",
       "      <td>69487.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>14850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>60521</td>\n",
       "      <td>69487.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>92821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>60521</td>\n",
       "      <td>69487.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>45243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>60521</td>\n",
       "      <td>69487.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>33183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iid  gender  condtn  wave  round  position  positin1  order  pid  match  \\\n",
       "0    1       0       1     1     10         7      -1.0      4   11      0   \n",
       "1    1       0       1     1     10         7      -1.0      3   12      0   \n",
       "2    1       0       1     1     10         7      -1.0     10   13      1   \n",
       "3    1       0       1     1     10         7      -1.0      5   14      1   \n",
       "4    1       0       1     1     10         7      -1.0      7   15      1   \n",
       "\n",
       "   int_corr  samerace  age_o  race_o  pf_o_att  pf_o_sin  pf_o_int  pf_o_fun  \\\n",
       "0      0.14         0   27.0     2.0      35.0      20.0      20.0      20.0   \n",
       "1      0.54         0   22.0     2.0      60.0       0.0       0.0      40.0   \n",
       "2      0.16         1   22.0     4.0      19.0      18.0      19.0      18.0   \n",
       "3      0.61         0   23.0     2.0      30.0       5.0      15.0      40.0   \n",
       "4      0.21         0   24.0     3.0      30.0      10.0      20.0      10.0   \n",
       "\n",
       "   pf_o_amb  pf_o_sha  attr_o  sinc_o  intel_o  fun_o  amb_o  shar_o  like_o  \\\n",
       "0       0.0       5.0     6.0     8.0      8.0    8.0    8.0     6.0     7.0   \n",
       "1       0.0       0.0     7.0     8.0     10.0    7.0    7.0     5.0     8.0   \n",
       "2      14.0      12.0    10.0    10.0     10.0   10.0   10.0    10.0    10.0   \n",
       "3       5.0       5.0     7.0     8.0      9.0    8.0    9.0     8.0     7.0   \n",
       "4      10.0      20.0     8.0     7.0      9.0    6.0    9.0     7.0     8.0   \n",
       "\n",
       "   prob_o  met_o   age  field_cd  mn_sat  tuition  race  imprace  imprelig  \\\n",
       "0     4.0    2.0  21.0       1.0    -1.0     -1.0   4.0      2.0       4.0   \n",
       "1     4.0    2.0  21.0       1.0    -1.0     -1.0   4.0      2.0       4.0   \n",
       "2    10.0    1.0  21.0       1.0    -1.0     -1.0   4.0      2.0       4.0   \n",
       "3     7.0    2.0  21.0       1.0    -1.0     -1.0   4.0      2.0       4.0   \n",
       "4     6.0    2.0  21.0       1.0    -1.0     -1.0   4.0      2.0       4.0   \n",
       "\n",
       "   zipcode   income  goal  date  go_out  career_c  sports  tvsports  exercise  \\\n",
       "0    60521  69487.0   2.0   7.0     1.0      -1.0     9.0       2.0       8.0   \n",
       "1    60521  69487.0   2.0   7.0     1.0      -1.0     9.0       2.0       8.0   \n",
       "2    60521  69487.0   2.0   7.0     1.0      -1.0     9.0       2.0       8.0   \n",
       "3    60521  69487.0   2.0   7.0     1.0      -1.0     9.0       2.0       8.0   \n",
       "4    60521  69487.0   2.0   7.0     1.0      -1.0     9.0       2.0       8.0   \n",
       "\n",
       "   dining  museums  art  hiking  gaming  clubbing  reading   tv  theater  \\\n",
       "0     9.0      1.0  1.0     5.0     1.0       5.0      6.0  9.0      1.0   \n",
       "1     9.0      1.0  1.0     5.0     1.0       5.0      6.0  9.0      1.0   \n",
       "2     9.0      1.0  1.0     5.0     1.0       5.0      6.0  9.0      1.0   \n",
       "3     9.0      1.0  1.0     5.0     1.0       5.0      6.0  9.0      1.0   \n",
       "4     9.0      1.0  1.0     5.0     1.0       5.0      6.0  9.0      1.0   \n",
       "\n",
       "   movies  concerts  music  shopping  yoga  exphappy  expnum  attr1_1  \\\n",
       "0    10.0      10.0    9.0       8.0   1.0       3.0     2.0     15.0   \n",
       "1    10.0      10.0    9.0       8.0   1.0       3.0     2.0     15.0   \n",
       "2    10.0      10.0    9.0       8.0   1.0       3.0     2.0     15.0   \n",
       "3    10.0      10.0    9.0       8.0   1.0       3.0     2.0     15.0   \n",
       "4    10.0      10.0    9.0       8.0   1.0       3.0     2.0     15.0   \n",
       "\n",
       "   sinc1_1  intel1_1  fun1_1  amb1_1  shar1_1  attr4_1  sinc4_1  intel4_1  \\\n",
       "0     20.0      20.0    15.0    15.0     15.0     -1.0     -1.0      -1.0   \n",
       "1     20.0      20.0    15.0    15.0     15.0     -1.0     -1.0      -1.0   \n",
       "2     20.0      20.0    15.0    15.0     15.0     -1.0     -1.0      -1.0   \n",
       "3     20.0      20.0    15.0    15.0     15.0     -1.0     -1.0      -1.0   \n",
       "4     20.0      20.0    15.0    15.0     15.0     -1.0     -1.0      -1.0   \n",
       "\n",
       "   fun4_1  amb4_1  shar4_1  attr2_1  sinc2_1  intel2_1  fun2_1  amb2_1  \\\n",
       "0    -1.0    -1.0     -1.0     35.0     20.0      15.0    20.0     5.0   \n",
       "1    -1.0    -1.0     -1.0     35.0     20.0      15.0    20.0     5.0   \n",
       "2    -1.0    -1.0     -1.0     35.0     20.0      15.0    20.0     5.0   \n",
       "3    -1.0    -1.0     -1.0     35.0     20.0      15.0    20.0     5.0   \n",
       "4    -1.0    -1.0     -1.0     35.0     20.0      15.0    20.0     5.0   \n",
       "\n",
       "   shar2_1  attr3_1  sinc3_1  fun3_1  intel3_1  amb3_1  attr5_1  sinc5_1  \\\n",
       "0      5.0      6.0      8.0     8.0       8.0     7.0     -1.0     -1.0   \n",
       "1      5.0      6.0      8.0     8.0       8.0     7.0     -1.0     -1.0   \n",
       "2      5.0      6.0      8.0     8.0       8.0     7.0     -1.0     -1.0   \n",
       "3      5.0      6.0      8.0     8.0       8.0     7.0     -1.0     -1.0   \n",
       "4      5.0      6.0      8.0     8.0       8.0     7.0     -1.0     -1.0   \n",
       "\n",
       "   intel5_1  fun5_1  amb5_1  attr  sinc  intel  fun  amb  shar  like  prob  \\\n",
       "0      -1.0    -1.0    -1.0   6.0   9.0    7.0  7.0  6.0   5.0   7.0   6.0   \n",
       "1      -1.0    -1.0    -1.0   7.0   8.0    7.0  8.0  5.0   6.0   7.0   5.0   \n",
       "2      -1.0    -1.0    -1.0   5.0   8.0    9.0  8.0  5.0   7.0   7.0  -1.0   \n",
       "3      -1.0    -1.0    -1.0   7.0   6.0    8.0  7.0  6.0   8.0   7.0   6.0   \n",
       "4      -1.0    -1.0    -1.0   5.0   6.0    7.0  7.0  6.0   6.0   6.0   6.0   \n",
       "\n",
       "   met  match_es  attr1_s  sinc1_s  intel1_s  fun1_s  amb1_s  shar1_s  \\\n",
       "0  2.0       4.0     -1.0     -1.0      -1.0    -1.0    -1.0     -1.0   \n",
       "1  1.0       4.0     -1.0     -1.0      -1.0    -1.0    -1.0     -1.0   \n",
       "2  1.0       4.0     -1.0     -1.0      -1.0    -1.0    -1.0     -1.0   \n",
       "3  2.0       4.0     -1.0     -1.0      -1.0    -1.0    -1.0     -1.0   \n",
       "4  2.0       4.0     -1.0     -1.0      -1.0    -1.0    -1.0     -1.0   \n",
       "\n",
       "   attr3_s  sinc3_s  intel3_s  fun3_s  amb3_s  part_zip  \n",
       "0     -1.0     -1.0      -1.0    -1.0    -1.0         0  \n",
       "1     -1.0     -1.0      -1.0    -1.0    -1.0     14850  \n",
       "2     -1.0     -1.0      -1.0    -1.0    -1.0     92821  \n",
       "3     -1.0     -1.0      -1.0    -1.0    -1.0     45243  \n",
       "4     -1.0     -1.0      -1.0    -1.0    -1.0     33183  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86655a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iid</th>\n",
       "      <th>gender</th>\n",
       "      <th>condtn</th>\n",
       "      <th>wave</th>\n",
       "      <th>round</th>\n",
       "      <th>position</th>\n",
       "      <th>positin1</th>\n",
       "      <th>order</th>\n",
       "      <th>pid</th>\n",
       "      <th>match</th>\n",
       "      <th>int_corr</th>\n",
       "      <th>samerace</th>\n",
       "      <th>age_o</th>\n",
       "      <th>race_o</th>\n",
       "      <th>pf_o_att</th>\n",
       "      <th>pf_o_sin</th>\n",
       "      <th>pf_o_int</th>\n",
       "      <th>pf_o_fun</th>\n",
       "      <th>pf_o_amb</th>\n",
       "      <th>pf_o_sha</th>\n",
       "      <th>attr_o</th>\n",
       "      <th>sinc_o</th>\n",
       "      <th>intel_o</th>\n",
       "      <th>fun_o</th>\n",
       "      <th>amb_o</th>\n",
       "      <th>shar_o</th>\n",
       "      <th>like_o</th>\n",
       "      <th>prob_o</th>\n",
       "      <th>met_o</th>\n",
       "      <th>age</th>\n",
       "      <th>field_cd</th>\n",
       "      <th>mn_sat</th>\n",
       "      <th>tuition</th>\n",
       "      <th>race</th>\n",
       "      <th>imprace</th>\n",
       "      <th>imprelig</th>\n",
       "      <th>income</th>\n",
       "      <th>goal</th>\n",
       "      <th>date</th>\n",
       "      <th>go_out</th>\n",
       "      <th>career_c</th>\n",
       "      <th>sports</th>\n",
       "      <th>tvsports</th>\n",
       "      <th>exercise</th>\n",
       "      <th>dining</th>\n",
       "      <th>museums</th>\n",
       "      <th>art</th>\n",
       "      <th>hiking</th>\n",
       "      <th>gaming</th>\n",
       "      <th>clubbing</th>\n",
       "      <th>reading</th>\n",
       "      <th>tv</th>\n",
       "      <th>theater</th>\n",
       "      <th>movies</th>\n",
       "      <th>concerts</th>\n",
       "      <th>music</th>\n",
       "      <th>shopping</th>\n",
       "      <th>yoga</th>\n",
       "      <th>exphappy</th>\n",
       "      <th>expnum</th>\n",
       "      <th>attr1_1</th>\n",
       "      <th>sinc1_1</th>\n",
       "      <th>intel1_1</th>\n",
       "      <th>fun1_1</th>\n",
       "      <th>amb1_1</th>\n",
       "      <th>shar1_1</th>\n",
       "      <th>attr4_1</th>\n",
       "      <th>sinc4_1</th>\n",
       "      <th>intel4_1</th>\n",
       "      <th>fun4_1</th>\n",
       "      <th>amb4_1</th>\n",
       "      <th>shar4_1</th>\n",
       "      <th>attr2_1</th>\n",
       "      <th>sinc2_1</th>\n",
       "      <th>intel2_1</th>\n",
       "      <th>fun2_1</th>\n",
       "      <th>amb2_1</th>\n",
       "      <th>shar2_1</th>\n",
       "      <th>attr3_1</th>\n",
       "      <th>sinc3_1</th>\n",
       "      <th>fun3_1</th>\n",
       "      <th>intel3_1</th>\n",
       "      <th>amb3_1</th>\n",
       "      <th>attr5_1</th>\n",
       "      <th>sinc5_1</th>\n",
       "      <th>intel5_1</th>\n",
       "      <th>fun5_1</th>\n",
       "      <th>amb5_1</th>\n",
       "      <th>attr</th>\n",
       "      <th>sinc</th>\n",
       "      <th>intel</th>\n",
       "      <th>fun</th>\n",
       "      <th>amb</th>\n",
       "      <th>shar</th>\n",
       "      <th>like</th>\n",
       "      <th>prob</th>\n",
       "      <th>met</th>\n",
       "      <th>match_es</th>\n",
       "      <th>attr1_s</th>\n",
       "      <th>sinc1_s</th>\n",
       "      <th>intel1_s</th>\n",
       "      <th>fun1_s</th>\n",
       "      <th>amb1_s</th>\n",
       "      <th>shar1_s</th>\n",
       "      <th>attr3_s</th>\n",
       "      <th>sinc3_s</th>\n",
       "      <th>intel3_s</th>\n",
       "      <th>fun3_s</th>\n",
       "      <th>amb3_s</th>\n",
       "      <th>zip_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>69487.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>69487.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>69487.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>69487.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>69487.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iid  gender  condtn  wave  round  position  positin1  order  pid  match  \\\n",
       "0    1       0       1     1     10         7      -1.0      4   11      0   \n",
       "1    1       0       1     1     10         7      -1.0      3   12      0   \n",
       "2    1       0       1     1     10         7      -1.0     10   13      1   \n",
       "3    1       0       1     1     10         7      -1.0      5   14      1   \n",
       "4    1       0       1     1     10         7      -1.0      7   15      1   \n",
       "\n",
       "   int_corr  samerace  age_o  race_o  pf_o_att  pf_o_sin  pf_o_int  pf_o_fun  \\\n",
       "0      0.14         0   27.0     2.0      35.0      20.0      20.0      20.0   \n",
       "1      0.54         0   22.0     2.0      60.0       0.0       0.0      40.0   \n",
       "2      0.16         1   22.0     4.0      19.0      18.0      19.0      18.0   \n",
       "3      0.61         0   23.0     2.0      30.0       5.0      15.0      40.0   \n",
       "4      0.21         0   24.0     3.0      30.0      10.0      20.0      10.0   \n",
       "\n",
       "   pf_o_amb  pf_o_sha  attr_o  sinc_o  intel_o  fun_o  amb_o  shar_o  like_o  \\\n",
       "0       0.0       5.0     6.0     8.0      8.0    8.0    8.0     6.0     7.0   \n",
       "1       0.0       0.0     7.0     8.0     10.0    7.0    7.0     5.0     8.0   \n",
       "2      14.0      12.0    10.0    10.0     10.0   10.0   10.0    10.0    10.0   \n",
       "3       5.0       5.0     7.0     8.0      9.0    8.0    9.0     8.0     7.0   \n",
       "4      10.0      20.0     8.0     7.0      9.0    6.0    9.0     7.0     8.0   \n",
       "\n",
       "   prob_o  met_o   age  field_cd  mn_sat  tuition  race  imprace  imprelig  \\\n",
       "0     4.0    2.0  21.0       1.0    -1.0     -1.0   4.0      2.0       4.0   \n",
       "1     4.0    2.0  21.0       1.0    -1.0     -1.0   4.0      2.0       4.0   \n",
       "2    10.0    1.0  21.0       1.0    -1.0     -1.0   4.0      2.0       4.0   \n",
       "3     7.0    2.0  21.0       1.0    -1.0     -1.0   4.0      2.0       4.0   \n",
       "4     6.0    2.0  21.0       1.0    -1.0     -1.0   4.0      2.0       4.0   \n",
       "\n",
       "    income  goal  date  go_out  career_c  sports  tvsports  exercise  dining  \\\n",
       "0  69487.0   2.0   7.0     1.0      -1.0     9.0       2.0       8.0     9.0   \n",
       "1  69487.0   2.0   7.0     1.0      -1.0     9.0       2.0       8.0     9.0   \n",
       "2  69487.0   2.0   7.0     1.0      -1.0     9.0       2.0       8.0     9.0   \n",
       "3  69487.0   2.0   7.0     1.0      -1.0     9.0       2.0       8.0     9.0   \n",
       "4  69487.0   2.0   7.0     1.0      -1.0     9.0       2.0       8.0     9.0   \n",
       "\n",
       "   museums  art  hiking  gaming  clubbing  reading   tv  theater  movies  \\\n",
       "0      1.0  1.0     5.0     1.0       5.0      6.0  9.0      1.0    10.0   \n",
       "1      1.0  1.0     5.0     1.0       5.0      6.0  9.0      1.0    10.0   \n",
       "2      1.0  1.0     5.0     1.0       5.0      6.0  9.0      1.0    10.0   \n",
       "3      1.0  1.0     5.0     1.0       5.0      6.0  9.0      1.0    10.0   \n",
       "4      1.0  1.0     5.0     1.0       5.0      6.0  9.0      1.0    10.0   \n",
       "\n",
       "   concerts  music  shopping  yoga  exphappy  expnum  attr1_1  sinc1_1  \\\n",
       "0      10.0    9.0       8.0   1.0       3.0     2.0     15.0     20.0   \n",
       "1      10.0    9.0       8.0   1.0       3.0     2.0     15.0     20.0   \n",
       "2      10.0    9.0       8.0   1.0       3.0     2.0     15.0     20.0   \n",
       "3      10.0    9.0       8.0   1.0       3.0     2.0     15.0     20.0   \n",
       "4      10.0    9.0       8.0   1.0       3.0     2.0     15.0     20.0   \n",
       "\n",
       "   intel1_1  fun1_1  amb1_1  shar1_1  attr4_1  sinc4_1  intel4_1  fun4_1  \\\n",
       "0      20.0    15.0    15.0     15.0     -1.0     -1.0      -1.0    -1.0   \n",
       "1      20.0    15.0    15.0     15.0     -1.0     -1.0      -1.0    -1.0   \n",
       "2      20.0    15.0    15.0     15.0     -1.0     -1.0      -1.0    -1.0   \n",
       "3      20.0    15.0    15.0     15.0     -1.0     -1.0      -1.0    -1.0   \n",
       "4      20.0    15.0    15.0     15.0     -1.0     -1.0      -1.0    -1.0   \n",
       "\n",
       "   amb4_1  shar4_1  attr2_1  sinc2_1  intel2_1  fun2_1  amb2_1  shar2_1  \\\n",
       "0    -1.0     -1.0     35.0     20.0      15.0    20.0     5.0      5.0   \n",
       "1    -1.0     -1.0     35.0     20.0      15.0    20.0     5.0      5.0   \n",
       "2    -1.0     -1.0     35.0     20.0      15.0    20.0     5.0      5.0   \n",
       "3    -1.0     -1.0     35.0     20.0      15.0    20.0     5.0      5.0   \n",
       "4    -1.0     -1.0     35.0     20.0      15.0    20.0     5.0      5.0   \n",
       "\n",
       "   attr3_1  sinc3_1  fun3_1  intel3_1  amb3_1  attr5_1  sinc5_1  intel5_1  \\\n",
       "0      6.0      8.0     8.0       8.0     7.0     -1.0     -1.0      -1.0   \n",
       "1      6.0      8.0     8.0       8.0     7.0     -1.0     -1.0      -1.0   \n",
       "2      6.0      8.0     8.0       8.0     7.0     -1.0     -1.0      -1.0   \n",
       "3      6.0      8.0     8.0       8.0     7.0     -1.0     -1.0      -1.0   \n",
       "4      6.0      8.0     8.0       8.0     7.0     -1.0     -1.0      -1.0   \n",
       "\n",
       "   fun5_1  amb5_1  attr  sinc  intel  fun  amb  shar  like  prob  met  \\\n",
       "0    -1.0    -1.0   6.0   9.0    7.0  7.0  6.0   5.0   7.0   6.0  2.0   \n",
       "1    -1.0    -1.0   7.0   8.0    7.0  8.0  5.0   6.0   7.0   5.0  1.0   \n",
       "2    -1.0    -1.0   5.0   8.0    9.0  8.0  5.0   7.0   7.0  -1.0  1.0   \n",
       "3    -1.0    -1.0   7.0   6.0    8.0  7.0  6.0   8.0   7.0   6.0  2.0   \n",
       "4    -1.0    -1.0   5.0   6.0    7.0  7.0  6.0   6.0   6.0   6.0  2.0   \n",
       "\n",
       "   match_es  attr1_s  sinc1_s  intel1_s  fun1_s  amb1_s  shar1_s  attr3_s  \\\n",
       "0       4.0     -1.0     -1.0      -1.0    -1.0    -1.0     -1.0     -1.0   \n",
       "1       4.0     -1.0     -1.0      -1.0    -1.0    -1.0     -1.0     -1.0   \n",
       "2       4.0     -1.0     -1.0      -1.0    -1.0    -1.0     -1.0     -1.0   \n",
       "3       4.0     -1.0     -1.0      -1.0    -1.0    -1.0     -1.0     -1.0   \n",
       "4       4.0     -1.0     -1.0      -1.0    -1.0    -1.0     -1.0     -1.0   \n",
       "\n",
       "   sinc3_s  intel3_s  fun3_s  amb3_s  zip_match  \n",
       "0     -1.0      -1.0    -1.0    -1.0          0  \n",
       "1     -1.0      -1.0    -1.0    -1.0          0  \n",
       "2     -1.0      -1.0    -1.0    -1.0          0  \n",
       "3     -1.0      -1.0    -1.0    -1.0          0  \n",
       "4     -1.0      -1.0    -1.0    -1.0          0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the zip_match\n",
    "df_dating['zip_match']=(df_dating['zipcode']==df_dating['part_zip'])\n",
    "df_dating['zip_match']=df_dating['zip_match'].astype(int)\n",
    "df_dating.head()\n",
    "df_dating=df_dating.drop(['part_zip','zipcode'],axis='columns')\n",
    "df_dating.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f69ae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into X and y\n",
    "X=df_dating.loc[:,df_dating.columns != 'match']\n",
    "y=df_dating['match']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "befaa032",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scl_ftrs =X.columns\n",
    "\n",
    "num_trsfm = Pipeline(steps=[\n",
    "    ('std_scaler', StandardScaler())])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', num_trsfm, std_scl_ftrs)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5e948a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 81\u001b[0m\n\u001b[1;32m     79\u001b[0m XGB_clf \u001b[38;5;241m=\u001b[39m xgboost\u001b[38;5;241m.\u001b[39mXGBClassifier(reg_alpha\u001b[38;5;241m=\u001b[39malpha, reg_lambda\u001b[38;5;241m=\u001b[39ml, gamma\u001b[38;5;241m=\u001b[39mg, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     80\u001b[0m XGB_clf\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mParameterGrid(param_grid)[\u001b[38;5;241m0\u001b[39m]) \n\u001b[0;32m---> 81\u001b[0m \u001b[43mXGB_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m y_val_pred \u001b[38;5;241m=\u001b[39m XGB_clf\u001b[38;5;241m.\u001b[39mpredict(df_val, ntree_limit\u001b[38;5;241m=\u001b[39mXGB_clf\u001b[38;5;241m.\u001b[39mbest_ntree_limit)\n\u001b[1;32m     87\u001b[0m val_scores_ls\u001b[38;5;241m.\u001b[39mappend(f1_score(y_val, y_val_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/xgboost/sklearn.py:1516\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1488\u001b[0m (\n\u001b[1;32m   1489\u001b[0m     model,\n\u001b[1;32m   1490\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1495\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1496\u001b[0m )\n\u001b[1;32m   1497\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1498\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1499\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1513\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1514\u001b[0m )\n\u001b[0;32m-> 1516\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1519\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1531\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/xgboost/training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/xgboost/core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1918\u001b[0m     _check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1922\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Split the data using group kfold or group shuffle splits within 3 loops.\n",
    "\n",
    "\n",
    "best_mod_ls=[]\n",
    "test_scores_ls=[]\n",
    "test_sets_ls=[]\n",
    "bl_score_ls=[]\n",
    "for rand_state in range(3):\n",
    "    groups=df_dating['wave']\n",
    "    gss1 = GroupShuffleSplit(n_splits=3, train_size=.6, random_state=rand_state)\n",
    "    for train_idx, other_idx in gss1.split(X, y, groups):\n",
    "    #     print(\"TRAIN:\", train_idx, \"other:\", other_idx)\n",
    "\n",
    "\n",
    "        X_train = X.loc[train_idx]\n",
    "        X_other = X.loc[other_idx].reset_index(drop=True)\n",
    "        y_train = y.loc[train_idx]\n",
    "        y_other = y.loc[other_idx].reset_index(drop=True)\n",
    "\n",
    "        groups=X_other['wave']\n",
    "        gss2 = GroupShuffleSplit(n_splits=1, train_size=.5, random_state=rand_state)\n",
    "        for val_idx, test_idx in gss1.split(X_other, y_other, groups):\n",
    "    #         print(\"VAL:\", val_idx, \"TEST:\", test_idx)\n",
    "            X_val=X_other.loc[val_idx].reset_index(drop=True)\n",
    "            y_val=y_other.loc[val_idx].reset_index(drop=True)\n",
    "            X_test=X_other.loc[test_idx].reset_index(drop=True)\n",
    "            y_test=y_other.loc[test_idx].reset_index(drop=True)\n",
    "\n",
    "    # X_train['wave'].unique()\n",
    "    # X_val['wave'].unique()\n",
    "    # X_test['wave'].unique()\n",
    "    \n",
    "    #create a prediction for baseline where we predict all class 1.\n",
    "    #the baseline model will always predict match to maximize the true positive.\n",
    "    #the recall will be 1 because we are always preicting positive.\n",
    "    y_row=len(y_test)\n",
    "    y_dummy_pred=[1] * y_row\n",
    "    #calculate baseline F1 score\n",
    "    bl_f1=f1_score(y_dummy_pred, y_test, average='macro')\n",
    "    bl_score_ls.append(bl_f1)\n",
    "#     print(bl_f1)\n",
    "    \n",
    "    \n",
    "    #after split we are going to preprocess\n",
    "    #build x training data frame\n",
    "    X_train_tran=preprocessor.fit_transform(X_train)\n",
    "    feature_names=preprocessor.get_feature_names_out()\n",
    "    df_train=pd.DataFrame(data=X_train_tran,columns=feature_names)\n",
    "    y_val.reset_index(drop=True, inplace=True)\n",
    "    y_train.reset_index(drop=True, inplace=True) #changes series to a df\n",
    "    \n",
    "    #build x val dataframe\n",
    "    X_val_tran=preprocessor.transform(X_val)\n",
    "    feature_names=preprocessor.get_feature_names_out()\n",
    "    df_val=pd.DataFrame(data=X_val_tran,columns=feature_names)\n",
    "    y_val.reset_index(drop=True, inplace=True) #changes series to a df\n",
    "    #print(df_test.shape,y_test.shape)\n",
    "    \n",
    "    \n",
    "    #build x test dataframe\n",
    "    X_test_tran=preprocessor.transform(X_test)\n",
    "    feature_names=preprocessor.get_feature_names_out()\n",
    "    df_test=pd.DataFrame(data=X_test_tran, columns=feature_names)\n",
    "    y_test.reset_index(drop=True, inplace=True) #changes series to a df \n",
    "    models_ls=[]\n",
    "    val_scores_ls=[]\n",
    "\n",
    "    \n",
    "    for l in np.logspace(-3,3,5):\n",
    "        for alpha in np.logspace(-3,3,5):\n",
    "            for g in np.linspace(0.1,0.4,5):\n",
    "                param_grid = {\"learning_rate\": [0.03],\n",
    "                          \"n_estimators\": [10000],\n",
    "                          \"random_state\": [rand_state],\n",
    "                          \"missing\": [np.nan], \n",
    "                          \"colsample_bytree\": [0.9],              \n",
    "                          \"subsample\": [0.75], \n",
    "                          \"use_label_encoder\": [False]}\n",
    "                XGB_clf = xgboost.XGBClassifier(reg_alpha=alpha, reg_lambda=l, gamma=g, n_jobs=-1)\n",
    "                XGB_clf.set_params(**ParameterGrid(param_grid)[0]) \n",
    "                XGB_clf.fit(df_train, y_train, \n",
    "                        early_stopping_rounds=50, \n",
    "                        eval_set=[(df_val, y_val)], \n",
    "                        eval_metric='logloss',\n",
    "                        verbose=False)\n",
    "                y_val_pred = XGB_clf.predict(df_val, ntree_limit=XGB_clf.best_ntree_limit)\n",
    "                val_scores_ls.append(f1_score(y_val, y_val_pred, average='macro'))\n",
    "                models_ls.append(XGB_clf)\n",
    "    #time to find the best model with the best val score\n",
    "    best_val_score=np.argmax(val_scores_ls)\n",
    "    best_model=models_ls[best_val_score]\n",
    "    best_mod_ls.append(best_model)\n",
    "    \n",
    "    ## Calculate test score\n",
    "    y_test_pred = best_model.predict(df_test)\n",
    "    test_score = f1_score(y_test, y_test_pred, average='macro')\n",
    "    test_scores_ls.append(test_score)  # Save test score\n",
    "    \n",
    "    test_set = X_test.reset_index(drop=True).copy()\n",
    "    test_set['match'] = y_test\n",
    "    test_set['pred_match'] = y_test_pred\n",
    "    test_sets_ls.append(test_set)  # Save test set WITH predictions from best model of random state\n",
    "    y_test=y_test.array\n",
    "    print(y_test_pred,y_test)\n",
    "    df_xgb_results=pd.DataFrame({\"y_test_pred\":y_test_pred,\"y_test_true\":y_test})\n",
    "\n",
    "    #making a df with prediction results so we can look at the incorrect rows in shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba08382",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "XGB_model_mean=statistics.mean(test_scores_ls)\n",
    "XGB_model_std=np.std(test_scores_ls)\n",
    "XGB_avg_bl=statistics.mean(bl_score_ls)\n",
    "\n",
    "\n",
    "print(f'the overall mean is: {XGB_model_mean} and the standard deviation is: {XGB_model_std}')\n",
    "print('the baseline score is:',XGB_avg_bl)\n",
    "# for model in best_mod_ls:\n",
    "#     print(model)\n",
    "#     plot_tree(model)\n",
    "#     plt.show()\n",
    "#this is the tree graph that failed\n",
    "    \n",
    "    \n",
    "#get the best model\n",
    "best_xb_idx=np.argmax(test_scores_ls)\n",
    "best_xgb_model=best_mod_ls[best_xb_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44939229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost feature importance\n",
    "# library to sort dictionary\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "# calculate feature importance measurements for all five metrics\n",
    "metrics = ['weight','gain','cover','total_gain','total_cover']\n",
    "results = []\n",
    "# save importance metrics into master list\n",
    "for metric in metrics:\n",
    "    results.append(best_xgb_model.get_booster().get_score(fmap=\"\",importance_type=metric))\n",
    "\n",
    "# plot correct data for each metric\n",
    "for i, metric in enumerate(results):\n",
    "    # grab top 10 feature for metric\n",
    "    c = Counter(metric)\n",
    "    top10 = c.most_common(10)\n",
    "    # create and reverse dataframe of info\n",
    "    mdf = pd.DataFrame(top10).iloc[::-1]\n",
    "    # create and label plots\n",
    "    plt.barh(mdf[0],mdf[1])\n",
    "    # rotate axis for last graph\n",
    "    if i == 4:\n",
    "            plt.xticks(rotation=80)\n",
    "    plt.title('Feature Importance using the {} Metric'.format(metrics[i]),weight='bold')\n",
    "    plt.ylabel('Feature',weight='bold')\n",
    "    plt.xlabel('{} Score'.format(metrics[i]),weight='bold')\n",
    "    plt.show()\n",
    "        # weight = how many times a feature is used to split\n",
    "        # gain = average amount of predictive gain from a feature\n",
    "        # cover = average amount of points that pass through node per tree\n",
    "        # total_gain = total gain across all trees\n",
    "        # total_cover = total number of points that pass through feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18038f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap global feature importance graphs\n",
    "explainer = shap.TreeExplainer(best_xgb_model)\n",
    "shap_values = explainer.shap_values(df_test)\n",
    "shap.summary_plot(shap_values,df_test, plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11d1acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, df_test,max_display=10,color_bar=True,show=False)\n",
    "plt.gcf().axes[-1].set_box_aspect(100)\n",
    "plt.title('Global SHAP Values for Top 10 Features',weight='bold')\n",
    "plt.ylabel('Features',weight='bold')\n",
    "plt.xlabel('SHAP Value (impact on model output)',weight='bold')\n",
    "plt.savefig('xgb_global_ftrs.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05aba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost local importance\n",
    "#create function to return a shap diagram for any index\n",
    "import shap\n",
    "shap.initjs()\n",
    "def shap_by_ind(index):  \n",
    "    explainerModel = shap.TreeExplainer(best_xgb_model)\n",
    "    shap_values_Model = explainerModel.shap_values(df_test)\n",
    "    p = shap.force_plot(explainerModel.expected_value, shap_values_Model[index], df_train.iloc[[index]])\n",
    "    return(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd41d063",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_by_ind(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8f5687",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_by_ind(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee2d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identifying false negatives\n",
    "df_false_neg=df_xgb_results[(df_xgb_results['y_test_pred']==0) & (df_xgb_results['y_test_true']==1)]\n",
    "df_false_neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdd604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at false negative \n",
    "shap_by_ind(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f567250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_by_ind(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fd4722",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_by_ind(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f181c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_by_ind(48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a0b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_by_ind(53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af519737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 additional models:elastic net, support vector machine, KNN\n",
    "def MLpipe_GroupShuffleSplit_F1(X,y,preproc, ML_alg,param):\n",
    "    random_state=42\n",
    "    best_models = []\n",
    "    test_scores_ls = []\n",
    "    random_state_ls=[1,2,3,4,5,6,7,8,9,10]\n",
    "    bl_score_ls=[]\n",
    "    y_pred_ls=[]\n",
    "    y_act_ls=[]\n",
    "    X_train_ls=[]\n",
    "    X_test_ls=[]\n",
    "    for random_state in random_state_ls:\n",
    "        model_name=str(clf)[:-2].lower()\n",
    "        print(model_name)\n",
    "        if param.get(f'{model_name}__random_state'):\n",
    "            print(param)\n",
    "            param[f'{model_name}__random_state']=[random_state]\n",
    "        # first split to separate out the test set\n",
    "        X_other, X_test, y_other, y_test = train_test_split(X,y,test_size = 0.2,random_state=random_state)\n",
    "        print('y_test shape:',y_test.shape)\n",
    "        gss = GroupShuffleSplit(n_splits=4, train_size=.75, random_state=random_state)\n",
    "        #create bl score on test set\n",
    "        \n",
    "        y_row=len(y_test)\n",
    "        y_dummy_pred=[1] * y_row\n",
    "        #calculate baseline F1 score\n",
    "        bl_f1=f1_score(y_dummy_pred, y_test, average='macro')\n",
    "        bl_score_ls.append(bl_f1)\n",
    "        print(bl_f1)\n",
    "        \n",
    "        #create the pipeline\n",
    "        pipe=make_pipeline(preproc,ML_alg)\n",
    "        # use GridSearchCV\n",
    "        # GridSearchCV loops through all parameter combinations and collects the results \n",
    "        grid = GridSearchCV(pipe, param_grid=param, scoring = 'f1_macro' ,cv=gss, return_train_score = True, n_jobs=-1, verbose=True)\n",
    "        print(X_other.shape,y_other.shape)\n",
    "        grid.fit(X_other, y_other.ravel(), groups=X_other['wave'])\n",
    "        #save results to a df.\n",
    "        results=pd.DataFrame(grid.cv_results_)\n",
    "        print(results)    \n",
    "        #save the model\n",
    "        best_models.append(grid)\n",
    "        #calc and save the test score\n",
    "        y_test_pred= best_models[-1].predict(X_test)\n",
    "        y_pred_ls.append(y_test_pred)\n",
    "        y_act_ls.append(y_test)\n",
    "        print('y_test_pred:', y_test_pred)\n",
    "        test_score=f1_score(y_test, y_test_pred, average='macro')\n",
    "        test_scores_ls.append(test_score)\n",
    "        print('test score:',test_score)\n",
    "        X_train_ls.append(X_other)\n",
    "        X_test_ls.append(X_test)\n",
    "    model_mean=statistics.mean(test_scores_ls)\n",
    "    model_std=np.std(test_scores_ls)\n",
    "    print(f'the overall mean of all 10 models is: {model_mean} and the standard deviation is: {model_std}')\n",
    "    avg_bl=statistics.mean(bl_score_ls)\n",
    "    print(avg_bl)\n",
    "    factors_better=(model_mean-avg_bl)/model_std\n",
    "    print(f'The model is on average {factors_better} better than the baseline')\n",
    "    return model_mean,model_std,test_scores_ls,best_models,avg_bl,y_pred_ls,y_act_ls,X_train_ls,X_test_ls\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b957d4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logisticregression\n",
      "y_test shape: (380,)\n",
      "0.14027149321266968\n",
      "(1516, 109) (1516,)\n",
      "Fitting 4 folds for each of 40 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0        0.017885      0.005847         0.003611        0.000483   \n",
      "1        0.018362      0.002750         0.003358        0.000156   \n",
      "2        0.013604      0.002402         0.007127        0.006473   \n",
      "3        0.016142      0.002007         0.004073        0.000378   \n",
      "4        0.012986      0.001817         0.003616        0.000495   \n",
      "5        0.016128      0.002726         0.003218        0.000091   \n",
      "6        0.012424      0.002110         0.003350        0.000155   \n",
      "7        0.016317      0.001362         0.003114        0.000087   \n",
      "8        0.012585      0.002440         0.003407        0.000426   \n",
      "9        0.017577      0.003076         0.003135        0.000067   \n",
      "10       0.015847      0.002685         0.003206        0.000172   \n",
      "11       0.020129      0.002911         0.003127        0.000089   \n",
      "12       0.013948      0.001202         0.003069        0.000060   \n",
      "13       0.022208      0.002874         0.003452        0.000371   \n",
      "14       0.015768      0.002283         0.003624        0.000608   \n",
      "15       0.028235      0.005003         0.003011        0.000109   \n",
      "16       0.024668      0.004897         0.003048        0.000116   \n",
      "17       0.033122      0.006498         0.002966        0.000084   \n",
      "18       0.102031      0.027216         0.005234        0.002875   \n",
      "19       0.043926      0.011430         0.003005        0.000182   \n",
      "20       0.287187      0.067198         0.005640        0.002435   \n",
      "21       0.050618      0.004556         0.005880        0.004645   \n",
      "22       0.824405      0.222031         0.003950        0.001680   \n",
      "23       0.076767      0.015682         0.005162        0.003596   \n",
      "24       2.356652      0.286930         0.004180        0.002098   \n",
      "25       0.088526      0.012102         0.004717        0.001827   \n",
      "26       3.013931      1.372286         0.003080        0.000080   \n",
      "27       0.103339      0.010388         0.003124        0.000199   \n",
      "28       2.939161      1.212690         0.003728        0.001414   \n",
      "29       0.124242      0.012041         0.004200        0.001967   \n",
      "30       3.234510      1.838909         0.003122        0.000261   \n",
      "31       0.139247      0.026789         0.003170        0.000218   \n",
      "32       3.908690      2.893774         0.002994        0.000084   \n",
      "33       0.147729      0.044788         0.003135        0.000055   \n",
      "34       4.201701      3.399998         0.003015        0.000087   \n",
      "35       0.165572      0.062920         0.004922        0.003271   \n",
      "36       4.690059      3.633532         0.003660        0.001091   \n",
      "37       0.202620      0.105036         0.004469        0.001260   \n",
      "38       4.495857      3.427001         0.003128        0.000208   \n",
      "39       0.248956      0.155114         0.003243        0.000316   \n",
      "\n",
      "   param_logisticregression__C param_logisticregression__penalty  \\\n",
      "0                       0.0001                                l1   \n",
      "1                       0.0001                                l2   \n",
      "2                     0.000264                                l1   \n",
      "3                     0.000264                                l2   \n",
      "4                     0.000695                                l1   \n",
      "5                     0.000695                                l2   \n",
      "6                     0.001833                                l1   \n",
      "7                     0.001833                                l2   \n",
      "8                     0.004833                                l1   \n",
      "9                     0.004833                                l2   \n",
      "10                    0.012743                                l1   \n",
      "11                    0.012743                                l2   \n",
      "12                    0.033598                                l1   \n",
      "13                    0.033598                                l2   \n",
      "14                    0.088587                                l1   \n",
      "15                    0.088587                                l2   \n",
      "16                    0.233572                                l1   \n",
      "17                    0.233572                                l2   \n",
      "18                    0.615848                                l1   \n",
      "19                    0.615848                                l2   \n",
      "20                    1.623777                                l1   \n",
      "21                    1.623777                                l2   \n",
      "22                    4.281332                                l1   \n",
      "23                    4.281332                                l2   \n",
      "24                   11.288379                                l1   \n",
      "25                   11.288379                                l2   \n",
      "26                   29.763514                                l1   \n",
      "27                   29.763514                                l2   \n",
      "28                   78.475997                                l1   \n",
      "29                   78.475997                                l2   \n",
      "30                  206.913808                                l1   \n",
      "31                  206.913808                                l2   \n",
      "32                  545.559478                                l1   \n",
      "33                  545.559478                                l2   \n",
      "34                 1438.449888                                l1   \n",
      "35                 1438.449888                                l2   \n",
      "36                 3792.690191                                l1   \n",
      "37                 3792.690191                                l2   \n",
      "38                     10000.0                                l1   \n",
      "39                     10000.0                                l2   \n",
      "\n",
      "   param_logisticregression__solver  \\\n",
      "0                         liblinear   \n",
      "1                         liblinear   \n",
      "2                         liblinear   \n",
      "3                         liblinear   \n",
      "4                         liblinear   \n",
      "5                         liblinear   \n",
      "6                         liblinear   \n",
      "7                         liblinear   \n",
      "8                         liblinear   \n",
      "9                         liblinear   \n",
      "10                        liblinear   \n",
      "11                        liblinear   \n",
      "12                        liblinear   \n",
      "13                        liblinear   \n",
      "14                        liblinear   \n",
      "15                        liblinear   \n",
      "16                        liblinear   \n",
      "17                        liblinear   \n",
      "18                        liblinear   \n",
      "19                        liblinear   \n",
      "20                        liblinear   \n",
      "21                        liblinear   \n",
      "22                        liblinear   \n",
      "23                        liblinear   \n",
      "24                        liblinear   \n",
      "25                        liblinear   \n",
      "26                        liblinear   \n",
      "27                        liblinear   \n",
      "28                        liblinear   \n",
      "29                        liblinear   \n",
      "30                        liblinear   \n",
      "31                        liblinear   \n",
      "32                        liblinear   \n",
      "33                        liblinear   \n",
      "34                        liblinear   \n",
      "35                        liblinear   \n",
      "36                        liblinear   \n",
      "37                        liblinear   \n",
      "38                        liblinear   \n",
      "39                        liblinear   \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "0   {'logisticregression__C': 0.0001, 'logisticreg...           0.451613   \n",
      "1   {'logisticregression__C': 0.0001, 'logisticreg...           0.516390   \n",
      "2   {'logisticregression__C': 0.000263665089873035...           0.451613   \n",
      "3   {'logisticregression__C': 0.000263665089873035...           0.552377   \n",
      "4   {'logisticregression__C': 0.000695192796177560...           0.451613   \n",
      "5   {'logisticregression__C': 0.000695192796177560...           0.589494   \n",
      "6   {'logisticregression__C': 0.001832980710832435...           0.451613   \n",
      "7   {'logisticregression__C': 0.001832980710832435...           0.658092   \n",
      "8   {'logisticregression__C': 0.004832930238571752...           0.451613   \n",
      "9   {'logisticregression__C': 0.004832930238571752...           0.668619   \n",
      "10  {'logisticregression__C': 0.012742749857031334...           0.451613   \n",
      "11  {'logisticregression__C': 0.012742749857031334...           0.668168   \n",
      "12  {'logisticregression__C': 0.03359818286283781,...           0.652387   \n",
      "13  {'logisticregression__C': 0.03359818286283781,...           0.669786   \n",
      "14  {'logisticregression__C': 0.08858667904100823,...           0.648276   \n",
      "15  {'logisticregression__C': 0.08858667904100823,...           0.665112   \n",
      "16  {'logisticregression__C': 0.23357214690901212,...           0.672791   \n",
      "17  {'logisticregression__C': 0.23357214690901212,...           0.652406   \n",
      "18  {'logisticregression__C': 0.615848211066026, '...           0.672791   \n",
      "19  {'logisticregression__C': 0.615848211066026, '...           0.626689   \n",
      "20  {'logisticregression__C': 1.623776739188721, '...           0.622634   \n",
      "21  {'logisticregression__C': 1.623776739188721, '...           0.639123   \n",
      "22  {'logisticregression__C': 4.281332398719396, '...           0.589827   \n",
      "23  {'logisticregression__C': 4.281332398719396, '...           0.543486   \n",
      "24  {'logisticregression__C': 11.288378916846883, ...           0.334842   \n",
      "25  {'logisticregression__C': 11.288378916846883, ...           0.423771   \n",
      "26  {'logisticregression__C': 29.763514416313132, ...           0.282585   \n",
      "27  {'logisticregression__C': 29.763514416313132, ...           0.321142   \n",
      "28  {'logisticregression__C': 78.47599703514607, '...           0.267655   \n",
      "29  {'logisticregression__C': 78.47599703514607, '...           0.287492   \n",
      "30  {'logisticregression__C': 206.913808111479, 'l...           0.262605   \n",
      "31  {'logisticregression__C': 206.913808111479, 'l...           0.267655   \n",
      "32  {'logisticregression__C': 545.5594781168514, '...           0.257518   \n",
      "33  {'logisticregression__C': 545.5594781168514, '...           0.257518   \n",
      "34  {'logisticregression__C': 1438.44988828766, 'l...           0.257518   \n",
      "35  {'logisticregression__C': 1438.44988828766, 'l...           0.257518   \n",
      "36  {'logisticregression__C': 3792.690190732246, '...           0.257518   \n",
      "37  {'logisticregression__C': 3792.690190732246, '...           0.252393   \n",
      "38  {'logisticregression__C': 10000.0, 'logisticre...           0.257518   \n",
      "39  {'logisticregression__C': 10000.0, 'logisticre...           0.252393   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
      "0            0.456186           0.451613           0.417910         0.444330   \n",
      "1            0.520649           0.516390           0.647917         0.550337   \n",
      "2            0.456186           0.451613           0.417910         0.444330   \n",
      "3            0.566521           0.552377           0.687583         0.589715   \n",
      "4            0.456186           0.451613           0.417910         0.444330   \n",
      "5            0.616809           0.589494           0.592551         0.597087   \n",
      "6            0.456186           0.451613           0.417910         0.444330   \n",
      "7            0.670015           0.658092           0.546032         0.633058   \n",
      "8            0.456186           0.451613           0.417910         0.444330   \n",
      "9            0.650276           0.668619           0.521205         0.627180   \n",
      "10           0.456186           0.451613           0.417910         0.444330   \n",
      "11           0.640079           0.668168           0.577290         0.638426   \n",
      "12           0.521072           0.652387           0.507576         0.583355   \n",
      "13           0.578234           0.669786           0.586307         0.626028   \n",
      "14           0.604936           0.648276           0.499847         0.600334   \n",
      "15           0.564112           0.665112           0.586307         0.620161   \n",
      "16           0.557658           0.672791           0.546032         0.612318   \n",
      "17           0.508361           0.652406           0.572650         0.596456   \n",
      "18           0.523372           0.672791           0.562931         0.607971   \n",
      "19           0.466287           0.626689           0.598683         0.579587   \n",
      "20           0.466287           0.622634           0.556818         0.567093   \n",
      "21           0.466287           0.639123           0.608796         0.588332   \n",
      "22           0.456186           0.589827           0.528011         0.540963   \n",
      "23           0.466287           0.543486           0.635854         0.547278   \n",
      "24           0.456186           0.334842           0.610000         0.433967   \n",
      "25           0.466287           0.423771           0.614215         0.482011   \n",
      "26           0.456186           0.282585           0.610000         0.407839   \n",
      "27           0.456186           0.321142           0.637857         0.434082   \n",
      "28           0.456186           0.267655           0.610000         0.400374   \n",
      "29           0.456186           0.287492           0.610000         0.410292   \n",
      "30           0.456186           0.257518           0.610000         0.396577   \n",
      "31           0.456186           0.267655           0.610000         0.400374   \n",
      "32           0.456186           0.257518           0.626794         0.399504   \n",
      "33           0.456186           0.257518           0.610000         0.395305   \n",
      "34           0.456186           0.257518           0.626794         0.399504   \n",
      "35           0.456186           0.257518           0.626794         0.399504   \n",
      "36           0.456186           0.257518           0.626794         0.399504   \n",
      "37           0.456186           0.252393           0.626794         0.396941   \n",
      "38           0.456186           0.257518           0.626794         0.399504   \n",
      "39           0.456186           0.252393           0.626794         0.396941   \n",
      "\n",
      "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
      "0         0.015367               20            0.453356            0.450871   \n",
      "1         0.056365               16            0.675910            0.659508   \n",
      "2         0.015367               20            0.453356            0.450871   \n",
      "3         0.056799               11            0.690284            0.686317   \n",
      "4         0.015367               20            0.453356            0.450871   \n",
      "5         0.011455                9            0.728988            0.721054   \n",
      "6         0.015367               20            0.453356            0.450871   \n",
      "7         0.050480                2            0.732971            0.750708   \n",
      "8         0.015367               20            0.453356            0.450871   \n",
      "9         0.061641                3            0.725804            0.741873   \n",
      "10        0.015367               20            0.453356            0.450871   \n",
      "11        0.037113                1            0.721162            0.753040   \n",
      "12        0.069196               13            0.602751            0.544769   \n",
      "13        0.043851                4            0.732536            0.753040   \n",
      "14        0.060654                8            0.691454            0.687395   \n",
      "15        0.045631                5            0.741505            0.778853   \n",
      "16        0.060613                6            0.730108            0.771575   \n",
      "17        0.060391               10            0.757140            0.774018   \n",
      "18        0.066312                7            0.747748            0.778534   \n",
      "19        0.066406               14            0.759370            0.783774   \n",
      "20        0.064104               15            0.765599            0.795918   \n",
      "21        0.071542               12            0.766605            0.802378   \n",
      "22        0.055069               18            0.776620            0.798218   \n",
      "23        0.060071               17            0.774464            0.803908   \n",
      "24        0.113063               27            0.777452            0.822591   \n",
      "25        0.078277               19            0.771477            0.817452   \n",
      "26        0.136550               29            0.773523            0.827670   \n",
      "27        0.129927               26            0.780439            0.824167   \n",
      "28        0.143428               30            0.774552            0.829537   \n",
      "29        0.134303               28            0.773523            0.821024   \n",
      "30        0.146960               39            0.776523            0.827040   \n",
      "31        0.143428               30            0.774552            0.828610   \n",
      "32        0.154267               32            0.776523            0.827040   \n",
      "33        0.148131               40            0.776523            0.829537   \n",
      "34        0.154267               32            0.776523            0.827040   \n",
      "35        0.154267               32            0.776523            0.829537   \n",
      "36        0.154267               32            0.776523            0.829537   \n",
      "37        0.156629               37            0.776523            0.831110   \n",
      "38        0.154267               32            0.776523            0.829537   \n",
      "39        0.156629               37            0.776523            0.832019   \n",
      "\n",
      "    split2_train_score  split3_train_score  mean_train_score  std_train_score  \n",
      "0             0.453356            0.454890          0.453118         0.001441  \n",
      "1             0.675910            0.673276          0.671151         0.006808  \n",
      "2             0.453356            0.454890          0.453118         0.001441  \n",
      "3             0.690284            0.690605          0.689373         0.001769  \n",
      "4             0.453356            0.454890          0.453118         0.001441  \n",
      "5             0.728988            0.725398          0.726107         0.003265  \n",
      "6             0.453356            0.454890          0.453118         0.001441  \n",
      "7             0.732971            0.726059          0.735677         0.009125  \n",
      "8             0.453356            0.454890          0.453118         0.001441  \n",
      "9             0.725804            0.698596          0.723019         0.015552  \n",
      "10            0.453356            0.454890          0.453118         0.001441  \n",
      "11            0.721162            0.703634          0.724750         0.017832  \n",
      "12            0.602751            0.578541          0.582203         0.023765  \n",
      "13            0.732536            0.718242          0.734088         0.012400  \n",
      "14            0.691454            0.679860          0.687541         0.004734  \n",
      "15            0.741505            0.736197          0.749515         0.017076  \n",
      "16            0.730108            0.723945          0.738934         0.019013  \n",
      "17            0.757140            0.750690          0.759747         0.008650  \n",
      "18            0.747748            0.739672          0.753425         0.014867  \n",
      "19            0.759370            0.756936          0.764863         0.010964  \n",
      "20            0.765599            0.752017          0.769783         0.016075  \n",
      "21            0.766605            0.754008          0.772399         0.018057  \n",
      "22            0.776620            0.755135          0.776648         0.015232  \n",
      "23            0.774464            0.761882          0.778680         0.015445  \n",
      "24            0.777452            0.762938          0.785108         0.022437  \n",
      "25            0.771477            0.761925          0.780583         0.021641  \n",
      "26            0.773523            0.766746          0.785366         0.024581  \n",
      "27            0.780439            0.758159          0.785801         0.023945  \n",
      "28            0.774552            0.764859          0.785875         0.025517  \n",
      "29            0.773523            0.768640          0.784178         0.021366  \n",
      "30            0.776523            0.764859          0.786236         0.024035  \n",
      "31            0.774552            0.764859          0.785643         0.025121  \n",
      "32            0.776523            0.764859          0.786236         0.024035  \n",
      "33            0.776523            0.764859          0.786860         0.025095  \n",
      "34            0.776523            0.764859          0.786236         0.024035  \n",
      "35            0.776523            0.764859          0.786860         0.025095  \n",
      "36            0.776523            0.764859          0.786860         0.025095  \n",
      "37            0.776523            0.764859          0.787253         0.025764  \n",
      "38            0.776523            0.764859          0.786860         0.025095  \n",
      "39            0.776523            0.764859          0.787481         0.026151  \n",
      "y_test_pred: [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 1 0 0 1 0]\n",
      "test score: 0.6998275056020378\n",
      "logisticregression\n",
      "y_test shape: (380,)\n",
      "0.16666666666666669\n",
      "(1516, 109) (1516,)\n",
      "Fitting 4 folds for each of 40 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0        0.007901      0.000695         0.002915        0.000101   \n",
      "1        0.009347      0.001035         0.003001        0.000087   \n",
      "2        0.008182      0.000767         0.003388        0.000357   \n",
      "3        0.010512      0.001360         0.002955        0.000155   \n",
      "4        0.008400      0.000865         0.003106        0.000086   \n",
      "5        0.010712      0.001788         0.002973        0.000116   \n",
      "6        0.008100      0.001650         0.002809        0.000087   \n",
      "7        0.013245      0.001962         0.003191        0.000426   \n",
      "8        0.008655      0.001682         0.002909        0.000068   \n",
      "9        0.013230      0.002396         0.003076        0.000210   \n",
      "10       0.009626      0.001312         0.002971        0.000097   \n",
      "11       0.017236      0.005114         0.002941        0.000069   \n",
      "12       0.010897      0.002646         0.002958        0.000078   \n",
      "13       0.019400      0.003302         0.003640        0.001134   \n",
      "14       0.013698      0.004359         0.002859        0.000111   \n",
      "15       0.028248      0.008001         0.005122        0.002089   \n",
      "16       0.018730      0.005940         0.003403        0.000472   \n",
      "17       0.033285      0.007765         0.004060        0.001480   \n",
      "18       0.074186      0.049282         0.004542        0.002475   \n",
      "19       0.038438      0.009138         0.003003        0.000172   \n",
      "20       0.193431      0.097044         0.003075        0.000180   \n",
      "21       0.050314      0.007428         0.003068        0.000186   \n",
      "22       1.238871      0.587642         0.003101        0.000166   \n",
      "23       0.066744      0.016603         0.004026        0.001681   \n",
      "24       1.972755      0.683078         0.003126        0.000221   \n",
      "25       0.085656      0.016078         0.003040        0.000217   \n",
      "26       3.554748      0.785007         0.003073        0.000176   \n",
      "27       0.096185      0.024848         0.003031        0.000163   \n",
      "28       5.494892      2.685799         0.003213        0.000495   \n",
      "29       0.129051      0.019621         0.003049        0.000186   \n",
      "30       5.434186      2.178240         0.003328        0.000590   \n",
      "31       0.158928      0.003796         0.003226        0.000410   \n",
      "32       5.614372      2.418991         0.003130        0.000292   \n",
      "33       0.206462      0.035654         0.003098        0.000167   \n",
      "34       4.333305      1.720084         0.003571        0.000738   \n",
      "35       0.236461      0.065987         0.005978        0.003181   \n",
      "36       7.043906      3.466128         0.003176        0.000392   \n",
      "37       0.289206      0.118035         0.004423        0.001800   \n",
      "38       6.576252      2.917833         0.003395        0.000607   \n",
      "39       0.287280      0.111777         0.004876        0.003370   \n",
      "\n",
      "   param_logisticregression__C param_logisticregression__penalty  \\\n",
      "0                       0.0001                                l1   \n",
      "1                       0.0001                                l2   \n",
      "2                     0.000264                                l1   \n",
      "3                     0.000264                                l2   \n",
      "4                     0.000695                                l1   \n",
      "5                     0.000695                                l2   \n",
      "6                     0.001833                                l1   \n",
      "7                     0.001833                                l2   \n",
      "8                     0.004833                                l1   \n",
      "9                     0.004833                                l2   \n",
      "10                    0.012743                                l1   \n",
      "11                    0.012743                                l2   \n",
      "12                    0.033598                                l1   \n",
      "13                    0.033598                                l2   \n",
      "14                    0.088587                                l1   \n",
      "15                    0.088587                                l2   \n",
      "16                    0.233572                                l1   \n",
      "17                    0.233572                                l2   \n",
      "18                    0.615848                                l1   \n",
      "19                    0.615848                                l2   \n",
      "20                    1.623777                                l1   \n",
      "21                    1.623777                                l2   \n",
      "22                    4.281332                                l1   \n",
      "23                    4.281332                                l2   \n",
      "24                   11.288379                                l1   \n",
      "25                   11.288379                                l2   \n",
      "26                   29.763514                                l1   \n",
      "27                   29.763514                                l2   \n",
      "28                   78.475997                                l1   \n",
      "29                   78.475997                                l2   \n",
      "30                  206.913808                                l1   \n",
      "31                  206.913808                                l2   \n",
      "32                  545.559478                                l1   \n",
      "33                  545.559478                                l2   \n",
      "34                 1438.449888                                l1   \n",
      "35                 1438.449888                                l2   \n",
      "36                 3792.690191                                l1   \n",
      "37                 3792.690191                                l2   \n",
      "38                     10000.0                                l1   \n",
      "39                     10000.0                                l2   \n",
      "\n",
      "   param_logisticregression__solver  \\\n",
      "0                         liblinear   \n",
      "1                         liblinear   \n",
      "2                         liblinear   \n",
      "3                         liblinear   \n",
      "4                         liblinear   \n",
      "5                         liblinear   \n",
      "6                         liblinear   \n",
      "7                         liblinear   \n",
      "8                         liblinear   \n",
      "9                         liblinear   \n",
      "10                        liblinear   \n",
      "11                        liblinear   \n",
      "12                        liblinear   \n",
      "13                        liblinear   \n",
      "14                        liblinear   \n",
      "15                        liblinear   \n",
      "16                        liblinear   \n",
      "17                        liblinear   \n",
      "18                        liblinear   \n",
      "19                        liblinear   \n",
      "20                        liblinear   \n",
      "21                        liblinear   \n",
      "22                        liblinear   \n",
      "23                        liblinear   \n",
      "24                        liblinear   \n",
      "25                        liblinear   \n",
      "26                        liblinear   \n",
      "27                        liblinear   \n",
      "28                        liblinear   \n",
      "29                        liblinear   \n",
      "30                        liblinear   \n",
      "31                        liblinear   \n",
      "32                        liblinear   \n",
      "33                        liblinear   \n",
      "34                        liblinear   \n",
      "35                        liblinear   \n",
      "36                        liblinear   \n",
      "37                        liblinear   \n",
      "38                        liblinear   \n",
      "39                        liblinear   \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "0   {'logisticregression__C': 0.0001, 'logisticreg...           0.437037   \n",
      "1   {'logisticregression__C': 0.0001, 'logisticreg...           0.680135   \n",
      "2   {'logisticregression__C': 0.000263665089873035...           0.437037   \n",
      "3   {'logisticregression__C': 0.000263665089873035...           0.636015   \n",
      "4   {'logisticregression__C': 0.000695192796177560...           0.437037   \n",
      "5   {'logisticregression__C': 0.000695192796177560...           0.584699   \n",
      "6   {'logisticregression__C': 0.001832980710832435...           0.437037   \n",
      "7   {'logisticregression__C': 0.001832980710832435...           0.604167   \n",
      "8   {'logisticregression__C': 0.004832930238571752...           0.437037   \n",
      "9   {'logisticregression__C': 0.004832930238571752...           0.530353   \n",
      "10  {'logisticregression__C': 0.012742749857031334...           0.437037   \n",
      "11  {'logisticregression__C': 0.012742749857031334...           0.554688   \n",
      "12  {'logisticregression__C': 0.03359818286283781,...           0.437037   \n",
      "13  {'logisticregression__C': 0.03359818286283781,...           0.562212   \n",
      "14  {'logisticregression__C': 0.08858667904100823,...           0.428571   \n",
      "15  {'logisticregression__C': 0.08858667904100823,...           0.543169   \n",
      "16  {'logisticregression__C': 0.23357214690901212,...           0.572148   \n",
      "17  {'logisticregression__C': 0.23357214690901212,...           0.543169   \n",
      "18  {'logisticregression__C': 0.615848211066026, '...           0.574513   \n",
      "19  {'logisticregression__C': 0.615848211066026, '...           0.574513   \n",
      "20  {'logisticregression__C': 1.623776739188721, '...           0.574513   \n",
      "21  {'logisticregression__C': 1.623776739188721, '...           0.554876   \n",
      "22  {'logisticregression__C': 4.281332398719396, '...           0.491071   \n",
      "23  {'logisticregression__C': 4.281332398719396, '...           0.516170   \n",
      "24  {'logisticregression__C': 11.288378916846883, ...           0.499887   \n",
      "25  {'logisticregression__C': 11.288378916846883, ...           0.507478   \n",
      "26  {'logisticregression__C': 29.763514416313132, ...           0.499887   \n",
      "27  {'logisticregression__C': 29.763514416313132, ...           0.490421   \n",
      "28  {'logisticregression__C': 78.47599703514607, '...           0.499887   \n",
      "29  {'logisticregression__C': 78.47599703514607, '...           0.482021   \n",
      "30  {'logisticregression__C': 206.913808111479, 'l...           0.499887   \n",
      "31  {'logisticregression__C': 206.913808111479, 'l...           0.473684   \n",
      "32  {'logisticregression__C': 545.5594781168514, '...           0.499887   \n",
      "33  {'logisticregression__C': 545.5594781168514, '...           0.499887   \n",
      "34  {'logisticregression__C': 1438.44988828766, 'l...           0.499887   \n",
      "35  {'logisticregression__C': 1438.44988828766, 'l...           0.499887   \n",
      "36  {'logisticregression__C': 3792.690190732246, '...           0.499887   \n",
      "37  {'logisticregression__C': 3792.690190732246, '...           0.499887   \n",
      "38  {'logisticregression__C': 10000.0, 'logisticre...           0.499887   \n",
      "39  {'logisticregression__C': 10000.0, 'logisticre...           0.499887   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
      "0            0.461342           0.456140           0.461342         0.453966   \n",
      "1            0.481222           0.483881           0.481222         0.531615   \n",
      "2            0.461342           0.456140           0.461342         0.453966   \n",
      "3            0.519697           0.528820           0.519697         0.551057   \n",
      "4            0.461342           0.456140           0.461342         0.453966   \n",
      "5            0.618218           0.558168           0.618218         0.594826   \n",
      "6            0.461342           0.456140           0.461342         0.453966   \n",
      "7            0.675205           0.637463           0.675205         0.648010   \n",
      "8            0.461342           0.456140           0.461342         0.453966   \n",
      "9            0.663761           0.660491           0.663761         0.629591   \n",
      "10           0.461342           0.456140           0.461342         0.453966   \n",
      "11           0.614303           0.637107           0.614303         0.605100   \n",
      "12           0.505021           0.648640           0.505021         0.523930   \n",
      "13           0.582495           0.623656           0.582495         0.587715   \n",
      "14           0.603453           0.613191           0.603453         0.562167   \n",
      "15           0.543311           0.637916           0.543311         0.566927   \n",
      "16           0.575604           0.619347           0.575604         0.585676   \n",
      "17           0.481584           0.616361           0.481584         0.530675   \n",
      "18           0.560825           0.606884           0.560825         0.575762   \n",
      "19           0.472604           0.589909           0.472604         0.527408   \n",
      "20           0.461342           0.591209           0.461342         0.522102   \n",
      "21           0.472604           0.591412           0.472604         0.522874   \n",
      "22           0.515331           0.507937           0.515331         0.507417   \n",
      "23           0.487495           0.496694           0.487495         0.496964   \n",
      "24           0.521269           0.346679           0.521269         0.472276   \n",
      "25           0.518006           0.386724           0.518006         0.482553   \n",
      "26           0.521797           0.355009           0.517764         0.473614   \n",
      "27           0.519076           0.357482           0.519076         0.471514   \n",
      "28           0.501323           0.352515           0.495873         0.462399   \n",
      "29           0.511687           0.361765           0.511687         0.466790   \n",
      "30           0.491268           0.351741           0.490370         0.458317   \n",
      "31           0.506915           0.351741           0.506915         0.459814   \n",
      "32           0.479785           0.346679           0.486806         0.453289   \n",
      "33           0.500410           0.346679           0.500410         0.461846   \n",
      "34           0.480655           0.346679           0.493070         0.455073   \n",
      "35           0.491461           0.349854           0.491461         0.458166   \n",
      "36           0.477185           0.350855           0.475789         0.450929   \n",
      "37           0.479284           0.349854           0.479284         0.452077   \n",
      "38           0.475789           0.350855           0.475789         0.450580   \n",
      "39           0.473549           0.349854           0.473549         0.449210   \n",
      "\n",
      "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
      "0         0.010002               30            0.456809            0.451833   \n",
      "1         0.085755               11            0.672252            0.663889   \n",
      "2         0.010002               30            0.456809            0.451833   \n",
      "3         0.049192               10            0.687824            0.687089   \n",
      "4         0.010002               30            0.456809            0.451833   \n",
      "5         0.025203                4            0.736912            0.731840   \n",
      "6         0.010002               30            0.456809            0.451833   \n",
      "7         0.029634                1            0.737883            0.761408   \n",
      "8         0.010002               30            0.456809            0.451833   \n",
      "9         0.057311                2            0.717507            0.724595   \n",
      "10        0.010002               30            0.456809            0.451833   \n",
      "11        0.030558                3            0.716535            0.719155   \n",
      "12        0.077165               14            0.572750            0.531301   \n",
      "13        0.022342                5            0.722177            0.741380   \n",
      "14        0.077234                9            0.688810            0.672960   \n",
      "15        0.040985                8            0.741415            0.762359   \n",
      "16        0.019491                6            0.742134            0.759259   \n",
      "17        0.055493               12            0.748992            0.782248   \n",
      "18        0.018817                7            0.759431            0.784626   \n",
      "19        0.055073               13            0.766613            0.797268   \n",
      "20        0.061045               16            0.770648            0.808962   \n",
      "21        0.051903               15            0.775576            0.813707   \n",
      "22        0.009908               17            0.784347            0.827931   \n",
      "23        0.011707               18            0.783341            0.816918   \n",
      "24        0.073037               21            0.788949            0.827931   \n",
      "25        0.055494               19            0.787171            0.822724   \n",
      "26        0.068971               20            0.792853            0.835840   \n",
      "27        0.066868               22            0.788065            0.828903   \n",
      "28        0.063473               24            0.792703            0.844126   \n",
      "29        0.061834               23            0.793715            0.833330   \n",
      "30        0.061643               27            0.793557            0.848175   \n",
      "31        0.063854               26            0.792703            0.838335   \n",
      "32        0.061973               36            0.793557            0.846562   \n",
      "33        0.066492               25            0.793557            0.846562   \n",
      "34        0.062960               29            0.793557            0.846562   \n",
      "35        0.062628               28            0.791695            0.846562   \n",
      "36        0.058564               38            0.793557            0.846562   \n",
      "37        0.059615               37            0.791695            0.846562   \n",
      "38        0.058411               39            0.793557            0.846562   \n",
      "39        0.058362               40            0.791695            0.840078   \n",
      "\n",
      "    split2_train_score  split3_train_score  mean_train_score  std_train_score  \n",
      "0             0.455802            0.451833          0.454069         0.002264  \n",
      "1             0.652596            0.663889          0.663156         0.006988  \n",
      "2             0.455802            0.451833          0.454069         0.002264  \n",
      "3             0.689241            0.687089          0.687811         0.000879  \n",
      "4             0.455802            0.451833          0.454069         0.002264  \n",
      "5             0.743338            0.731840          0.735983         0.004724  \n",
      "6             0.455802            0.451833          0.454069         0.002264  \n",
      "7             0.745870            0.761408          0.751642         0.010166  \n",
      "8             0.455802            0.451833          0.454069         0.002264  \n",
      "9             0.716892            0.724595          0.720897         0.003704  \n",
      "10            0.455802            0.451833          0.454069         0.002264  \n",
      "11            0.723164            0.719155          0.719503         0.002369  \n",
      "12            0.572052            0.531301          0.551851         0.020551  \n",
      "13            0.750959            0.741380          0.738974         0.010456  \n",
      "14            0.701270            0.672960          0.684000         0.011886  \n",
      "15            0.765456            0.762359          0.757897         0.009600  \n",
      "16            0.747257            0.759259          0.751977         0.007504  \n",
      "17            0.778659            0.782248          0.773037         0.013959  \n",
      "18            0.767828            0.784626          0.774128         0.010910  \n",
      "19            0.780727            0.797268          0.785469         0.012811  \n",
      "20            0.780646            0.808962          0.792305         0.017029  \n",
      "21            0.780646            0.813707          0.795909         0.017888  \n",
      "22            0.784724            0.827931          0.806233         0.021698  \n",
      "23            0.776241            0.816918          0.798354         0.018732  \n",
      "24            0.790730            0.827931          0.808885         0.019056  \n",
      "25            0.790563            0.822724          0.805795         0.016971  \n",
      "26            0.807767            0.838335          0.818699         0.019150  \n",
      "27            0.794765            0.828903          0.810159         0.018893  \n",
      "28            0.805807            0.846562          0.822300         0.023522  \n",
      "29            0.803837            0.833330          0.816053         0.017644  \n",
      "30            0.807767            0.846562          0.824015         0.023894  \n",
      "31            0.805807            0.838335          0.818795         0.020082  \n",
      "32            0.808889            0.846562          0.823893         0.023309  \n",
      "33            0.806649            0.846562          0.823333         0.023686  \n",
      "34            0.808889            0.846562          0.823893         0.023309  \n",
      "35            0.807767            0.846562          0.823147         0.024095  \n",
      "36            0.807767            0.844126          0.823003         0.022915  \n",
      "37            0.807767            0.846562          0.823147         0.024095  \n",
      "38            0.807767            0.846562          0.823612         0.023494  \n",
      "39            0.807767            0.840078          0.819904         0.020958  \n",
      "y_test_pred: [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0\n",
      " 0 0 1 0 0 0 1 0 0 0]\n",
      "test score: 0.7138517696008986\n",
      "logisticregression\n",
      "y_test shape: (380,)\n",
      "0.1264367816091954\n",
      "(1516, 109) (1516,)\n",
      "Fitting 4 folds for each of 40 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0        0.008440      0.001018         0.002911        0.000129   \n",
      "1        0.008900      0.000917         0.002806        0.000114   \n",
      "2        0.007922      0.000628         0.003055        0.000110   \n",
      "3        0.010056      0.001019         0.003164        0.000154   \n",
      "4        0.007735      0.000456         0.002963        0.000086   \n",
      "5        0.010220      0.001252         0.003023        0.000087   \n",
      "6        0.007398      0.000737         0.002880        0.000066   \n",
      "7        0.011695      0.002024         0.002916        0.000080   \n",
      "8        0.007980      0.001030         0.002828        0.000068   \n",
      "9        0.013005      0.001665         0.003036        0.000123   \n",
      "10       0.009423      0.001745         0.002967        0.000076   \n",
      "11       0.015563      0.002487         0.002956        0.000193   \n",
      "12       0.010252      0.001658         0.002984        0.000133   \n",
      "13       0.019326      0.004275         0.002982        0.000151   \n",
      "14       0.011598      0.001877         0.002975        0.000116   \n",
      "15       0.024752      0.003124         0.003027        0.000210   \n",
      "16       0.021074      0.005624         0.003037        0.000136   \n",
      "17       0.027144      0.004538         0.002927        0.000088   \n",
      "18       0.076017      0.032174         0.002961        0.000090   \n",
      "19       0.036914      0.006283         0.004519        0.002352   \n",
      "20       0.226248      0.023865         0.004406        0.002212   \n",
      "21       0.047799      0.008194         0.002951        0.000143   \n",
      "22       0.860793      0.118579         0.003853        0.001344   \n",
      "23       0.062484      0.012949         0.003005        0.000118   \n",
      "24       1.766050      0.343880         0.003289        0.000386   \n",
      "25       0.081167      0.018113         0.003043        0.000103   \n",
      "26       2.672329      0.471404         0.003699        0.001174   \n",
      "27       0.102614      0.008993         0.003022        0.000124   \n",
      "28       3.355836      0.277553         0.004948        0.002594   \n",
      "29       0.127945      0.032525         0.003205        0.000229   \n",
      "30       3.786860      0.424582         0.003237        0.000448   \n",
      "31       0.152908      0.028460         0.003947        0.001563   \n",
      "32       3.859788      0.420344         0.003329        0.000437   \n",
      "33       0.149484      0.011153         0.003389        0.000578   \n",
      "34       4.072733      0.676666         0.003410        0.000632   \n",
      "35       0.180601      0.009763         0.003020        0.000081   \n",
      "36       5.212988      1.475909         0.005285        0.002175   \n",
      "37       0.184425      0.020955         0.003001        0.000085   \n",
      "38       3.942702      1.169234         0.003056        0.000127   \n",
      "39       0.208029      0.018694         0.003028        0.000129   \n",
      "\n",
      "   param_logisticregression__C param_logisticregression__penalty  \\\n",
      "0                       0.0001                                l1   \n",
      "1                       0.0001                                l2   \n",
      "2                     0.000264                                l1   \n",
      "3                     0.000264                                l2   \n",
      "4                     0.000695                                l1   \n",
      "5                     0.000695                                l2   \n",
      "6                     0.001833                                l1   \n",
      "7                     0.001833                                l2   \n",
      "8                     0.004833                                l1   \n",
      "9                     0.004833                                l2   \n",
      "10                    0.012743                                l1   \n",
      "11                    0.012743                                l2   \n",
      "12                    0.033598                                l1   \n",
      "13                    0.033598                                l2   \n",
      "14                    0.088587                                l1   \n",
      "15                    0.088587                                l2   \n",
      "16                    0.233572                                l1   \n",
      "17                    0.233572                                l2   \n",
      "18                    0.615848                                l1   \n",
      "19                    0.615848                                l2   \n",
      "20                    1.623777                                l1   \n",
      "21                    1.623777                                l2   \n",
      "22                    4.281332                                l1   \n",
      "23                    4.281332                                l2   \n",
      "24                   11.288379                                l1   \n",
      "25                   11.288379                                l2   \n",
      "26                   29.763514                                l1   \n",
      "27                   29.763514                                l2   \n",
      "28                   78.475997                                l1   \n",
      "29                   78.475997                                l2   \n",
      "30                  206.913808                                l1   \n",
      "31                  206.913808                                l2   \n",
      "32                  545.559478                                l1   \n",
      "33                  545.559478                                l2   \n",
      "34                 1438.449888                                l1   \n",
      "35                 1438.449888                                l2   \n",
      "36                 3792.690191                                l1   \n",
      "37                 3792.690191                                l2   \n",
      "38                     10000.0                                l1   \n",
      "39                     10000.0                                l2   \n",
      "\n",
      "   param_logisticregression__solver  \\\n",
      "0                         liblinear   \n",
      "1                         liblinear   \n",
      "2                         liblinear   \n",
      "3                         liblinear   \n",
      "4                         liblinear   \n",
      "5                         liblinear   \n",
      "6                         liblinear   \n",
      "7                         liblinear   \n",
      "8                         liblinear   \n",
      "9                         liblinear   \n",
      "10                        liblinear   \n",
      "11                        liblinear   \n",
      "12                        liblinear   \n",
      "13                        liblinear   \n",
      "14                        liblinear   \n",
      "15                        liblinear   \n",
      "16                        liblinear   \n",
      "17                        liblinear   \n",
      "18                        liblinear   \n",
      "19                        liblinear   \n",
      "20                        liblinear   \n",
      "21                        liblinear   \n",
      "22                        liblinear   \n",
      "23                        liblinear   \n",
      "24                        liblinear   \n",
      "25                        liblinear   \n",
      "26                        liblinear   \n",
      "27                        liblinear   \n",
      "28                        liblinear   \n",
      "29                        liblinear   \n",
      "30                        liblinear   \n",
      "31                        liblinear   \n",
      "32                        liblinear   \n",
      "33                        liblinear   \n",
      "34                        liblinear   \n",
      "35                        liblinear   \n",
      "36                        liblinear   \n",
      "37                        liblinear   \n",
      "38                        liblinear   \n",
      "39                        liblinear   \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "0   {'logisticregression__C': 0.0001, 'logisticreg...           0.453202   \n",
      "1   {'logisticregression__C': 0.0001, 'logisticreg...           0.474394   \n",
      "2   {'logisticregression__C': 0.000263665089873035...           0.453202   \n",
      "3   {'logisticregression__C': 0.000263665089873035...           0.503291   \n",
      "4   {'logisticregression__C': 0.000695192796177560...           0.453202   \n",
      "5   {'logisticregression__C': 0.000695192796177560...           0.530569   \n",
      "6   {'logisticregression__C': 0.001832980710832435...           0.453202   \n",
      "7   {'logisticregression__C': 0.001832980710832435...           0.583402   \n",
      "8   {'logisticregression__C': 0.004832930238571752...           0.453202   \n",
      "9   {'logisticregression__C': 0.004832930238571752...           0.619899   \n",
      "10  {'logisticregression__C': 0.012742749857031334...           0.453202   \n",
      "11  {'logisticregression__C': 0.012742749857031334...           0.634066   \n",
      "12  {'logisticregression__C': 0.03359818286283781,...           0.691667   \n",
      "13  {'logisticregression__C': 0.03359818286283781,...           0.618254   \n",
      "14  {'logisticregression__C': 0.08858667904100823,...           0.685036   \n",
      "15  {'logisticregression__C': 0.08858667904100823,...           0.603166   \n",
      "16  {'logisticregression__C': 0.23357214690901212,...           0.642287   \n",
      "17  {'logisticregression__C': 0.23357214690901212,...           0.608632   \n",
      "18  {'logisticregression__C': 0.615848211066026, '...           0.618254   \n",
      "19  {'logisticregression__C': 0.615848211066026, '...           0.585633   \n",
      "20  {'logisticregression__C': 1.623776739188721, '...           0.595857   \n",
      "21  {'logisticregression__C': 1.623776739188721, '...           0.547908   \n",
      "22  {'logisticregression__C': 4.281332398719396, '...           0.523014   \n",
      "23  {'logisticregression__C': 4.281332398719396, '...           0.492532   \n",
      "24  {'logisticregression__C': 11.288378916846883, ...           0.420390   \n",
      "25  {'logisticregression__C': 11.288378916846883, ...           0.403522   \n",
      "26  {'logisticregression__C': 29.763514416313132, ...           0.390410   \n",
      "27  {'logisticregression__C': 29.763514416313132, ...           0.382375   \n",
      "28  {'logisticregression__C': 78.47599703514607, '...           0.390410   \n",
      "29  {'logisticregression__C': 78.47599703514607, '...           0.382375   \n",
      "30  {'logisticregression__C': 206.913808111479, 'l...           0.394401   \n",
      "31  {'logisticregression__C': 206.913808111479, 'l...           0.382375   \n",
      "32  {'logisticregression__C': 545.5594781168514, '...           0.394401   \n",
      "33  {'logisticregression__C': 545.5594781168514, '...           0.390410   \n",
      "34  {'logisticregression__C': 1438.44988828766, 'l...           0.394401   \n",
      "35  {'logisticregression__C': 1438.44988828766, 'l...           0.394401   \n",
      "36  {'logisticregression__C': 3792.690190732246, '...           0.394401   \n",
      "37  {'logisticregression__C': 3792.690190732246, '...           0.394401   \n",
      "38  {'logisticregression__C': 10000.0, 'logisticre...           0.394401   \n",
      "39  {'logisticregression__C': 10000.0, 'logisticre...           0.394401   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
      "0            0.449756           0.453202           0.456067         0.453057   \n",
      "1            0.718735           0.474394           0.421940         0.522366   \n",
      "2            0.449756           0.453202           0.456067         0.453057   \n",
      "3            0.696178           0.503291           0.452162         0.538730   \n",
      "4            0.449756           0.453202           0.456067         0.453057   \n",
      "5            0.672681           0.530569           0.523513         0.564333   \n",
      "6            0.449756           0.453202           0.456067         0.453057   \n",
      "7            0.614995           0.583402           0.595831         0.594407   \n",
      "8            0.449756           0.453202           0.456067         0.453057   \n",
      "9            0.562432           0.619899           0.667707         0.617484   \n",
      "10           0.449756           0.453202           0.456067         0.453057   \n",
      "11           0.535405           0.634066           0.684375         0.621978   \n",
      "12           0.523013           0.691667           0.544473         0.612705   \n",
      "13           0.545651           0.618254           0.630174         0.603083   \n",
      "14           0.616178           0.685036           0.606626         0.648219   \n",
      "15           0.557977           0.603166           0.574407         0.584679   \n",
      "16           0.607033           0.642287           0.596276         0.621970   \n",
      "17           0.547195           0.608632           0.561540         0.581500   \n",
      "18           0.572120           0.618254           0.636409         0.611259   \n",
      "19           0.559412           0.585633           0.511537         0.560554   \n",
      "20           0.523900           0.595857           0.502491         0.554526   \n",
      "21           0.539923           0.547908           0.491064         0.531700   \n",
      "22           0.518942           0.526357           0.559295         0.531902   \n",
      "23           0.511936           0.492532           0.512953         0.502488   \n",
      "24           0.523166           0.420390           0.554561         0.479626   \n",
      "25           0.513005           0.403522           0.526852         0.461725   \n",
      "26           0.519436           0.390410           0.517891         0.454537   \n",
      "27           0.505149           0.382375           0.533420         0.450830   \n",
      "28           0.511530           0.390410           0.478844         0.442799   \n",
      "29           0.525918           0.382375           0.513419         0.451021   \n",
      "30           0.520652           0.394401           0.491361         0.450204   \n",
      "31           0.528428           0.382375           0.504843         0.449505   \n",
      "32           0.513793           0.394401           0.512459         0.453763   \n",
      "33           0.522190           0.390410           0.504213         0.451806   \n",
      "34           0.517567           0.394401           0.501654         0.452006   \n",
      "35           0.522970           0.394401           0.497478         0.452312   \n",
      "36           0.514927           0.394401           0.499084         0.450703   \n",
      "37           0.523594           0.394401           0.500329         0.453181   \n",
      "38           0.520199           0.394401           0.499440         0.452110   \n",
      "39           0.524760           0.394401           0.499082         0.453161   \n",
      "\n",
      "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
      "0         0.002236               25            0.451462            0.452874   \n",
      "1         0.115379               17            0.664060            0.648252   \n",
      "2         0.002236               25            0.451462            0.452874   \n",
      "3         0.093268               14            0.693885            0.671332   \n",
      "4         0.002236               25            0.451462            0.452874   \n",
      "5         0.062621               11            0.746026            0.714101   \n",
      "6         0.002236               25            0.451462            0.452874   \n",
      "7         0.012924                8            0.759149            0.758069   \n",
      "8         0.002236               25            0.451462            0.452874   \n",
      "9         0.037299                4            0.734539            0.728804   \n",
      "10        0.002236               25            0.451462            0.452874   \n",
      "11        0.054038                2            0.737642            0.713186   \n",
      "12        0.079326                5            0.628215            0.531471   \n",
      "13        0.033514                7            0.745565            0.715241   \n",
      "14        0.036971                1            0.726764            0.639056   \n",
      "15        0.019379                9            0.755415            0.730584   \n",
      "16        0.020669                3            0.749963            0.707225   \n",
      "17        0.027602               10            0.766439            0.750916   \n",
      "18        0.023782                6            0.754464            0.740256   \n",
      "19        0.030257               12            0.783979            0.752612   \n",
      "20        0.042018               13            0.788815            0.755515   \n",
      "21        0.023687               16            0.793631            0.759909   \n",
      "22        0.016032               15            0.799231            0.755834   \n",
      "23        0.009963               18            0.801082            0.757242   \n",
      "24        0.060268               19            0.802646            0.766696   \n",
      "25        0.058409               20            0.800547            0.764028   \n",
      "26        0.064129               21            0.797129            0.765276   \n",
      "27        0.069181               36            0.802646            0.769348   \n",
      "28        0.053648               40            0.797928            0.770391   \n",
      "29        0.068789               35            0.800815            0.767807   \n",
      "30        0.056756               38            0.799765            0.767807   \n",
      "31        0.067646               39            0.797928            0.767807   \n",
      "32        0.059365               22            0.799765            0.767807   \n",
      "33        0.061724               34            0.799765            0.767807   \n",
      "34        0.057879               33            0.799765            0.767807   \n",
      "35        0.058609               31            0.799765            0.767807   \n",
      "36        0.056580               37            0.799765            0.767807   \n",
      "37        0.059353               23            0.799765            0.767807   \n",
      "38        0.058174               32            0.799765            0.767807   \n",
      "39        0.059457               24            0.799765            0.767807   \n",
      "\n",
      "    split2_train_score  split3_train_score  mean_train_score  std_train_score  \n",
      "0             0.451462            0.448408          0.451052         0.001632  \n",
      "1             0.664060            0.673375          0.662437         0.009029  \n",
      "2             0.451462            0.448408          0.451052         0.001632  \n",
      "3             0.693885            0.693096          0.688049         0.009657  \n",
      "4             0.451462            0.448408          0.451052         0.001632  \n",
      "5             0.746026            0.744247          0.737600         0.013586  \n",
      "6             0.451462            0.448408          0.451052         0.001632  \n",
      "7             0.759149            0.752930          0.757324         0.002575  \n",
      "8             0.451462            0.448408          0.451052         0.001632  \n",
      "9             0.734539            0.754242          0.738031         0.009648  \n",
      "10            0.451462            0.448408          0.451052         0.001632  \n",
      "11            0.737642            0.729067          0.729384         0.009986  \n",
      "12            0.628215            0.554285          0.585547         0.043424  \n",
      "13            0.745565            0.742391          0.737191         0.012739  \n",
      "14            0.726764            0.699184          0.697942         0.035814  \n",
      "15            0.755415            0.757212          0.749657         0.011036  \n",
      "16            0.749963            0.755745          0.740724         0.019484  \n",
      "17            0.766439            0.789094          0.768222         0.013615  \n",
      "18            0.754464            0.787598          0.759196         0.017394  \n",
      "19            0.783979            0.801623          0.780548         0.017665  \n",
      "20            0.788815            0.822879          0.789006         0.023818  \n",
      "21            0.793631            0.815645          0.790704         0.019922  \n",
      "22            0.799231            0.822271          0.794142         0.024034  \n",
      "23            0.801082            0.818185          0.794398         0.022560  \n",
      "24            0.802646            0.823535          0.798881         0.020445  \n",
      "25            0.800547            0.822607          0.796932         0.021024  \n",
      "26            0.797129            0.827780          0.796828         0.022100  \n",
      "27            0.802646            0.823535          0.799544         0.019408  \n",
      "28            0.797928            0.830193          0.799110         0.021176  \n",
      "29            0.800815            0.829323          0.799690         0.021778  \n",
      "30            0.799765            0.830193          0.799382         0.022060  \n",
      "31            0.797928            0.828654          0.798079         0.021513  \n",
      "32            0.799765            0.830193          0.799382         0.022060  \n",
      "33            0.799765            0.828654          0.798998         0.021526  \n",
      "34            0.799765            0.830193          0.799382         0.022060  \n",
      "35            0.799765            0.830193          0.799382         0.022060  \n",
      "36            0.799765            0.830193          0.799382         0.022060  \n",
      "37            0.799765            0.830193          0.799382         0.022060  \n",
      "38            0.799765            0.830193          0.799382         0.022060  \n",
      "39            0.799765            0.827780          0.798779         0.021227  \n",
      "y_test_pred: [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "test score: 0.6637426900584795\n",
      "logisticregression\n",
      "y_test shape: (380,)\n",
      "0.16483516483516483\n",
      "(1516, 109) (1516,)\n",
      "Fitting 4 folds for each of 40 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0        0.008023      0.000607         0.002942        0.000177   \n",
      "1        0.009072      0.000564         0.003035        0.000202   \n",
      "2        0.007884      0.000844         0.002970        0.000144   \n",
      "3        0.009813      0.000789         0.002991        0.000123   \n",
      "4        0.007479      0.000679         0.002827        0.000243   \n",
      "5        0.012482      0.003304         0.002954        0.000168   \n",
      "6        0.007909      0.000233         0.002878        0.000117   \n",
      "7        0.012343      0.001477         0.002927        0.000175   \n",
      "8        0.008760      0.000208         0.002833        0.000147   \n",
      "9        0.014992      0.002733         0.003024        0.000195   \n",
      "10       0.009078      0.001302         0.002953        0.000133   \n",
      "11       0.016144      0.002674         0.002906        0.000197   \n",
      "12       0.010634      0.001576         0.002887        0.000139   \n",
      "13       0.021926      0.002411         0.003035        0.000088   \n",
      "14       0.014149      0.003564         0.002928        0.000140   \n",
      "15       0.024164      0.005676         0.002939        0.000129   \n",
      "16       0.016927      0.003466         0.002881        0.000168   \n",
      "17       0.035326      0.012797         0.003168        0.000315   \n",
      "18       0.080180      0.041170         0.003249        0.000393   \n",
      "19       0.042275      0.004213         0.003352        0.000717   \n",
      "20       0.208084      0.036968         0.003005        0.000155   \n",
      "21       0.046782      0.005207         0.002973        0.000142   \n",
      "22       0.778239      0.101436         0.003017        0.000117   \n",
      "23       0.065226      0.008952         0.004442        0.002539   \n",
      "24       1.934605      0.137960         0.004903        0.003012   \n",
      "25       0.079179      0.015082         0.003177        0.000316   \n",
      "26       3.219176      0.934287         0.003037        0.000131   \n",
      "27       0.097259      0.013638         0.004122        0.002098   \n",
      "28       4.940732      2.867135         0.003666        0.000704   \n",
      "29       0.127655      0.021700         0.002986        0.000148   \n",
      "30       5.424370      3.018962         0.003019        0.000106   \n",
      "31       0.137981      0.025142         0.002997        0.000148   \n",
      "32       3.869814      0.925458         0.003156        0.000279   \n",
      "33       0.168503      0.017994         0.004221        0.002340   \n",
      "34       3.748910      1.515645         0.003064        0.000114   \n",
      "35       0.179087      0.018029         0.002957        0.000122   \n",
      "36       5.002371      1.831174         0.003043        0.000172   \n",
      "37       0.176343      0.034828         0.004564        0.002378   \n",
      "38       4.004944      1.816573         0.003149        0.000293   \n",
      "39       0.188749      0.050263         0.002910        0.000176   \n",
      "\n",
      "   param_logisticregression__C param_logisticregression__penalty  \\\n",
      "0                       0.0001                                l1   \n",
      "1                       0.0001                                l2   \n",
      "2                     0.000264                                l1   \n",
      "3                     0.000264                                l2   \n",
      "4                     0.000695                                l1   \n",
      "5                     0.000695                                l2   \n",
      "6                     0.001833                                l1   \n",
      "7                     0.001833                                l2   \n",
      "8                     0.004833                                l1   \n",
      "9                     0.004833                                l2   \n",
      "10                    0.012743                                l1   \n",
      "11                    0.012743                                l2   \n",
      "12                    0.033598                                l1   \n",
      "13                    0.033598                                l2   \n",
      "14                    0.088587                                l1   \n",
      "15                    0.088587                                l2   \n",
      "16                    0.233572                                l1   \n",
      "17                    0.233572                                l2   \n",
      "18                    0.615848                                l1   \n",
      "19                    0.615848                                l2   \n",
      "20                    1.623777                                l1   \n",
      "21                    1.623777                                l2   \n",
      "22                    4.281332                                l1   \n",
      "23                    4.281332                                l2   \n",
      "24                   11.288379                                l1   \n",
      "25                   11.288379                                l2   \n",
      "26                   29.763514                                l1   \n",
      "27                   29.763514                                l2   \n",
      "28                   78.475997                                l1   \n",
      "29                   78.475997                                l2   \n",
      "30                  206.913808                                l1   \n",
      "31                  206.913808                                l2   \n",
      "32                  545.559478                                l1   \n",
      "33                  545.559478                                l2   \n",
      "34                 1438.449888                                l1   \n",
      "35                 1438.449888                                l2   \n",
      "36                 3792.690191                                l1   \n",
      "37                 3792.690191                                l2   \n",
      "38                     10000.0                                l1   \n",
      "39                     10000.0                                l2   \n",
      "\n",
      "   param_logisticregression__solver  \\\n",
      "0                         liblinear   \n",
      "1                         liblinear   \n",
      "2                         liblinear   \n",
      "3                         liblinear   \n",
      "4                         liblinear   \n",
      "5                         liblinear   \n",
      "6                         liblinear   \n",
      "7                         liblinear   \n",
      "8                         liblinear   \n",
      "9                         liblinear   \n",
      "10                        liblinear   \n",
      "11                        liblinear   \n",
      "12                        liblinear   \n",
      "13                        liblinear   \n",
      "14                        liblinear   \n",
      "15                        liblinear   \n",
      "16                        liblinear   \n",
      "17                        liblinear   \n",
      "18                        liblinear   \n",
      "19                        liblinear   \n",
      "20                        liblinear   \n",
      "21                        liblinear   \n",
      "22                        liblinear   \n",
      "23                        liblinear   \n",
      "24                        liblinear   \n",
      "25                        liblinear   \n",
      "26                        liblinear   \n",
      "27                        liblinear   \n",
      "28                        liblinear   \n",
      "29                        liblinear   \n",
      "30                        liblinear   \n",
      "31                        liblinear   \n",
      "32                        liblinear   \n",
      "33                        liblinear   \n",
      "34                        liblinear   \n",
      "35                        liblinear   \n",
      "36                        liblinear   \n",
      "37                        liblinear   \n",
      "38                        liblinear   \n",
      "39                        liblinear   \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "0   {'logisticregression__C': 0.0001, 'logisticreg...           0.460504   \n",
      "1   {'logisticregression__C': 0.0001, 'logisticreg...           0.500467   \n",
      "2   {'logisticregression__C': 0.000263665089873035...           0.460504   \n",
      "3   {'logisticregression__C': 0.000263665089873035...           0.534238   \n",
      "4   {'logisticregression__C': 0.000695192796177560...           0.460504   \n",
      "5   {'logisticregression__C': 0.000695192796177560...           0.595941   \n",
      "6   {'logisticregression__C': 0.001832980710832435...           0.460504   \n",
      "7   {'logisticregression__C': 0.001832980710832435...           0.633461   \n",
      "8   {'logisticregression__C': 0.004832930238571752...           0.460504   \n",
      "9   {'logisticregression__C': 0.004832930238571752...           0.655467   \n",
      "10  {'logisticregression__C': 0.012742749857031334...           0.460504   \n",
      "11  {'logisticregression__C': 0.012742749857031334...           0.627163   \n",
      "12  {'logisticregression__C': 0.03359818286283781,...           0.531402   \n",
      "13  {'logisticregression__C': 0.03359818286283781,...           0.589695   \n",
      "14  {'logisticregression__C': 0.08858667904100823,...           0.619730   \n",
      "15  {'logisticregression__C': 0.08858667904100823,...           0.558715   \n",
      "16  {'logisticregression__C': 0.23357214690901212,...           0.613349   \n",
      "17  {'logisticregression__C': 0.23357214690901212,...           0.536772   \n",
      "18  {'logisticregression__C': 0.615848211066026, '...           0.573343   \n",
      "19  {'logisticregression__C': 0.615848211066026, '...           0.498615   \n",
      "20  {'logisticregression__C': 1.623776739188721, '...           0.481444   \n",
      "21  {'logisticregression__C': 1.623776739188721, '...           0.488803   \n",
      "22  {'logisticregression__C': 4.281332398719396, '...           0.487321   \n",
      "23  {'logisticregression__C': 4.281332398719396, '...           0.498568   \n",
      "24  {'logisticregression__C': 11.288378916846883, ...           0.474744   \n",
      "25  {'logisticregression__C': 11.288378916846883, ...           0.518981   \n",
      "26  {'logisticregression__C': 29.763514416313132, ...           0.151263   \n",
      "27  {'logisticregression__C': 29.763514416313132, ...           0.538044   \n",
      "28  {'logisticregression__C': 78.47599703514607, '...           0.127717   \n",
      "29  {'logisticregression__C': 78.47599703514607, '...           0.493246   \n",
      "30  {'logisticregression__C': 206.913808111479, 'l...           0.167094   \n",
      "31  {'logisticregression__C': 206.913808111479, 'l...           0.364245   \n",
      "32  {'logisticregression__C': 545.5594781168514, '...           0.289278   \n",
      "33  {'logisticregression__C': 545.5594781168514, '...           0.299561   \n",
      "34  {'logisticregression__C': 1438.44988828766, 'l...           0.309590   \n",
      "35  {'logisticregression__C': 1438.44988828766, 'l...           0.286161   \n",
      "36  {'logisticregression__C': 3792.690190732246, '...           0.304621   \n",
      "37  {'logisticregression__C': 3792.690190732246, '...           0.271000   \n",
      "38  {'logisticregression__C': 10000.0, 'logisticre...           0.303128   \n",
      "39  {'logisticregression__C': 10000.0, 'logisticre...           0.298084   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
      "0            0.431507           0.452471           0.458438         0.450730   \n",
      "1            0.699275           0.701299           0.505747         0.601697   \n",
      "2            0.431507           0.452471           0.458438         0.450730   \n",
      "3            0.658717           0.702004           0.529947         0.606227   \n",
      "4            0.431507           0.452471           0.458438         0.450730   \n",
      "5            0.603071           0.694854           0.583201         0.619267   \n",
      "6            0.431507           0.452471           0.458438         0.450730   \n",
      "7            0.520523           0.684799           0.666595         0.626344   \n",
      "8            0.431507           0.452471           0.458438         0.450730   \n",
      "9            0.520523           0.619384           0.655940         0.612829   \n",
      "10           0.431507           0.452471           0.458438         0.450730   \n",
      "11           0.520523           0.582821           0.636575         0.591771   \n",
      "12           0.482102           0.472230           0.689057         0.543698   \n",
      "13           0.603071           0.607492           0.636575         0.609208   \n",
      "14           0.482102           0.609815           0.695509         0.601789   \n",
      "15           0.602871           0.617684           0.624126         0.600849   \n",
      "16           0.543956           0.635700           0.674538         0.616886   \n",
      "17           0.593137           0.597615           0.615697         0.585806   \n",
      "18           0.634821           0.624968           0.655940         0.622268   \n",
      "19           0.593137           0.584650           0.616414         0.573204   \n",
      "20           0.623178           0.534180           0.586684         0.556372   \n",
      "21           0.623178           0.548240           0.612812         0.568258   \n",
      "22           0.633824           0.523853           0.555516         0.550128   \n",
      "23           0.633824           0.530118           0.610422         0.568233   \n",
      "24           0.603071           0.513671           0.448960         0.510111   \n",
      "25           0.633824           0.507835           0.491197         0.537959   \n",
      "26           0.579803           0.505251           0.456638         0.423239   \n",
      "27           0.613354           0.514677           0.441735         0.526952   \n",
      "28           0.579803           0.516795           0.449205         0.418380   \n",
      "29           0.579803           0.508600           0.434223         0.503968   \n",
      "30           0.579803           0.511197           0.437984         0.424020   \n",
      "31           0.579803           0.510535           0.434223         0.472202   \n",
      "32           0.579803           0.498905           0.437984         0.451493   \n",
      "33           0.579803           0.501935           0.434223         0.453881   \n",
      "34           0.579803           0.495818           0.437984         0.455799   \n",
      "35           0.579803           0.504578           0.437984         0.452131   \n",
      "36           0.579803           0.495818           0.430451         0.452673   \n",
      "37           0.579803           0.519057           0.445475         0.453834   \n",
      "38           0.579803           0.490976           0.430451         0.451089   \n",
      "39           0.579803           0.516652           0.445475         0.460003   \n",
      "\n",
      "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
      "0         0.011484               32            0.452038            0.456991   \n",
      "1         0.098610                9            0.663299            0.668581   \n",
      "2         0.011484               32            0.452038            0.456991   \n",
      "3         0.075713                7            0.692129            0.696855   \n",
      "4         0.011484               32            0.452038            0.456991   \n",
      "5         0.044217                3            0.736675            0.721285   \n",
      "6         0.011484               32            0.452038            0.456991   \n",
      "7         0.063808                1            0.764173            0.722125   \n",
      "8         0.011484               32            0.452038            0.456991   \n",
      "9         0.055317                5            0.715279            0.684055   \n",
      "10        0.011484               32            0.452038            0.456991   \n",
      "11        0.045871               11            0.715265            0.691050   \n",
      "12        0.086865               18            0.542159            0.589057   \n",
      "13        0.017105                6            0.732881            0.713529   \n",
      "14        0.076640                8            0.685838            0.685196   \n",
      "15        0.025518               10            0.760355            0.728086   \n",
      "16        0.047458                4            0.758891            0.723227   \n",
      "17        0.029542               12            0.783317            0.751226   \n",
      "18        0.030382                2            0.800463            0.750413   \n",
      "19        0.044607               13            0.803286            0.759435   \n",
      "20        0.053592               16            0.807267            0.766476   \n",
      "21        0.054118               14            0.818221            0.763424   \n",
      "22        0.054012               17            0.822414            0.768479   \n",
      "23        0.055651               15            0.817678            0.771463   \n",
      "24        0.058405               21            0.831833            0.776321   \n",
      "25        0.056224               19            0.822414            0.773420   \n",
      "26        0.163037               39            0.837871            0.770477   \n",
      "27        0.061239               20            0.831833            0.773420   \n",
      "28        0.174053               40            0.839502            0.772434   \n",
      "29        0.051845               22            0.835333            0.770477   \n",
      "30        0.156584               38            0.837871            0.777312   \n",
      "31        0.080846               23            0.839502            0.773420   \n",
      "32        0.106310               30            0.837871            0.777312   \n",
      "33        0.102916               26            0.837871            0.776321   \n",
      "34        0.098327               25            0.837871            0.777312   \n",
      "35        0.108163               29            0.837871            0.777312   \n",
      "36        0.100544               28            0.837871            0.777312   \n",
      "37        0.115780               27            0.837871            0.776321   \n",
      "38        0.100594               31            0.837871            0.777312   \n",
      "39        0.104869               24            0.837871            0.776321   \n",
      "\n",
      "    split2_train_score  split3_train_score  mean_train_score  std_train_score  \n",
      "0             0.457588            0.455193          0.455452         0.002160  \n",
      "1             0.658329            0.660136          0.662586         0.003891  \n",
      "2             0.457588            0.455193          0.455452         0.002160  \n",
      "3             0.681267            0.691599          0.690463         0.005690  \n",
      "4             0.457588            0.455193          0.455452         0.002160  \n",
      "5             0.716348            0.727644          0.725488         0.007599  \n",
      "6             0.457588            0.455193          0.455452         0.002160  \n",
      "7             0.750941            0.731454          0.742173         0.016414  \n",
      "8             0.457588            0.455193          0.455452         0.002160  \n",
      "9             0.709889            0.711917          0.705285         0.012407  \n",
      "10            0.457588            0.455193          0.455452         0.002160  \n",
      "11            0.712317            0.712297          0.707732         0.009707  \n",
      "12            0.521100            0.586252          0.559642         0.029002  \n",
      "13            0.711720            0.722959          0.720272         0.008438  \n",
      "14            0.631284            0.684479          0.671699         0.023338  \n",
      "15            0.739818            0.746462          0.743680         0.011661  \n",
      "16            0.714390            0.726965          0.730868         0.016811  \n",
      "17            0.770825            0.760319          0.766422         0.011969  \n",
      "18            0.757929            0.752679          0.765371         0.020443  \n",
      "19            0.775485            0.777371          0.778894         0.015713  \n",
      "20            0.773924            0.775229          0.780724         0.015684  \n",
      "21            0.790218            0.782493          0.788589         0.019692  \n",
      "22            0.791562            0.790618          0.793268         0.019196  \n",
      "23            0.792886            0.790618          0.793161         0.016420  \n",
      "24            0.798273            0.791180          0.799402         0.020331  \n",
      "25            0.805020            0.791710          0.798141         0.017951  \n",
      "26            0.802275            0.794029          0.801163         0.024193  \n",
      "27            0.801058            0.789203          0.798878         0.021404  \n",
      "28            0.802275            0.792072          0.801571         0.024385  \n",
      "29            0.801058            0.791180          0.799512         0.023441  \n",
      "30            0.802275            0.792072          0.802382         0.022329  \n",
      "31            0.802275            0.790106          0.801326         0.024305  \n",
      "32            0.802275            0.792072          0.802382         0.022329  \n",
      "33            0.802275            0.792072          0.802135         0.022609  \n",
      "34            0.802275            0.792072          0.802382         0.022329  \n",
      "35            0.802275            0.792072          0.802382         0.022329  \n",
      "36            0.802275            0.793148          0.802652         0.022209  \n",
      "37            0.802275            0.792072          0.802135         0.022609  \n",
      "38            0.802275            0.793148          0.802652         0.022209  \n",
      "39            0.802275            0.792072          0.802135         0.022609  \n",
      "y_test_pred: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0\n",
      " 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 1 0]\n",
      "test score: 0.7120867727399609\n",
      "logisticregression\n",
      "y_test shape: (380,)\n",
      "0.1383219954648526\n",
      "(1516, 109) (1516,)\n",
      "Fitting 4 folds for each of 40 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0        0.008153      0.000430         0.002784        0.000171   \n",
      "1        0.009464      0.000417         0.002888        0.000121   \n",
      "2        0.007854      0.000675         0.002913        0.000121   \n",
      "3        0.010332      0.000764         0.002911        0.000135   \n",
      "4        0.007791      0.000544         0.002822        0.000108   \n",
      "5        0.013244      0.004427         0.003718        0.001328   \n",
      "6        0.007880      0.000553         0.002910        0.000075   \n",
      "7        0.012706      0.001794         0.002902        0.000159   \n",
      "8        0.008582      0.001195         0.002836        0.000097   \n",
      "9        0.015818      0.002871         0.002916        0.000166   \n",
      "10       0.009838      0.001162         0.002930        0.000131   \n",
      "11       0.016933      0.002576         0.002921        0.000180   \n",
      "12       0.010754      0.001204         0.002872        0.000118   \n",
      "13       0.021716      0.005043         0.002904        0.000191   \n",
      "14       0.013660      0.001276         0.002920        0.000165   \n",
      "15       0.027745      0.005312         0.003435        0.000834   \n",
      "16       0.022106      0.006147         0.004661        0.002281   \n",
      "17       0.033795      0.009846         0.003530        0.001301   \n",
      "18       0.106221      0.045105         0.003168        0.000328   \n",
      "19       0.041211      0.007966         0.003099        0.000195   \n",
      "20       0.335072      0.119247         0.002968        0.000104   \n",
      "21       0.048583      0.001341         0.003314        0.000722   \n",
      "22       0.996889      0.333160         0.003992        0.001871   \n",
      "23       0.066480      0.005325         0.004129        0.002159   \n",
      "24       2.405113      0.498735         0.005616        0.003408   \n",
      "25       0.085599      0.024811         0.003845        0.001638   \n",
      "26       3.756484      0.714881         0.002934        0.000131   \n",
      "27       0.111714      0.012247         0.006003        0.002762   \n",
      "28       5.061193      1.852450         0.003530        0.000831   \n",
      "29       0.156125      0.026164         0.002972        0.000129   \n",
      "30       4.337711      1.507518         0.003068        0.000124   \n",
      "31       0.173072      0.045009         0.002931        0.000154   \n",
      "32       4.375798      1.525181         0.002978        0.000153   \n",
      "33       0.187586      0.044800         0.002939        0.000155   \n",
      "34       3.817416      0.744159         0.002959        0.000126   \n",
      "35       0.186277      0.040816         0.002941        0.000125   \n",
      "36       3.999586      0.767747         0.003170        0.000231   \n",
      "37       0.198335      0.025548         0.004519        0.002882   \n",
      "38       5.561603      1.779350         0.002981        0.000158   \n",
      "39       0.194536      0.031051         0.004063        0.001626   \n",
      "\n",
      "   param_logisticregression__C param_logisticregression__penalty  \\\n",
      "0                       0.0001                                l1   \n",
      "1                       0.0001                                l2   \n",
      "2                     0.000264                                l1   \n",
      "3                     0.000264                                l2   \n",
      "4                     0.000695                                l1   \n",
      "5                     0.000695                                l2   \n",
      "6                     0.001833                                l1   \n",
      "7                     0.001833                                l2   \n",
      "8                     0.004833                                l1   \n",
      "9                     0.004833                                l2   \n",
      "10                    0.012743                                l1   \n",
      "11                    0.012743                                l2   \n",
      "12                    0.033598                                l1   \n",
      "13                    0.033598                                l2   \n",
      "14                    0.088587                                l1   \n",
      "15                    0.088587                                l2   \n",
      "16                    0.233572                                l1   \n",
      "17                    0.233572                                l2   \n",
      "18                    0.615848                                l1   \n",
      "19                    0.615848                                l2   \n",
      "20                    1.623777                                l1   \n",
      "21                    1.623777                                l2   \n",
      "22                    4.281332                                l1   \n",
      "23                    4.281332                                l2   \n",
      "24                   11.288379                                l1   \n",
      "25                   11.288379                                l2   \n",
      "26                   29.763514                                l1   \n",
      "27                   29.763514                                l2   \n",
      "28                   78.475997                                l1   \n",
      "29                   78.475997                                l2   \n",
      "30                  206.913808                                l1   \n",
      "31                  206.913808                                l2   \n",
      "32                  545.559478                                l1   \n",
      "33                  545.559478                                l2   \n",
      "34                 1438.449888                                l1   \n",
      "35                 1438.449888                                l2   \n",
      "36                 3792.690191                                l1   \n",
      "37                 3792.690191                                l2   \n",
      "38                     10000.0                                l1   \n",
      "39                     10000.0                                l2   \n",
      "\n",
      "   param_logisticregression__solver  \\\n",
      "0                         liblinear   \n",
      "1                         liblinear   \n",
      "2                         liblinear   \n",
      "3                         liblinear   \n",
      "4                         liblinear   \n",
      "5                         liblinear   \n",
      "6                         liblinear   \n",
      "7                         liblinear   \n",
      "8                         liblinear   \n",
      "9                         liblinear   \n",
      "10                        liblinear   \n",
      "11                        liblinear   \n",
      "12                        liblinear   \n",
      "13                        liblinear   \n",
      "14                        liblinear   \n",
      "15                        liblinear   \n",
      "16                        liblinear   \n",
      "17                        liblinear   \n",
      "18                        liblinear   \n",
      "19                        liblinear   \n",
      "20                        liblinear   \n",
      "21                        liblinear   \n",
      "22                        liblinear   \n",
      "23                        liblinear   \n",
      "24                        liblinear   \n",
      "25                        liblinear   \n",
      "26                        liblinear   \n",
      "27                        liblinear   \n",
      "28                        liblinear   \n",
      "29                        liblinear   \n",
      "30                        liblinear   \n",
      "31                        liblinear   \n",
      "32                        liblinear   \n",
      "33                        liblinear   \n",
      "34                        liblinear   \n",
      "35                        liblinear   \n",
      "36                        liblinear   \n",
      "37                        liblinear   \n",
      "38                        liblinear   \n",
      "39                        liblinear   \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "0   {'logisticregression__C': 0.0001, 'logisticreg...           0.457699   \n",
      "1   {'logisticregression__C': 0.0001, 'logisticreg...           0.573597   \n",
      "2   {'logisticregression__C': 0.000263665089873035...           0.457699   \n",
      "3   {'logisticregression__C': 0.000263665089873035...           0.609958   \n",
      "4   {'logisticregression__C': 0.000695192796177560...           0.457699   \n",
      "5   {'logisticregression__C': 0.000695192796177560...           0.650337   \n",
      "6   {'logisticregression__C': 0.001832980710832435...           0.457699   \n",
      "7   {'logisticregression__C': 0.001832980710832435...           0.685201   \n",
      "8   {'logisticregression__C': 0.004832930238571752...           0.457699   \n",
      "9   {'logisticregression__C': 0.004832930238571752...           0.637597   \n",
      "10  {'logisticregression__C': 0.012742749857031334...           0.457699   \n",
      "11  {'logisticregression__C': 0.012742749857031334...           0.563421   \n",
      "12  {'logisticregression__C': 0.03359818286283781,...           0.525846   \n",
      "13  {'logisticregression__C': 0.03359818286283781,...           0.571982   \n",
      "14  {'logisticregression__C': 0.08858667904100823,...           0.591757   \n",
      "15  {'logisticregression__C': 0.08858667904100823,...           0.552353   \n",
      "16  {'logisticregression__C': 0.23357214690901212,...           0.597234   \n",
      "17  {'logisticregression__C': 0.23357214690901212,...           0.504418   \n",
      "18  {'logisticregression__C': 0.615848211066026, '...           0.500894   \n",
      "19  {'logisticregression__C': 0.615848211066026, '...           0.482194   \n",
      "20  {'logisticregression__C': 1.623776739188721, '...           0.467987   \n",
      "21  {'logisticregression__C': 1.623776739188721, '...           0.486515   \n",
      "22  {'logisticregression__C': 4.281332398719396, '...           0.467987   \n",
      "23  {'logisticregression__C': 4.281332398719396, '...           0.467987   \n",
      "24  {'logisticregression__C': 11.288378916846883, ...           0.482904   \n",
      "25  {'logisticregression__C': 11.288378916846883, ...           0.467987   \n",
      "26  {'logisticregression__C': 29.763514416313132, ...           0.535131   \n",
      "27  {'logisticregression__C': 29.763514416313132, ...           0.457699   \n",
      "28  {'logisticregression__C': 78.47599703514607, '...           0.482194   \n",
      "29  {'logisticregression__C': 78.47599703514607, '...           0.457699   \n",
      "30  {'logisticregression__C': 206.913808111479, 'l...           0.457699   \n",
      "31  {'logisticregression__C': 206.913808111479, 'l...           0.457699   \n",
      "32  {'logisticregression__C': 545.5594781168514, '...           0.457699   \n",
      "33  {'logisticregression__C': 545.5594781168514, '...           0.457699   \n",
      "34  {'logisticregression__C': 1438.44988828766, 'l...           0.457699   \n",
      "35  {'logisticregression__C': 1438.44988828766, 'l...           0.457699   \n",
      "36  {'logisticregression__C': 3792.690190732246, '...           0.457699   \n",
      "37  {'logisticregression__C': 3792.690190732246, '...           0.457699   \n",
      "38  {'logisticregression__C': 10000.0, 'logisticre...           0.457699   \n",
      "39  {'logisticregression__C': 10000.0, 'logisticre...           0.467987   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
      "0            0.446154           0.442029           0.446154         0.448009   \n",
      "1            0.522932           0.629259           0.522932         0.562180   \n",
      "2            0.446154           0.442029           0.446154         0.448009   \n",
      "3            0.556164           0.733129           0.556164         0.613854   \n",
      "4            0.446154           0.442029           0.446154         0.448009   \n",
      "5            0.599860           0.660294           0.599860         0.627588   \n",
      "6            0.446154           0.442029           0.446154         0.448009   \n",
      "7            0.655911           0.605128           0.655911         0.650538   \n",
      "8            0.446154           0.442029           0.446154         0.448009   \n",
      "9            0.673314           0.575758           0.673314         0.639996   \n",
      "10           0.446154           0.442029           0.446154         0.448009   \n",
      "11           0.640156           0.565549           0.640156         0.602321   \n",
      "12           0.665945           0.559641           0.665945         0.604344   \n",
      "13           0.644412           0.607143           0.644412         0.616987   \n",
      "14           0.641989           0.597761           0.641989         0.618374   \n",
      "15           0.649532           0.616313           0.649532         0.616932   \n",
      "16           0.644412           0.629808           0.644412         0.628966   \n",
      "17           0.632984           0.633609           0.632984         0.600999   \n",
      "18           0.635959           0.660294           0.635959         0.608277   \n",
      "19           0.634405           0.556474           0.634405         0.576869   \n",
      "20           0.585678           0.611765           0.585678         0.562777   \n",
      "21           0.598524           0.575924           0.598524         0.564872   \n",
      "22           0.563443           0.575924           0.563443         0.542699   \n",
      "23           0.567091           0.586022           0.567091         0.547048   \n",
      "24           0.457143           0.586022           0.457143         0.495803   \n",
      "25           0.468794           0.553931           0.468794         0.489876   \n",
      "26           0.431413           0.586022           0.431413         0.495995   \n",
      "27           0.450912           0.553931           0.450912         0.478363   \n",
      "28           0.435065           0.586022           0.435065         0.484586   \n",
      "29           0.454869           0.553931           0.454869         0.480342   \n",
      "30           0.438707           0.586022           0.432557         0.478746   \n",
      "31           0.445622           0.586022           0.445622         0.483741   \n",
      "32           0.432557           0.586022           0.433485         0.477441   \n",
      "33           0.435065           0.586022           0.435065         0.478463   \n",
      "34           0.438707           0.586022           0.435065         0.479373   \n",
      "35           0.429924           0.586022           0.429924         0.475892   \n",
      "36           0.438707           0.586022           0.438707         0.480284   \n",
      "37           0.431211           0.586022           0.431211         0.476536   \n",
      "38           0.427742           0.586022           0.431211         0.475668   \n",
      "39           0.431211           0.586022           0.431211         0.479108   \n",
      "\n",
      "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
      "0         0.005842               35            0.449339            0.454011   \n",
      "1         0.043905               16            0.664673            0.669782   \n",
      "2         0.005842               35            0.449339            0.454011   \n",
      "3         0.072281                8            0.691223            0.696356   \n",
      "4         0.005842               35            0.449339            0.454011   \n",
      "5         0.027950                4            0.720168            0.721071   \n",
      "6         0.005842               35            0.449339            0.454011   \n",
      "7         0.028816                1            0.748385            0.725717   \n",
      "8         0.005842               35            0.449339            0.454011   \n",
      "9         0.039851                2            0.753401            0.726281   \n",
      "10        0.005842               35            0.449339            0.454011   \n",
      "11        0.037843               11            0.742514            0.727587   \n",
      "12        0.062749               10            0.560852            0.607916   \n",
      "13        0.030111                6            0.747031            0.737386   \n",
      "14        0.023710                5            0.710364            0.696014   \n",
      "15        0.039675                7            0.776487            0.757051   \n",
      "16        0.019266                3            0.753023            0.727285   \n",
      "17        0.055761               12            0.789524            0.767870   \n",
      "18        0.062788                9            0.781431            0.761510   \n",
      "19        0.063246               13            0.801183            0.780114   \n",
      "20        0.055753               15            0.807376            0.773964   \n",
      "21        0.046170               14            0.807376            0.783064   \n",
      "22        0.043435               18            0.818391            0.789903   \n",
      "23        0.046295               17            0.816872            0.790971   \n",
      "24        0.053139               20            0.817245            0.789365   \n",
      "25        0.036983               21            0.815361            0.791665   \n",
      "26        0.067041               19            0.817245            0.790227   \n",
      "27        0.043717               30            0.815748            0.790416   \n",
      "28        0.061643               22            0.813315            0.797199   \n",
      "29        0.042502               24            0.815748            0.790227   \n",
      "30        0.062625               28            0.813315            0.799091   \n",
      "31        0.059257               23            0.813315            0.790416   \n",
      "32        0.063494               31            0.813315            0.800975   \n",
      "33        0.062783               29            0.813315            0.797199   \n",
      "34        0.062170               26            0.813315            0.800975   \n",
      "35        0.064587               33            0.813315            0.802045   \n",
      "36        0.061538               25            0.813315            0.800975   \n",
      "37        0.064130               32            0.813315            0.802045   \n",
      "38        0.064757               34            0.813315            0.802045   \n",
      "39        0.063526               27            0.813315            0.802045   \n",
      "\n",
      "    split2_train_score  split3_train_score  mean_train_score  std_train_score  \n",
      "0             0.453475            0.454011          0.452709         0.001958  \n",
      "1             0.667134            0.669782          0.667843         0.002126  \n",
      "2             0.453475            0.454011          0.452709         0.001958  \n",
      "3             0.686769            0.696356          0.692676         0.004003  \n",
      "4             0.453475            0.454011          0.452709         0.001958  \n",
      "5             0.717399            0.721071          0.719927         0.001505  \n",
      "6             0.453475            0.454011          0.452709         0.001958  \n",
      "7             0.718390            0.725717          0.729552         0.011277  \n",
      "8             0.453475            0.454011          0.452709         0.001958  \n",
      "9             0.723199            0.726281          0.732290         0.012253  \n",
      "10            0.453475            0.454011          0.452709         0.001958  \n",
      "11            0.712904            0.727587          0.727648         0.010469  \n",
      "12            0.597073            0.607916          0.593439         0.019328  \n",
      "13            0.731359            0.737386          0.738291         0.005615  \n",
      "14            0.676487            0.696014          0.694719         0.012047  \n",
      "15            0.747251            0.757051          0.759460         0.010614  \n",
      "16            0.726224            0.727285          0.733454         0.011306  \n",
      "17            0.743729            0.767870          0.767248         0.016203  \n",
      "18            0.744021            0.761510          0.762118         0.013240  \n",
      "19            0.748798            0.780114          0.777552         0.018697  \n",
      "20            0.745184            0.773964          0.775122         0.022019  \n",
      "21            0.764897            0.783064          0.784600         0.015097  \n",
      "22            0.769489            0.789903          0.791922         0.017407  \n",
      "23            0.769523            0.790971          0.792084         0.016778  \n",
      "24            0.764882            0.789365          0.790214         0.018533  \n",
      "25            0.770382            0.791665          0.792268         0.015914  \n",
      "26            0.765787            0.790227          0.790872         0.018205  \n",
      "27            0.766708            0.790416          0.790822         0.017343  \n",
      "28            0.765787            0.797199          0.793375         0.017233  \n",
      "29            0.767603            0.790227          0.790952         0.017037  \n",
      "30            0.765787            0.800159          0.794588         0.017546  \n",
      "31            0.767603            0.790416          0.790438         0.016162  \n",
      "32            0.765787            0.802045          0.795531         0.017840  \n",
      "33            0.765787            0.797199          0.793375         0.017233  \n",
      "34            0.765787            0.800975          0.795263         0.017748  \n",
      "35            0.765787            0.802045          0.795798         0.017927  \n",
      "36            0.765787            0.800975          0.795263         0.017748  \n",
      "37            0.764869            0.802045          0.795569         0.018312  \n",
      "38            0.765787            0.802045          0.795798         0.017927  \n",
      "39            0.764869            0.802045          0.795569         0.018312  \n",
      "y_test_pred: [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1\n",
      " 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 0 1 0 0 0 0]\n",
      "test score: 0.738205467372134\n",
      "logisticregression\n",
      "y_test shape: (380,)\n",
      "0.15929203539823011\n",
      "(1516, 109) (1516,)\n",
      "Fitting 4 folds for each of 40 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0        0.007637      0.000416         0.002971        0.000148   \n",
      "1        0.008716      0.000427         0.003067        0.000115   \n",
      "2        0.007173      0.000499         0.003001        0.000100   \n",
      "3        0.009398      0.000792         0.003048        0.000101   \n",
      "4        0.007794      0.000935         0.002957        0.000083   \n",
      "5        0.009635      0.000605         0.003018        0.000087   \n",
      "6        0.007020      0.000330         0.003003        0.000130   \n",
      "7        0.011782      0.001309         0.003065        0.000212   \n",
      "8        0.007613      0.000612         0.003001        0.000130   \n",
      "9        0.014704      0.002451         0.003087        0.000267   \n",
      "10       0.008423      0.000743         0.002980        0.000175   \n",
      "11       0.014694      0.001983         0.003041        0.000096   \n",
      "12       0.009244      0.000931         0.002957        0.000182   \n",
      "13       0.019180      0.002386         0.002979        0.000163   \n",
      "14       0.011335      0.001745         0.002958        0.000099   \n",
      "15       0.023097      0.004682         0.005323        0.002405   \n",
      "16       0.015883      0.002496         0.002972        0.000100   \n",
      "17       0.024853      0.003427         0.003083        0.000150   \n",
      "18       0.071155      0.036952         0.002993        0.000075   \n",
      "19       0.036282      0.003712         0.003096        0.000193   \n",
      "20       0.248991      0.119027         0.003073        0.000152   \n",
      "21       0.043875      0.011148         0.002936        0.000116   \n",
      "22       0.641190      0.157197         0.003092        0.000110   \n",
      "23       0.061563      0.008303         0.003034        0.000073   \n",
      "24       1.880857      0.391851         0.006833        0.002731   \n",
      "25       0.070861      0.010874         0.004144        0.002064   \n",
      "26       3.614869      0.918739         0.003218        0.000240   \n",
      "27       0.095029      0.016077         0.003274        0.000443   \n",
      "28       5.290072      0.975224         0.003471        0.000683   \n",
      "29       0.112552      0.016937         0.003011        0.000115   \n",
      "30       4.714863      1.209021         0.003632        0.000582   \n",
      "31       0.143012      0.019301         0.003010        0.000110   \n",
      "32       5.989877      1.817591         0.003383        0.000569   \n",
      "33       0.188281      0.022759         0.004245        0.002197   \n",
      "34       4.520162      0.879651         0.003350        0.000491   \n",
      "35       0.220818      0.028656         0.003987        0.001724   \n",
      "36       5.244937      1.362532         0.003252        0.000371   \n",
      "37       0.209094      0.005911         0.003130        0.000174   \n",
      "38       5.424753      1.467125         0.003595        0.000638   \n",
      "39       0.228115      0.023391         0.003002        0.000040   \n",
      "\n",
      "   param_logisticregression__C param_logisticregression__penalty  \\\n",
      "0                       0.0001                                l1   \n",
      "1                       0.0001                                l2   \n",
      "2                     0.000264                                l1   \n",
      "3                     0.000264                                l2   \n",
      "4                     0.000695                                l1   \n",
      "5                     0.000695                                l2   \n",
      "6                     0.001833                                l1   \n",
      "7                     0.001833                                l2   \n",
      "8                     0.004833                                l1   \n",
      "9                     0.004833                                l2   \n",
      "10                    0.012743                                l1   \n",
      "11                    0.012743                                l2   \n",
      "12                    0.033598                                l1   \n",
      "13                    0.033598                                l2   \n",
      "14                    0.088587                                l1   \n",
      "15                    0.088587                                l2   \n",
      "16                    0.233572                                l1   \n",
      "17                    0.233572                                l2   \n",
      "18                    0.615848                                l1   \n",
      "19                    0.615848                                l2   \n",
      "20                    1.623777                                l1   \n",
      "21                    1.623777                                l2   \n",
      "22                    4.281332                                l1   \n",
      "23                    4.281332                                l2   \n",
      "24                   11.288379                                l1   \n",
      "25                   11.288379                                l2   \n",
      "26                   29.763514                                l1   \n",
      "27                   29.763514                                l2   \n",
      "28                   78.475997                                l1   \n",
      "29                   78.475997                                l2   \n",
      "30                  206.913808                                l1   \n",
      "31                  206.913808                                l2   \n",
      "32                  545.559478                                l1   \n",
      "33                  545.559478                                l2   \n",
      "34                 1438.449888                                l1   \n",
      "35                 1438.449888                                l2   \n",
      "36                 3792.690191                                l1   \n",
      "37                 3792.690191                                l2   \n",
      "38                     10000.0                                l1   \n",
      "39                     10000.0                                l2   \n",
      "\n",
      "   param_logisticregression__solver  \\\n",
      "0                         liblinear   \n",
      "1                         liblinear   \n",
      "2                         liblinear   \n",
      "3                         liblinear   \n",
      "4                         liblinear   \n",
      "5                         liblinear   \n",
      "6                         liblinear   \n",
      "7                         liblinear   \n",
      "8                         liblinear   \n",
      "9                         liblinear   \n",
      "10                        liblinear   \n",
      "11                        liblinear   \n",
      "12                        liblinear   \n",
      "13                        liblinear   \n",
      "14                        liblinear   \n",
      "15                        liblinear   \n",
      "16                        liblinear   \n",
      "17                        liblinear   \n",
      "18                        liblinear   \n",
      "19                        liblinear   \n",
      "20                        liblinear   \n",
      "21                        liblinear   \n",
      "22                        liblinear   \n",
      "23                        liblinear   \n",
      "24                        liblinear   \n",
      "25                        liblinear   \n",
      "26                        liblinear   \n",
      "27                        liblinear   \n",
      "28                        liblinear   \n",
      "29                        liblinear   \n",
      "30                        liblinear   \n",
      "31                        liblinear   \n",
      "32                        liblinear   \n",
      "33                        liblinear   \n",
      "34                        liblinear   \n",
      "35                        liblinear   \n",
      "36                        liblinear   \n",
      "37                        liblinear   \n",
      "38                        liblinear   \n",
      "39                        liblinear   \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "0   {'logisticregression__C': 0.0001, 'logisticreg...           0.459550   \n",
      "1   {'logisticregression__C': 0.0001, 'logisticreg...           0.506491   \n",
      "2   {'logisticregression__C': 0.000263665089873035...           0.459550   \n",
      "3   {'logisticregression__C': 0.000263665089873035...           0.551192   \n",
      "4   {'logisticregression__C': 0.000695192796177560...           0.459550   \n",
      "5   {'logisticregression__C': 0.000695192796177560...           0.624229   \n",
      "6   {'logisticregression__C': 0.001832980710832435...           0.459550   \n",
      "7   {'logisticregression__C': 0.001832980710832435...           0.676020   \n",
      "8   {'logisticregression__C': 0.004832930238571752...           0.459550   \n",
      "9   {'logisticregression__C': 0.004832930238571752...           0.627658   \n",
      "10  {'logisticregression__C': 0.012742749857031334...           0.459550   \n",
      "11  {'logisticregression__C': 0.012742749857031334...           0.601102   \n",
      "12  {'logisticregression__C': 0.03359818286283781,...           0.536499   \n",
      "13  {'logisticregression__C': 0.03359818286283781,...           0.583904   \n",
      "14  {'logisticregression__C': 0.08858667904100823,...           0.578022   \n",
      "15  {'logisticregression__C': 0.08858667904100823,...           0.527652   \n",
      "16  {'logisticregression__C': 0.23357214690901212,...           0.571156   \n",
      "17  {'logisticregression__C': 0.23357214690901212,...           0.497373   \n",
      "18  {'logisticregression__C': 0.615848211066026, '...           0.540816   \n",
      "19  {'logisticregression__C': 0.615848211066026, '...           0.479866   \n",
      "20  {'logisticregression__C': 1.623776739188721, '...           0.470137   \n",
      "21  {'logisticregression__C': 1.623776739188721, '...           0.469583   \n",
      "22  {'logisticregression__C': 4.281332398719396, '...           0.476011   \n",
      "23  {'logisticregression__C': 4.281332398719396, '...           0.478569   \n",
      "24  {'logisticregression__C': 11.288378916846883, ...           0.478569   \n",
      "25  {'logisticregression__C': 11.288378916846883, ...           0.476011   \n",
      "26  {'logisticregression__C': 29.763514416313132, ...           0.484973   \n",
      "27  {'logisticregression__C': 29.763514416313132, ...           0.473494   \n",
      "28  {'logisticregression__C': 78.47599703514607, '...           0.490110   \n",
      "29  {'logisticregression__C': 78.47599703514607, '...           0.478567   \n",
      "30  {'logisticregression__C': 206.913808111479, 'l...           0.488549   \n",
      "31  {'logisticregression__C': 206.913808111479, 'l...           0.477886   \n",
      "32  {'logisticregression__C': 545.5594781168514, '...           0.488549   \n",
      "33  {'logisticregression__C': 545.5594781168514, '...           0.469172   \n",
      "34  {'logisticregression__C': 1438.44988828766, 'l...           0.495554   \n",
      "35  {'logisticregression__C': 1438.44988828766, 'l...           0.469172   \n",
      "36  {'logisticregression__C': 3792.690190732246, '...           0.502343   \n",
      "37  {'logisticregression__C': 3792.690190732246, '...           0.488549   \n",
      "38  {'logisticregression__C': 10000.0, 'logisticre...           0.500604   \n",
      "39  {'logisticregression__C': 10000.0, 'logisticre...           0.503221   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
      "0            0.451948           0.454115           0.459550         0.456291   \n",
      "1            0.496841           0.693819           0.506491         0.550911   \n",
      "2            0.451948           0.454115           0.459550         0.456291   \n",
      "3            0.540734           0.701850           0.551192         0.586242   \n",
      "4            0.451948           0.454115           0.459550         0.456291   \n",
      "5            0.583040           0.708599           0.624229         0.635024   \n",
      "6            0.451948           0.454115           0.459550         0.456291   \n",
      "7            0.662544           0.702228           0.676020         0.679203   \n",
      "8            0.451948           0.454115           0.459550         0.456291   \n",
      "9            0.686730           0.677342           0.627658         0.654847   \n",
      "10           0.451948           0.454115           0.459550         0.456291   \n",
      "11           0.701977           0.629701           0.601102         0.633471   \n",
      "12           0.705888           0.494539           0.536499         0.568356   \n",
      "13           0.703664           0.633944           0.583904         0.626354   \n",
      "14           0.679476           0.632168           0.578022         0.616922   \n",
      "15           0.651374           0.621975           0.527652         0.582163   \n",
      "16           0.674594           0.662842           0.571156         0.619937   \n",
      "17           0.638268           0.615430           0.497373         0.562111   \n",
      "18           0.660439           0.618532           0.540816         0.590151   \n",
      "19           0.616298           0.557192           0.479866         0.533305   \n",
      "20           0.635159           0.525615           0.470137         0.525262   \n",
      "21           0.595429           0.547400           0.469583         0.520499   \n",
      "22           0.511771           0.512922           0.476011         0.494179   \n",
      "23           0.536171           0.518374           0.478569         0.502921   \n",
      "24           0.355088           0.507021           0.478569         0.454812   \n",
      "25           0.366881           0.502313           0.476011         0.455304   \n",
      "26           0.302754           0.500776           0.485714         0.443554   \n",
      "27           0.307668           0.510289           0.473494         0.441236   \n",
      "28           0.302754           0.509515           0.490897         0.448319   \n",
      "29           0.292822           0.512119           0.478567         0.440519   \n",
      "30           0.297805           0.505373           0.489327         0.445264   \n",
      "31           0.287804           0.503238           0.477886         0.436703   \n",
      "32           0.292822           0.506416           0.488549         0.444084   \n",
      "33           0.297805           0.490194           0.469172         0.431586   \n",
      "34           0.287804           0.503300           0.495554         0.445553   \n",
      "35           0.297805           0.505373           0.469172         0.435381   \n",
      "36           0.297805           0.504334           0.503221         0.451926   \n",
      "37           0.297805           0.502464           0.488549         0.444342   \n",
      "38           0.287804           0.504334           0.508927         0.450417   \n",
      "39           0.297805           0.503379           0.503221         0.451906   \n",
      "\n",
      "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
      "0         0.003348               19            0.451674            0.455569   \n",
      "1         0.082602               13            0.657913            0.657954   \n",
      "2         0.003348               19            0.451674            0.455569   \n",
      "3         0.066883                9            0.681301            0.689064   \n",
      "4         0.003348               19            0.451674            0.455569   \n",
      "5         0.045686                3            0.731492            0.731902   \n",
      "6         0.003348               19            0.451674            0.455569   \n",
      "7         0.014387                1            0.760296            0.738622   \n",
      "8         0.003348               19            0.451674            0.455569   \n",
      "9         0.027391                2            0.763206            0.718091   \n",
      "10        0.003348               19            0.451674            0.455569   \n",
      "11        0.041240                4            0.752334            0.702492   \n",
      "12        0.081231               11            0.574332            0.615548   \n",
      "13        0.049088                5            0.739785            0.736043   \n",
      "14        0.042343                7            0.726255            0.697429   \n",
      "15        0.055493               10            0.755483            0.739063   \n",
      "16        0.048958                6            0.763228            0.734271   \n",
      "17        0.065239               12            0.776925            0.750275   \n",
      "18        0.051511                8            0.795775            0.749229   \n",
      "19        0.057380               14            0.801045            0.758159   \n",
      "20        0.067370               15            0.811409            0.766932   \n",
      "21        0.053673               16            0.805418            0.768000   \n",
      "22        0.018172               18            0.821154            0.781817   \n",
      "23        0.025151               17            0.814751            0.782911   \n",
      "24        0.058735               26            0.840026            0.779791   \n",
      "25        0.052168               25            0.826412            0.785002   \n",
      "26        0.081535               35            0.845975            0.776471   \n",
      "27        0.078565               36            0.840026            0.779718   \n",
      "28        0.084400               30            0.857568            0.776349   \n",
      "29        0.086366               37            0.842577            0.775397   \n",
      "30        0.085399               32            0.859245            0.779477   \n",
      "31        0.086588               38            0.848478            0.776349   \n",
      "32        0.087635               34            0.859245            0.779477   \n",
      "33        0.077713               40            0.857568            0.777415   \n",
      "34        0.091131               31            0.859245            0.779477   \n",
      "35        0.080792               39            0.859245            0.779477   \n",
      "36        0.088984               27            0.859245            0.779477   \n",
      "37        0.084793               33            0.859245            0.779477   \n",
      "38        0.093931               29            0.859245            0.779477   \n",
      "39        0.088970               28            0.859245            0.779477   \n",
      "\n",
      "    split2_train_score  split3_train_score  mean_train_score  std_train_score  \n",
      "0             0.455652            0.451674          0.453642         0.001969  \n",
      "1             0.650859            0.657913          0.656160         0.003061  \n",
      "2             0.455652            0.451674          0.453642         0.001969  \n",
      "3             0.664577            0.681301          0.679061         0.008943  \n",
      "4             0.455652            0.451674          0.453642         0.001969  \n",
      "5             0.708854            0.731492          0.725935         0.009863  \n",
      "6             0.455652            0.451674          0.453642         0.001969  \n",
      "7             0.739008            0.760296          0.749555         0.010741  \n",
      "8             0.455652            0.451674          0.453642         0.001969  \n",
      "9             0.724011            0.763206          0.742128         0.021181  \n",
      "10            0.455652            0.451674          0.453642         0.001969  \n",
      "11            0.697768            0.752334          0.726232         0.026155  \n",
      "12            0.517601            0.574332          0.570453         0.034846  \n",
      "13            0.705470            0.739785          0.730271         0.014400  \n",
      "14            0.620601            0.726255          0.692635         0.043221  \n",
      "15            0.725186            0.755483          0.743804         0.012668  \n",
      "16            0.687055            0.763228          0.736945         0.031136  \n",
      "17            0.739109            0.776925          0.760808         0.016593  \n",
      "18            0.727701            0.795775          0.767120         0.029649  \n",
      "19            0.743038            0.801045          0.775822         0.025784  \n",
      "20            0.741602            0.811409          0.782838         0.029942  \n",
      "21            0.752504            0.805418          0.782835         0.023238  \n",
      "22            0.758824            0.821154          0.795737         0.026685  \n",
      "23            0.755770            0.814751          0.792046         0.024650  \n",
      "24            0.763451            0.840026          0.805824         0.034687  \n",
      "25            0.767872            0.826412          0.801424         0.025711  \n",
      "26            0.767873            0.845975          0.809074         0.037027  \n",
      "27            0.767873            0.840026          0.806911         0.033379  \n",
      "28            0.767873            0.857568          0.814839         0.042833  \n",
      "29            0.770782            0.842577          0.807833         0.034782  \n",
      "30            0.769339            0.859245          0.816827         0.042570  \n",
      "31            0.764945            0.848478          0.809562         0.039124  \n",
      "32            0.769339            0.859245          0.816827         0.042570  \n",
      "33            0.767873            0.857568          0.815106         0.042596  \n",
      "34            0.769339            0.859245          0.816827         0.042570  \n",
      "35            0.769339            0.859245          0.816827         0.042570  \n",
      "36            0.769339            0.859245          0.816827         0.042570  \n",
      "37            0.769339            0.859245          0.816827         0.042570  \n",
      "38            0.769339            0.859245          0.816827         0.042570  \n",
      "39            0.769339            0.859245          0.816827         0.042570  \n",
      "y_test_pred: [1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0\n",
      " 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0\n",
      " 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "test score: 0.7198618709502191\n",
      "logisticregression\n",
      "y_test shape: (380,)\n",
      "0.14027149321266968\n",
      "(1516, 109) (1516,)\n",
      "Fitting 4 folds for each of 40 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0        0.008668      0.000488         0.002854        0.000169   \n",
      "1        0.009606      0.000603         0.002796        0.000219   \n",
      "2        0.007938      0.000582         0.002859        0.000067   \n",
      "3        0.010241      0.000808         0.002940        0.000099   \n",
      "4        0.007926      0.000491         0.002908        0.000118   \n",
      "5        0.011205      0.001173         0.002904        0.000037   \n",
      "6        0.007787      0.000652         0.002880        0.000087   \n",
      "7        0.013260      0.002142         0.004194        0.002153   \n",
      "8        0.008466      0.000882         0.002837        0.000081   \n",
      "9        0.015779      0.003799         0.002908        0.000139   \n",
      "10       0.010015      0.001227         0.002952        0.000083   \n",
      "11       0.016670      0.002912         0.002904        0.000057   \n",
      "12       0.010770      0.001029         0.002835        0.000075   \n",
      "13       0.019324      0.002461         0.002925        0.000235   \n",
      "14       0.015118      0.004636         0.002909        0.000151   \n",
      "15       0.024213      0.002783         0.002982        0.000097   \n",
      "16       0.023946      0.004512         0.004506        0.002435   \n",
      "17       0.032991      0.007287         0.003454        0.000907   \n",
      "18       0.111200      0.053762         0.003011        0.000115   \n",
      "19       0.040692      0.006969         0.004541        0.002313   \n",
      "20       0.318771      0.049238         0.002975        0.000092   \n",
      "21       0.051260      0.006722         0.006570        0.003951   \n",
      "22       0.712937      0.031993         0.004882        0.002556   \n",
      "23       0.068708      0.012091         0.003009        0.000101   \n",
      "24       1.765295      0.182271         0.003131        0.000207   \n",
      "25       0.086763      0.011014         0.004200        0.001720   \n",
      "26       2.442020      0.560983         0.004250        0.001512   \n",
      "27       0.103392      0.014823         0.003027        0.000118   \n",
      "28       2.202517      0.187675         0.004424        0.001966   \n",
      "29       0.139118      0.012026         0.003101        0.000244   \n",
      "30       3.033956      1.254196         0.003130        0.000380   \n",
      "31       0.147637      0.009340         0.005166        0.001530   \n",
      "32       3.083782      1.350094         0.003419        0.000597   \n",
      "33       0.151200      0.021305         0.003008        0.000080   \n",
      "34       3.666758      2.249537         0.002984        0.000133   \n",
      "35       0.186364      0.026809         0.002998        0.000125   \n",
      "36       3.436568      1.781761         0.002974        0.000108   \n",
      "37       0.196647      0.044725         0.002964        0.000132   \n",
      "38       3.566819      1.977119         0.003038        0.000192   \n",
      "39       0.197548      0.052385         0.004259        0.001923   \n",
      "\n",
      "   param_logisticregression__C param_logisticregression__penalty  \\\n",
      "0                       0.0001                                l1   \n",
      "1                       0.0001                                l2   \n",
      "2                     0.000264                                l1   \n",
      "3                     0.000264                                l2   \n",
      "4                     0.000695                                l1   \n",
      "5                     0.000695                                l2   \n",
      "6                     0.001833                                l1   \n",
      "7                     0.001833                                l2   \n",
      "8                     0.004833                                l1   \n",
      "9                     0.004833                                l2   \n",
      "10                    0.012743                                l1   \n",
      "11                    0.012743                                l2   \n",
      "12                    0.033598                                l1   \n",
      "13                    0.033598                                l2   \n",
      "14                    0.088587                                l1   \n",
      "15                    0.088587                                l2   \n",
      "16                    0.233572                                l1   \n",
      "17                    0.233572                                l2   \n",
      "18                    0.615848                                l1   \n",
      "19                    0.615848                                l2   \n",
      "20                    1.623777                                l1   \n",
      "21                    1.623777                                l2   \n",
      "22                    4.281332                                l1   \n",
      "23                    4.281332                                l2   \n",
      "24                   11.288379                                l1   \n",
      "25                   11.288379                                l2   \n",
      "26                   29.763514                                l1   \n",
      "27                   29.763514                                l2   \n",
      "28                   78.475997                                l1   \n",
      "29                   78.475997                                l2   \n",
      "30                  206.913808                                l1   \n",
      "31                  206.913808                                l2   \n",
      "32                  545.559478                                l1   \n",
      "33                  545.559478                                l2   \n",
      "34                 1438.449888                                l1   \n",
      "35                 1438.449888                                l2   \n",
      "36                 3792.690191                                l1   \n",
      "37                 3792.690191                                l2   \n",
      "38                     10000.0                                l1   \n",
      "39                     10000.0                                l2   \n",
      "\n",
      "   param_logisticregression__solver  \\\n",
      "0                         liblinear   \n",
      "1                         liblinear   \n",
      "2                         liblinear   \n",
      "3                         liblinear   \n",
      "4                         liblinear   \n",
      "5                         liblinear   \n",
      "6                         liblinear   \n",
      "7                         liblinear   \n",
      "8                         liblinear   \n",
      "9                         liblinear   \n",
      "10                        liblinear   \n",
      "11                        liblinear   \n",
      "12                        liblinear   \n",
      "13                        liblinear   \n",
      "14                        liblinear   \n",
      "15                        liblinear   \n",
      "16                        liblinear   \n",
      "17                        liblinear   \n",
      "18                        liblinear   \n",
      "19                        liblinear   \n",
      "20                        liblinear   \n",
      "21                        liblinear   \n",
      "22                        liblinear   \n",
      "23                        liblinear   \n",
      "24                        liblinear   \n",
      "25                        liblinear   \n",
      "26                        liblinear   \n",
      "27                        liblinear   \n",
      "28                        liblinear   \n",
      "29                        liblinear   \n",
      "30                        liblinear   \n",
      "31                        liblinear   \n",
      "32                        liblinear   \n",
      "33                        liblinear   \n",
      "34                        liblinear   \n",
      "35                        liblinear   \n",
      "36                        liblinear   \n",
      "37                        liblinear   \n",
      "38                        liblinear   \n",
      "39                        liblinear   \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "0   {'logisticregression__C': 0.0001, 'logisticreg...           0.423611   \n",
      "1   {'logisticregression__C': 0.0001, 'logisticreg...           0.654646   \n",
      "2   {'logisticregression__C': 0.000263665089873035...           0.423611   \n",
      "3   {'logisticregression__C': 0.000263665089873035...           0.691970   \n",
      "4   {'logisticregression__C': 0.000695192796177560...           0.423611   \n",
      "5   {'logisticregression__C': 0.000695192796177560...           0.637698   \n",
      "6   {'logisticregression__C': 0.001832980710832435...           0.423611   \n",
      "7   {'logisticregression__C': 0.001832980710832435...           0.593137   \n",
      "8   {'logisticregression__C': 0.004832930238571752...           0.423611   \n",
      "9   {'logisticregression__C': 0.004832930238571752...           0.602871   \n",
      "10  {'logisticregression__C': 0.012742749857031334...           0.423611   \n",
      "11  {'logisticregression__C': 0.012742749857031334...           0.593137   \n",
      "12  {'logisticregression__C': 0.03359818286283781,...           0.512911   \n",
      "13  {'logisticregression__C': 0.03359818286283781,...           0.634821   \n",
      "14  {'logisticregression__C': 0.08858667904100823,...           0.552624   \n",
      "15  {'logisticregression__C': 0.08858667904100823,...           0.692293   \n",
      "16  {'logisticregression__C': 0.23357214690901212,...           0.593137   \n",
      "17  {'logisticregression__C': 0.23357214690901212,...           0.681260   \n",
      "18  {'logisticregression__C': 0.615848211066026, '...           0.614577   \n",
      "19  {'logisticregression__C': 0.615848211066026, '...           0.677613   \n",
      "20  {'logisticregression__C': 1.623776739188721, '...           0.589689   \n",
      "21  {'logisticregression__C': 1.623776739188721, '...           0.635965   \n",
      "22  {'logisticregression__C': 4.281332398719396, '...           0.560381   \n",
      "23  {'logisticregression__C': 4.281332398719396, '...           0.625814   \n",
      "24  {'logisticregression__C': 11.288378916846883, ...           0.570690   \n",
      "25  {'logisticregression__C': 11.288378916846883, ...           0.589599   \n",
      "26  {'logisticregression__C': 29.763514416313132, ...           0.580150   \n",
      "27  {'logisticregression__C': 29.763514416313132, ...           0.589599   \n",
      "28  {'logisticregression__C': 78.47599703514607, '...           0.599310   \n",
      "29  {'logisticregression__C': 78.47599703514607, '...           0.599310   \n",
      "30  {'logisticregression__C': 206.913808111479, 'l...           0.599310   \n",
      "31  {'logisticregression__C': 206.913808111479, 'l...           0.599310   \n",
      "32  {'logisticregression__C': 545.5594781168514, '...           0.599310   \n",
      "33  {'logisticregression__C': 545.5594781168514, '...           0.599310   \n",
      "34  {'logisticregression__C': 1438.44988828766, 'l...           0.599310   \n",
      "35  {'logisticregression__C': 1438.44988828766, 'l...           0.599310   \n",
      "36  {'logisticregression__C': 3792.690190732246, '...           0.599310   \n",
      "37  {'logisticregression__C': 3792.690190732246, '...           0.599310   \n",
      "38  {'logisticregression__C': 10000.0, 'logisticre...           0.599310   \n",
      "39  {'logisticregression__C': 10000.0, 'logisticre...           0.599310   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
      "0            0.448780           0.448780           0.455238         0.444103   \n",
      "1            0.526325           0.526325           0.694098         0.600348   \n",
      "2            0.448780           0.448780           0.455238         0.444103   \n",
      "3            0.557966           0.557966           0.697959         0.626465   \n",
      "4            0.448780           0.448780           0.455238         0.444103   \n",
      "5            0.586314           0.586314           0.706270         0.629149   \n",
      "6            0.448780           0.448780           0.455238         0.444103   \n",
      "7            0.642600           0.642600           0.692654         0.642748   \n",
      "8            0.448780           0.448780           0.455238         0.444103   \n",
      "9            0.679735           0.679735           0.680280         0.660655   \n",
      "10           0.448780           0.448780           0.455238         0.444103   \n",
      "11           0.676616           0.676616           0.647670         0.648510   \n",
      "12           0.649407           0.649407           0.542516         0.588560   \n",
      "13           0.646534           0.646534           0.637334         0.641306   \n",
      "14           0.709138           0.709138           0.685419         0.664080   \n",
      "15           0.649068           0.649068           0.648234         0.659666   \n",
      "16           0.683651           0.683651           0.671104         0.657886   \n",
      "17           0.649068           0.649068           0.639441         0.654709   \n",
      "18           0.647228           0.647228           0.632931         0.635491   \n",
      "19           0.620168           0.620168           0.605505         0.630863   \n",
      "20           0.647228           0.647228           0.570826         0.613743   \n",
      "21           0.612893           0.612893           0.585031         0.611695   \n",
      "22           0.607437           0.607437           0.536086         0.577835   \n",
      "23           0.575074           0.575074           0.544453         0.580104   \n",
      "24           0.322996           0.322996           0.515254         0.432984   \n",
      "25           0.380578           0.380578           0.529757         0.470128   \n",
      "26           0.288469           0.288469           0.535342         0.423108   \n",
      "27           0.318103           0.318103           0.530910         0.439179   \n",
      "28           0.273627           0.273627           0.541667         0.422058   \n",
      "29           0.283557           0.283557           0.524744         0.422792   \n",
      "30           0.273627           0.273627           0.511174         0.414435   \n",
      "31           0.273627           0.273627           0.527891         0.418614   \n",
      "32           0.273627           0.273627           0.499910         0.411619   \n",
      "33           0.273627           0.273627           0.513042         0.414902   \n",
      "34           0.273627           0.273627           0.501298         0.411966   \n",
      "35           0.273627           0.273627           0.508126         0.413673   \n",
      "36           0.273627           0.273627           0.502630         0.412299   \n",
      "37           0.273627           0.273627           0.505319         0.412971   \n",
      "38           0.273627           0.273627           0.498217         0.411195   \n",
      "39           0.273627           0.273627           0.508601         0.413791   \n",
      "\n",
      "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
      "0         0.012121               20            0.454718            0.453853   \n",
      "1         0.075326               15            0.664098            0.677299   \n",
      "2         0.012121               20            0.454718            0.453853   \n",
      "3         0.068532               12            0.698162            0.700326   \n",
      "4         0.012121               20            0.454718            0.453853   \n",
      "5         0.049220               11            0.729895            0.743759   \n",
      "6         0.012121               20            0.454718            0.453853   \n",
      "7         0.035185                7            0.736375            0.756390   \n",
      "8         0.012121               20            0.454718            0.453853   \n",
      "9         0.033363                2            0.713600            0.731961   \n",
      "10        0.012121               20            0.454718            0.453853   \n",
      "11        0.034084                6            0.720841            0.723041   \n",
      "12        0.061740               16            0.614871            0.629270   \n",
      "13        0.005303                8            0.723617            0.749130   \n",
      "14        0.065073                1            0.689472            0.697737   \n",
      "15        0.018840                3            0.733677            0.764167   \n",
      "16        0.037732                4            0.725288            0.753140   \n",
      "17        0.015825                5            0.752588            0.774024   \n",
      "18        0.013411                9            0.758631            0.769671   \n",
      "19        0.027647               10            0.757827            0.796031   \n",
      "20        0.034143               13            0.762703            0.792869   \n",
      "21        0.018048               14            0.770452            0.793799   \n",
      "22        0.030823               18            0.775249            0.792689   \n",
      "23        0.029202               17            0.767559            0.800749   \n",
      "24        0.111721               27            0.778065            0.800271   \n",
      "25        0.092016               19            0.774275            0.803603   \n",
      "26        0.135567               28            0.782806            0.794125   \n",
      "27        0.122841               26            0.779948            0.800271   \n",
      "28        0.149823               30            0.781823            0.793036   \n",
      "29        0.141709               29            0.780929            0.796102   \n",
      "30        0.144214               33            0.781729            0.793924   \n",
      "31        0.147169               31            0.781823            0.795011   \n",
      "32        0.142396               39            0.781729            0.793924   \n",
      "33        0.144529               32            0.781729            0.793924   \n",
      "34        0.142613               38            0.781729            0.793924   \n",
      "35        0.143708               35            0.781729            0.793924   \n",
      "36        0.142822               37            0.781729            0.793924   \n",
      "37        0.143252               36            0.781729            0.793924   \n",
      "38        0.142135               40            0.781729            0.793924   \n",
      "39        0.143786               34            0.781729            0.793036   \n",
      "\n",
      "    split2_train_score  split3_train_score  mean_train_score  std_train_score  \n",
      "0             0.453853            0.451800          0.453556         0.001074  \n",
      "1             0.677299            0.662821          0.670379         0.006934  \n",
      "2             0.453853            0.451800          0.453556         0.001074  \n",
      "3             0.700326            0.676096          0.693727         0.010218  \n",
      "4             0.453853            0.451800          0.453556         0.001074  \n",
      "5             0.743759            0.719595          0.734252         0.010181  \n",
      "6             0.453853            0.451800          0.453556         0.001074  \n",
      "7             0.756390            0.775765          0.756230         0.013928  \n",
      "8             0.453853            0.451800          0.453556         0.001074  \n",
      "9             0.731961            0.770363          0.736971         0.020685  \n",
      "10            0.453853            0.451800          0.453556         0.001074  \n",
      "11            0.723041            0.740448          0.726843         0.007906  \n",
      "12            0.629270            0.550212          0.605906         0.032688  \n",
      "13            0.749130            0.754948          0.744207         0.012123  \n",
      "14            0.697737            0.680744          0.691422         0.007028  \n",
      "15            0.764167            0.763759          0.756443         0.013145  \n",
      "16            0.753140            0.733379          0.741237         0.012242  \n",
      "17            0.774024            0.770849          0.767871         0.008919  \n",
      "18            0.769671            0.773806          0.767945         0.005636  \n",
      "19            0.796031            0.770718          0.780152         0.016520  \n",
      "20            0.792869            0.767904          0.779086         0.013905  \n",
      "21            0.793799            0.782651          0.785175         0.009642  \n",
      "22            0.792689            0.782651          0.785820         0.007351  \n",
      "23            0.800749            0.783864          0.788230         0.013782  \n",
      "24            0.800271            0.789958          0.792141         0.009152  \n",
      "25            0.803603            0.790234          0.792929         0.012074  \n",
      "26            0.794125            0.789678          0.790184         0.004630  \n",
      "27            0.800271            0.789958          0.792612         0.008437  \n",
      "28            0.793036            0.792198          0.790023         0.004747  \n",
      "29            0.796102            0.788548          0.790420         0.006288  \n",
      "30            0.793924            0.791086          0.790166         0.005007  \n",
      "31            0.795011            0.784595          0.789110         0.005982  \n",
      "32            0.793924            0.793610          0.790797         0.005236  \n",
      "33            0.793924            0.791086          0.790166         0.005007  \n",
      "34            0.793924            0.792198          0.790444         0.005080  \n",
      "35            0.793924            0.792198          0.790444         0.005080  \n",
      "36            0.793924            0.792198          0.790444         0.005080  \n",
      "37            0.793924            0.792198          0.790444         0.005080  \n",
      "38            0.793924            0.792198          0.790444         0.005080  \n",
      "39            0.793036            0.789678          0.789370         0.004619  \n",
      "y_test_pred: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0\n",
      " 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1]\n",
      "test score: 0.6357298474945534\n",
      "logisticregression\n",
      "y_test shape: (380,)\n",
      "0.14414414414414414\n",
      "(1516, 109) (1516,)\n",
      "Fitting 4 folds for each of 40 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0        0.008633      0.000068         0.002662        0.000033   \n",
      "1        0.010169      0.000646         0.002758        0.000093   \n",
      "2        0.009009      0.000409         0.002784        0.000122   \n",
      "3        0.012036      0.000610         0.002872        0.000095   \n",
      "4        0.008700      0.000433         0.002821        0.000087   \n",
      "5        0.011673      0.000739         0.002654        0.000118   \n",
      "6        0.007596      0.000690         0.002738        0.000217   \n",
      "7        0.014335      0.000790         0.002835        0.000055   \n",
      "8        0.009211      0.000746         0.002768        0.000071   \n",
      "9        0.015456      0.001068         0.002789        0.000037   \n",
      "10       0.010151      0.000610         0.002839        0.000178   \n",
      "11       0.019405      0.001038         0.003179        0.000361   \n",
      "12       0.011544      0.000454         0.002810        0.000154   \n",
      "13       0.021702      0.001210         0.002843        0.000126   \n",
      "14       0.014228      0.001239         0.002899        0.000084   \n",
      "15       0.040095      0.011183         0.002906        0.000091   \n",
      "16       0.027604      0.005701         0.002863        0.000177   \n",
      "17       0.037406      0.003615         0.002910        0.000133   \n",
      "18       0.151241      0.024841         0.002905        0.000044   \n",
      "19       0.041265      0.002837         0.002780        0.000042   \n",
      "20       0.369760      0.004729         0.003001        0.000070   \n",
      "21       0.057009      0.004445         0.002982        0.000139   \n",
      "22       1.263528      0.203381         0.003166        0.000403   \n",
      "23       0.075859      0.006898         0.002927        0.000116   \n",
      "24       1.910020      0.423270         0.002968        0.000027   \n",
      "25       0.096798      0.004966         0.006214        0.002517   \n",
      "26       2.465244      0.117717         0.002933        0.000039   \n",
      "27       0.129315      0.007305         0.002899        0.000101   \n",
      "28       2.677622      0.175730         0.004482        0.002641   \n",
      "29       0.121652      0.018155         0.002923        0.000083   \n",
      "30       2.694283      0.196995         0.002865        0.000065   \n",
      "31       0.144541      0.009693         0.003763        0.001543   \n",
      "32       2.740385      0.224894         0.002954        0.000097   \n",
      "33       0.149776      0.024852         0.004393        0.002418   \n",
      "34       2.752817      0.372776         0.002850        0.000089   \n",
      "35       0.154871      0.038244         0.003165        0.000441   \n",
      "36       3.025589      0.649409         0.004365        0.002489   \n",
      "37       0.151130      0.023956         0.002879        0.000128   \n",
      "38       2.753873      0.287845         0.002946        0.000097   \n",
      "39       0.148401      0.023478         0.002933        0.000035   \n",
      "\n",
      "   param_logisticregression__C param_logisticregression__penalty  \\\n",
      "0                       0.0001                                l1   \n",
      "1                       0.0001                                l2   \n",
      "2                     0.000264                                l1   \n",
      "3                     0.000264                                l2   \n",
      "4                     0.000695                                l1   \n",
      "5                     0.000695                                l2   \n",
      "6                     0.001833                                l1   \n",
      "7                     0.001833                                l2   \n",
      "8                     0.004833                                l1   \n",
      "9                     0.004833                                l2   \n",
      "10                    0.012743                                l1   \n",
      "11                    0.012743                                l2   \n",
      "12                    0.033598                                l1   \n",
      "13                    0.033598                                l2   \n",
      "14                    0.088587                                l1   \n",
      "15                    0.088587                                l2   \n",
      "16                    0.233572                                l1   \n",
      "17                    0.233572                                l2   \n",
      "18                    0.615848                                l1   \n",
      "19                    0.615848                                l2   \n",
      "20                    1.623777                                l1   \n",
      "21                    1.623777                                l2   \n",
      "22                    4.281332                                l1   \n",
      "23                    4.281332                                l2   \n",
      "24                   11.288379                                l1   \n",
      "25                   11.288379                                l2   \n",
      "26                   29.763514                                l1   \n",
      "27                   29.763514                                l2   \n",
      "28                   78.475997                                l1   \n",
      "29                   78.475997                                l2   \n",
      "30                  206.913808                                l1   \n",
      "31                  206.913808                                l2   \n",
      "32                  545.559478                                l1   \n",
      "33                  545.559478                                l2   \n",
      "34                 1438.449888                                l1   \n",
      "35                 1438.449888                                l2   \n",
      "36                 3792.690191                                l1   \n",
      "37                 3792.690191                                l2   \n",
      "38                     10000.0                                l1   \n",
      "39                     10000.0                                l2   \n",
      "\n",
      "   param_logisticregression__solver  \\\n",
      "0                         liblinear   \n",
      "1                         liblinear   \n",
      "2                         liblinear   \n",
      "3                         liblinear   \n",
      "4                         liblinear   \n",
      "5                         liblinear   \n",
      "6                         liblinear   \n",
      "7                         liblinear   \n",
      "8                         liblinear   \n",
      "9                         liblinear   \n",
      "10                        liblinear   \n",
      "11                        liblinear   \n",
      "12                        liblinear   \n",
      "13                        liblinear   \n",
      "14                        liblinear   \n",
      "15                        liblinear   \n",
      "16                        liblinear   \n",
      "17                        liblinear   \n",
      "18                        liblinear   \n",
      "19                        liblinear   \n",
      "20                        liblinear   \n",
      "21                        liblinear   \n",
      "22                        liblinear   \n",
      "23                        liblinear   \n",
      "24                        liblinear   \n",
      "25                        liblinear   \n",
      "26                        liblinear   \n",
      "27                        liblinear   \n",
      "28                        liblinear   \n",
      "29                        liblinear   \n",
      "30                        liblinear   \n",
      "31                        liblinear   \n",
      "32                        liblinear   \n",
      "33                        liblinear   \n",
      "34                        liblinear   \n",
      "35                        liblinear   \n",
      "36                        liblinear   \n",
      "37                        liblinear   \n",
      "38                        liblinear   \n",
      "39                        liblinear   \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "0   {'logisticregression__C': 0.0001, 'logisticreg...           0.438462   \n",
      "1   {'logisticregression__C': 0.0001, 'logisticreg...           0.694829   \n",
      "2   {'logisticregression__C': 0.000263665089873035...           0.438462   \n",
      "3   {'logisticregression__C': 0.000263665089873035...           0.692289   \n",
      "4   {'logisticregression__C': 0.000695192796177560...           0.438462   \n",
      "5   {'logisticregression__C': 0.000695192796177560...           0.613757   \n",
      "6   {'logisticregression__C': 0.001832980710832435...           0.438462   \n",
      "7   {'logisticregression__C': 0.001832980710832435...           0.638017   \n",
      "8   {'logisticregression__C': 0.004832930238571752...           0.438462   \n",
      "9   {'logisticregression__C': 0.004832930238571752...           0.582857   \n",
      "10  {'logisticregression__C': 0.012742749857031334...           0.438462   \n",
      "11  {'logisticregression__C': 0.012742749857031334...           0.571848   \n",
      "12  {'logisticregression__C': 0.03359818286283781,...           0.556424   \n",
      "13  {'logisticregression__C': 0.03359818286283781,...           0.672436   \n",
      "14  {'logisticregression__C': 0.08858667904100823,...           0.556424   \n",
      "15  {'logisticregression__C': 0.08858667904100823,...           0.677277   \n",
      "16  {'logisticregression__C': 0.23357214690901212,...           0.734545   \n",
      "17  {'logisticregression__C': 0.23357214690901212,...           0.654937   \n",
      "18  {'logisticregression__C': 0.615848211066026, '...           0.679825   \n",
      "19  {'logisticregression__C': 0.615848211066026, '...           0.668182   \n",
      "20  {'logisticregression__C': 1.623776739188721, '...           0.642986   \n",
      "21  {'logisticregression__C': 1.623776739188721, '...           0.621226   \n",
      "22  {'logisticregression__C': 4.281332398719396, '...           0.576074   \n",
      "23  {'logisticregression__C': 4.281332398719396, '...           0.621226   \n",
      "24  {'logisticregression__C': 11.288378916846883, ...           0.607780   \n",
      "25  {'logisticregression__C': 11.288378916846883, ...           0.609988   \n",
      "26  {'logisticregression__C': 29.763514416313132, ...           0.585315   \n",
      "27  {'logisticregression__C': 29.763514416313132, ...           0.609626   \n",
      "28  {'logisticregression__C': 78.47599703514607, '...           0.585315   \n",
      "29  {'logisticregression__C': 78.47599703514607, '...           0.607780   \n",
      "30  {'logisticregression__C': 206.913808111479, 'l...           0.585315   \n",
      "31  {'logisticregression__C': 206.913808111479, 'l...           0.585315   \n",
      "32  {'logisticregression__C': 545.5594781168514, '...           0.585315   \n",
      "33  {'logisticregression__C': 545.5594781168514, '...           0.585315   \n",
      "34  {'logisticregression__C': 1438.44988828766, 'l...           0.585315   \n",
      "35  {'logisticregression__C': 1438.44988828766, 'l...           0.585315   \n",
      "36  {'logisticregression__C': 3792.690190732246, '...           0.585315   \n",
      "37  {'logisticregression__C': 3792.690190732246, '...           0.585315   \n",
      "38  {'logisticregression__C': 10000.0, 'logisticre...           0.585315   \n",
      "39  {'logisticregression__C': 10000.0, 'logisticre...           0.585315   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
      "0            0.438462           0.438462           0.453202         0.442147   \n",
      "1            0.694829           0.694829           0.530292         0.653695   \n",
      "2            0.438462           0.438462           0.453202         0.442147   \n",
      "3            0.692289           0.692289           0.547883         0.656187   \n",
      "4            0.438462           0.438462           0.453202         0.442147   \n",
      "5            0.613757           0.613757           0.579899         0.605292   \n",
      "6            0.438462           0.438462           0.453202         0.442147   \n",
      "7            0.638017           0.638017           0.643520         0.639392   \n",
      "8            0.438462           0.438462           0.453202         0.442147   \n",
      "9            0.582857           0.582857           0.635325         0.595974   \n",
      "10           0.438462           0.438462           0.453202         0.442147   \n",
      "11           0.571848           0.571848           0.630526         0.586517   \n",
      "12           0.556424           0.556424           0.636831         0.576525   \n",
      "13           0.672436           0.672436           0.620797         0.659526   \n",
      "14           0.556424           0.556424           0.626263         0.573883   \n",
      "15           0.677277           0.677277           0.630526         0.665589   \n",
      "16           0.734545           0.734545           0.644060         0.711924   \n",
      "17           0.654937           0.654937           0.622075         0.646722   \n",
      "18           0.679825           0.679825           0.606317         0.661448   \n",
      "19           0.668182           0.668182           0.610811         0.653839   \n",
      "20           0.642986           0.642986           0.592972         0.630482   \n",
      "21           0.621226           0.621226           0.611104         0.618696   \n",
      "22           0.576074           0.576074           0.571547         0.574942   \n",
      "23           0.621226           0.621226           0.578654         0.610583   \n",
      "24           0.607780           0.607780           0.479629         0.575742   \n",
      "25           0.609988           0.609988           0.486531         0.579124   \n",
      "26           0.585315           0.585315           0.430051         0.546499   \n",
      "27           0.609626           0.609626           0.436512         0.566347   \n",
      "28           0.585315           0.585315           0.417996         0.543485   \n",
      "29           0.607780           0.607780           0.428025         0.562841   \n",
      "30           0.585315           0.585315           0.416605         0.543137   \n",
      "31           0.585315           0.585315           0.406271         0.540554   \n",
      "32           0.585315           0.585315           0.412614         0.542139   \n",
      "33           0.585315           0.585315           0.419339         0.543821   \n",
      "34           0.585315           0.585315           0.412614         0.542139   \n",
      "35           0.585315           0.585315           0.423258         0.544800   \n",
      "36           0.585315           0.585315           0.416605         0.543137   \n",
      "37           0.585315           0.585315           0.423258         0.544800   \n",
      "38           0.585315           0.585315           0.412614         0.542139   \n",
      "39           0.585315           0.585315           0.421875         0.544455   \n",
      "\n",
      "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
      "0         0.006383               35            0.454236            0.454236   \n",
      "1         0.071247                7            0.669139            0.669139   \n",
      "2         0.006383               35            0.454236            0.454236   \n",
      "3         0.062530                5            0.685756            0.685756   \n",
      "4         0.006383               35            0.454236            0.454236   \n",
      "5         0.014661               13            0.721329            0.721329   \n",
      "6         0.006383               35            0.454236            0.454236   \n",
      "7         0.002383                9            0.740122            0.740122   \n",
      "8         0.006383               35            0.454236            0.454236   \n",
      "9         0.022719               14            0.719179            0.719179   \n",
      "10        0.006383               35            0.454236            0.454236   \n",
      "11        0.025408               15            0.721894            0.721894   \n",
      "12        0.034817               17            0.613838            0.613838   \n",
      "13        0.022360                4            0.729979            0.729979   \n",
      "14        0.030241               20            0.687859            0.687859   \n",
      "15        0.020244                2            0.746288            0.746288   \n",
      "16        0.039182                1            0.725115            0.725115   \n",
      "17        0.014230                8            0.756436            0.756436   \n",
      "18        0.031830                3            0.753500            0.753500   \n",
      "19        0.024842                6            0.762189            0.762189   \n",
      "20        0.021657               10            0.764069            0.764069   \n",
      "21        0.004383               11            0.767805            0.767805   \n",
      "22        0.001961               19            0.770533            0.770533   \n",
      "23        0.018435               12            0.773350            0.773350   \n",
      "24        0.055491               18            0.784937            0.783988   \n",
      "25        0.053459               16            0.781380            0.781380   \n",
      "26        0.067231               23            0.783988            0.783988   \n",
      "27        0.074960               21            0.785890            0.785890   \n",
      "28        0.072451               28            0.784113            0.784113   \n",
      "29        0.077836               22            0.785890            0.785890   \n",
      "30        0.073053               29            0.783162            0.783162   \n",
      "31        0.077528               34            0.785067            0.785067   \n",
      "32        0.074781               31            0.783162            0.783162   \n",
      "33        0.071870               27            0.783162            0.783162   \n",
      "34        0.074781               31            0.783162            0.783162   \n",
      "35        0.070173               24            0.783162            0.783162   \n",
      "36        0.073053               29            0.783162            0.783162   \n",
      "37        0.070173               24            0.783162            0.783162   \n",
      "38        0.074781               31            0.783162            0.783162   \n",
      "39        0.070771               26            0.783162            0.783162   \n",
      "\n",
      "    split2_train_score  split3_train_score  mean_train_score  std_train_score  \n",
      "0             0.454236            0.453547          0.454064         0.000298  \n",
      "1             0.669139            0.666998          0.668604         0.000927  \n",
      "2             0.454236            0.453547          0.454064         0.000298  \n",
      "3             0.685756            0.695030          0.688075         0.004015  \n",
      "4             0.454236            0.453547          0.454064         0.000298  \n",
      "5             0.721329            0.735208          0.724799         0.006010  \n",
      "6             0.454236            0.453547          0.454064         0.000298  \n",
      "7             0.740122            0.756551          0.744229         0.007114  \n",
      "8             0.454236            0.453547          0.454064         0.000298  \n",
      "9             0.719179            0.743719          0.725314         0.010626  \n",
      "10            0.454236            0.453547          0.454064         0.000298  \n",
      "11            0.721894            0.741370          0.726763         0.008433  \n",
      "12            0.613838            0.624381          0.616474         0.004565  \n",
      "13            0.729979            0.751291          0.735307         0.009228  \n",
      "14            0.687859            0.723773          0.696838         0.015551  \n",
      "15            0.746288            0.761283          0.750036         0.006493  \n",
      "16            0.725115            0.742497          0.729460         0.007527  \n",
      "17            0.756436            0.774902          0.761052         0.007996  \n",
      "18            0.753500            0.764542          0.756261         0.004782  \n",
      "19            0.762189            0.790801          0.769342         0.012390  \n",
      "20            0.764069            0.789731          0.770484         0.011112  \n",
      "21            0.767805            0.796418          0.774958         0.012390  \n",
      "22            0.770533            0.792359          0.775989         0.009451  \n",
      "23            0.773350            0.796192          0.779061         0.009891  \n",
      "24            0.784937            0.797856          0.787930         0.005744  \n",
      "25            0.781380            0.797856          0.785499         0.007134  \n",
      "26            0.783988            0.789188          0.785288         0.002252  \n",
      "27            0.785890            0.790241          0.786978         0.001884  \n",
      "28            0.784113            0.794065          0.786601         0.004309  \n",
      "29            0.785890            0.790241          0.786978         0.001884  \n",
      "30            0.783162            0.795126          0.786153         0.005181  \n",
      "31            0.785067            0.794065          0.787316         0.003896  \n",
      "32            0.783162            0.797028          0.786629         0.006004  \n",
      "33            0.783162            0.797028          0.786629         0.006004  \n",
      "34            0.783162            0.797028          0.786629         0.006004  \n",
      "35            0.783162            0.795126          0.786153         0.005181  \n",
      "36            0.783162            0.795126          0.786153         0.005181  \n",
      "37            0.783162            0.795126          0.786153         0.005181  \n",
      "38            0.783162            0.797028          0.786629         0.006004  \n",
      "39            0.783162            0.795126          0.786153         0.005181  \n",
      "y_test_pred: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 1 1 0 0]\n",
      "test score: 0.7006060606060607\n",
      "logisticregression\n",
      "y_test shape: (380,)\n",
      "0.14988814317673377\n",
      "(1516, 109) (1516,)\n",
      "Fitting 4 folds for each of 40 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0        0.007361      0.000616         0.002870        0.000155   \n",
      "1        0.008682      0.000830         0.002843        0.000141   \n",
      "2        0.007425      0.000590         0.002904        0.000097   \n",
      "3        0.009628      0.000817         0.003007        0.000218   \n",
      "4        0.008047      0.001393         0.004330        0.002391   \n",
      "5        0.010565      0.000538         0.003141        0.000217   \n",
      "6        0.008982      0.002574         0.002919        0.000129   \n",
      "7        0.013626      0.001810         0.003078        0.000437   \n",
      "8        0.009672      0.001993         0.002815        0.000230   \n",
      "9        0.013940      0.001830         0.004223        0.002411   \n",
      "10       0.008873      0.001103         0.002864        0.000102   \n",
      "11       0.015603      0.003124         0.004917        0.003466   \n",
      "12       0.011027      0.001885         0.002934        0.000209   \n",
      "13       0.019202      0.002522         0.002899        0.000096   \n",
      "14       0.012942      0.001995         0.002927        0.000135   \n",
      "15       0.025864      0.006854         0.003925        0.001773   \n",
      "16       0.023412      0.007079         0.002936        0.000138   \n",
      "17       0.031614      0.006652         0.002934        0.000174   \n",
      "18       0.056636      0.019584         0.002960        0.000127   \n",
      "19       0.035686      0.005864         0.002992        0.000108   \n",
      "20       0.280424      0.132824         0.003228        0.000417   \n",
      "21       0.050451      0.009842         0.003410        0.000622   \n",
      "22       0.579026      0.028624         0.007170        0.002878   \n",
      "23       0.063654      0.014245         0.004552        0.002543   \n",
      "24       1.901451      0.256835         0.003756        0.001008   \n",
      "25       0.081023      0.015791         0.003549        0.001037   \n",
      "26     118.492598    200.025713         0.004942        0.002886   \n",
      "27       0.095471      0.015511         0.004139        0.001977   \n",
      "28     119.151492    200.654536         0.004294        0.002222   \n",
      "29       0.124165      0.021844         0.003008        0.000115   \n",
      "30     120.157032    199.261955         0.003597        0.000357   \n",
      "31       0.162753      0.031513         0.006568        0.006235   \n",
      "32     119.878598    201.663167         0.003156        0.000239   \n",
      "33       0.183468      0.020576         0.004065        0.001724   \n",
      "34     120.236700    201.799178         0.004204        0.001236   \n",
      "35       0.216331      0.020194         0.002906        0.000099   \n",
      "36       5.263423      2.466923         0.003027        0.000074   \n",
      "37       0.206012      0.043657         0.002972        0.000138   \n",
      "38     119.339254    201.566386         0.003141        0.000281   \n",
      "39       0.240816      0.070455         0.002993        0.000126   \n",
      "\n",
      "   param_logisticregression__C param_logisticregression__penalty  \\\n",
      "0                       0.0001                                l1   \n",
      "1                       0.0001                                l2   \n",
      "2                     0.000264                                l1   \n",
      "3                     0.000264                                l2   \n",
      "4                     0.000695                                l1   \n",
      "5                     0.000695                                l2   \n",
      "6                     0.001833                                l1   \n",
      "7                     0.001833                                l2   \n",
      "8                     0.004833                                l1   \n",
      "9                     0.004833                                l2   \n",
      "10                    0.012743                                l1   \n",
      "11                    0.012743                                l2   \n",
      "12                    0.033598                                l1   \n",
      "13                    0.033598                                l2   \n",
      "14                    0.088587                                l1   \n",
      "15                    0.088587                                l2   \n",
      "16                    0.233572                                l1   \n",
      "17                    0.233572                                l2   \n",
      "18                    0.615848                                l1   \n",
      "19                    0.615848                                l2   \n",
      "20                    1.623777                                l1   \n",
      "21                    1.623777                                l2   \n",
      "22                    4.281332                                l1   \n",
      "23                    4.281332                                l2   \n",
      "24                   11.288379                                l1   \n",
      "25                   11.288379                                l2   \n",
      "26                   29.763514                                l1   \n",
      "27                   29.763514                                l2   \n",
      "28                   78.475997                                l1   \n",
      "29                   78.475997                                l2   \n",
      "30                  206.913808                                l1   \n",
      "31                  206.913808                                l2   \n",
      "32                  545.559478                                l1   \n",
      "33                  545.559478                                l2   \n",
      "34                 1438.449888                                l1   \n",
      "35                 1438.449888                                l2   \n",
      "36                 3792.690191                                l1   \n",
      "37                 3792.690191                                l2   \n",
      "38                     10000.0                                l1   \n",
      "39                     10000.0                                l2   \n",
      "\n",
      "   param_logisticregression__solver  \\\n",
      "0                         liblinear   \n",
      "1                         liblinear   \n",
      "2                         liblinear   \n",
      "3                         liblinear   \n",
      "4                         liblinear   \n",
      "5                         liblinear   \n",
      "6                         liblinear   \n",
      "7                         liblinear   \n",
      "8                         liblinear   \n",
      "9                         liblinear   \n",
      "10                        liblinear   \n",
      "11                        liblinear   \n",
      "12                        liblinear   \n",
      "13                        liblinear   \n",
      "14                        liblinear   \n",
      "15                        liblinear   \n",
      "16                        liblinear   \n",
      "17                        liblinear   \n",
      "18                        liblinear   \n",
      "19                        liblinear   \n",
      "20                        liblinear   \n",
      "21                        liblinear   \n",
      "22                        liblinear   \n",
      "23                        liblinear   \n",
      "24                        liblinear   \n",
      "25                        liblinear   \n",
      "26                        liblinear   \n",
      "27                        liblinear   \n",
      "28                        liblinear   \n",
      "29                        liblinear   \n",
      "30                        liblinear   \n",
      "31                        liblinear   \n",
      "32                        liblinear   \n",
      "33                        liblinear   \n",
      "34                        liblinear   \n",
      "35                        liblinear   \n",
      "36                        liblinear   \n",
      "37                        liblinear   \n",
      "38                        liblinear   \n",
      "39                        liblinear   \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "0   {'logisticregression__C': 0.0001, 'logisticreg...           0.448767   \n",
      "1   {'logisticregression__C': 0.0001, 'logisticreg...           0.705993   \n",
      "2   {'logisticregression__C': 0.000263665089873035...           0.448767   \n",
      "3   {'logisticregression__C': 0.000263665089873035...           0.718166   \n",
      "4   {'logisticregression__C': 0.000695192796177560...           0.448767   \n",
      "5   {'logisticregression__C': 0.000695192796177560...           0.707289   \n",
      "6   {'logisticregression__C': 0.001832980710832435...           0.448767   \n",
      "7   {'logisticregression__C': 0.001832980710832435...           0.667830   \n",
      "8   {'logisticregression__C': 0.004832930238571752...           0.448767   \n",
      "9   {'logisticregression__C': 0.004832930238571752...           0.587808   \n",
      "10  {'logisticregression__C': 0.012742749857031334...           0.448767   \n",
      "11  {'logisticregression__C': 0.012742749857031334...           0.585812   \n",
      "12  {'logisticregression__C': 0.03359818286283781,...           0.512637   \n",
      "13  {'logisticregression__C': 0.03359818286283781,...           0.600488   \n",
      "14  {'logisticregression__C': 0.08858667904100823,...           0.608322   \n",
      "15  {'logisticregression__C': 0.08858667904100823,...           0.630836   \n",
      "16  {'logisticregression__C': 0.23357214690901212,...           0.648178   \n",
      "17  {'logisticregression__C': 0.23357214690901212,...           0.629785   \n",
      "18  {'logisticregression__C': 0.615848211066026, '...           0.619885   \n",
      "19  {'logisticregression__C': 0.615848211066026, '...           0.602896   \n",
      "20  {'logisticregression__C': 1.623776739188721, '...           0.611029   \n",
      "21  {'logisticregression__C': 1.623776739188721, '...           0.588010   \n",
      "22  {'logisticregression__C': 4.281332398719396, '...           0.556574   \n",
      "23  {'logisticregression__C': 4.281332398719396, '...           0.557012   \n",
      "24  {'logisticregression__C': 11.288378916846883, ...           0.512720   \n",
      "25  {'logisticregression__C': 11.288378916846883, ...           0.538431   \n",
      "26  {'logisticregression__C': 29.763514416313132, ...           0.498652   \n",
      "27  {'logisticregression__C': 29.763514416313132, ...           0.547864   \n",
      "28  {'logisticregression__C': 78.47599703514607, '...           0.497485   \n",
      "29  {'logisticregression__C': 78.47599703514607, '...           0.520692   \n",
      "30  {'logisticregression__C': 206.913808111479, 'l...           0.489104   \n",
      "31  {'logisticregression__C': 206.913808111479, 'l...           0.492077   \n",
      "32  {'logisticregression__C': 545.5594781168514, '...           0.515492   \n",
      "33  {'logisticregression__C': 545.5594781168514, '...           0.501844   \n",
      "34  {'logisticregression__C': 1438.44988828766, 'l...           0.510126   \n",
      "35  {'logisticregression__C': 1438.44988828766, 'l...           0.515492   \n",
      "36  {'logisticregression__C': 3792.690190732246, '...           0.514614   \n",
      "37  {'logisticregression__C': 3792.690190732246, '...           0.519869   \n",
      "38  {'logisticregression__C': 10000.0, 'logisticre...           0.512448   \n",
      "39  {'logisticregression__C': 10000.0, 'logisticre...           0.520976   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
      "0            0.450739           0.462901           0.450739         0.453286   \n",
      "1            0.478060           0.450121           0.478060         0.528058   \n",
      "2            0.450739           0.462901           0.450739         0.453286   \n",
      "3            0.516932           0.484802           0.516932         0.559208   \n",
      "4            0.450739           0.462901           0.450739         0.453286   \n",
      "5            0.576809           0.572568           0.576809         0.608369   \n",
      "6            0.450739           0.462901           0.450739         0.453286   \n",
      "7            0.618673           0.638859           0.618673         0.636009   \n",
      "8            0.450739           0.462901           0.450739         0.453286   \n",
      "9            0.701153           0.676661           0.701153         0.666694   \n",
      "10           0.450739           0.462901           0.450739         0.453286   \n",
      "11           0.671411           0.670495           0.671411         0.649782   \n",
      "12           0.647995           0.536906           0.647995         0.586383   \n",
      "13           0.675966           0.645095           0.675966         0.649379   \n",
      "14           0.664218           0.671207           0.664218         0.651991   \n",
      "15           0.655382           0.617957           0.655382         0.639889   \n",
      "16           0.655382           0.656618           0.655382         0.653890   \n",
      "17           0.651067           0.583020           0.651067         0.628735   \n",
      "18           0.643563           0.650711           0.643563         0.639431   \n",
      "19           0.602496           0.523448           0.602496         0.582834   \n",
      "20           0.627361           0.473373           0.627361         0.584781   \n",
      "21           0.611047           0.504272           0.611047         0.578594   \n",
      "22           0.592350           0.540921           0.592350         0.570549   \n",
      "23           0.581657           0.517560           0.581657         0.559471   \n",
      "24           0.349723           0.549474           0.349723         0.440410   \n",
      "25           0.495137           0.543177           0.495137         0.517970   \n",
      "26           0.289064           0.334146           0.289064         0.352731   \n",
      "27           0.384412           0.527556           0.384412         0.461061   \n",
      "28           0.274291           0.313532           0.274291         0.339900   \n",
      "29           0.308290           0.336206           0.308290         0.368370   \n",
      "30           0.264265           0.214402           0.264265         0.308009   \n",
      "31           0.274291           0.260589           0.274291         0.325312   \n",
      "32           0.259196           0.160533           0.259196         0.298604   \n",
      "33           0.269296           0.193221           0.269296         0.308414   \n",
      "34           0.259196           0.148636           0.259196         0.294289   \n",
      "35           0.259196           0.148636           0.259196         0.295630   \n",
      "36           0.259196           0.137096           0.259196         0.292526   \n",
      "37           0.259196           0.123365           0.259196         0.290407   \n",
      "38           0.259196           0.144813           0.259196         0.293913   \n",
      "39           0.259196           0.121379           0.259196         0.290187   \n",
      "\n",
      "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
      "0         0.005609               21            0.457342            0.454660   \n",
      "1         0.103362               18            0.656043            0.654043   \n",
      "2         0.005609               21            0.457342            0.454660   \n",
      "3         0.092707               17            0.677364            0.687316   \n",
      "4         0.005609               21            0.457342            0.454660   \n",
      "5         0.057138               10            0.720276            0.743805   \n",
      "6         0.005609               21            0.457342            0.454660   \n",
      "7         0.020136                8            0.759198            0.762316   \n",
      "8         0.005609               21            0.457342            0.454660   \n",
      "9         0.046629                1            0.761610            0.742043   \n",
      "10        0.005609               21            0.457342            0.454660   \n",
      "11        0.036935                4            0.733773            0.735860   \n",
      "12        0.062207               11            0.522274            0.614593   \n",
      "13        0.030913                5            0.727333            0.753229   \n",
      "14        0.025374                3            0.653624            0.715202   \n",
      "15        0.016148                6            0.745993            0.778297   \n",
      "16        0.003336                2            0.719595            0.763225   \n",
      "17        0.027787                9            0.756598            0.787727   \n",
      "18        0.011656                7            0.752687            0.789471   \n",
      "19        0.034287               13            0.769096            0.799714   \n",
      "20        0.064666               12            0.762801            0.801457   \n",
      "21        0.043928               14            0.775271            0.800351   \n",
      "22        0.022493               15            0.788854            0.805106   \n",
      "23        0.026206               16            0.787449            0.806214   \n",
      "24        0.091613               27            0.793169            0.813444   \n",
      "25        0.022895               19            0.791812            0.808444   \n",
      "26        0.086234               29            0.792939            0.819961   \n",
      "27        0.076984               20            0.793169            0.813444   \n",
      "28        0.092382               30            0.798683            0.814693   \n",
      "29        0.088679               28            0.790038            0.816157   \n",
      "30        0.106518               33            0.795556            0.816585   \n",
      "31        0.096444               31            0.798683            0.813910   \n",
      "32        0.131539               34            0.795556            0.816585   \n",
      "33        0.115915               32            0.795556            0.814693   \n",
      "34        0.132536               36            0.795556            0.814693   \n",
      "35        0.134723               35            0.795556            0.814693   \n",
      "36        0.137571               38            0.795556            0.814693   \n",
      "37        0.143618               39            0.795556            0.814693   \n",
      "38        0.134535               37            0.795556            0.814693   \n",
      "39        0.144638               40            0.795556            0.814693   \n",
      "\n",
      "    split2_train_score  split3_train_score  mean_train_score  std_train_score  \n",
      "0             0.447517            0.454660          0.453545         0.003648  \n",
      "1             0.674234            0.654043          0.659591         0.008494  \n",
      "2             0.447517            0.454660          0.453545         0.003648  \n",
      "3             0.699908            0.687316          0.687976         0.007998  \n",
      "4             0.447517            0.454660          0.453545         0.003648  \n",
      "5             0.745950            0.743805          0.738459         0.010534  \n",
      "6             0.447517            0.454660          0.453545         0.003648  \n",
      "7             0.764554            0.762316          0.762096         0.001906  \n",
      "8             0.447517            0.454660          0.453545         0.003648  \n",
      "9             0.750835            0.742043          0.749133         0.008048  \n",
      "10            0.447517            0.454660          0.453545         0.003648  \n",
      "11            0.734462            0.735860          0.734989         0.000904  \n",
      "12            0.556531            0.614593          0.576997         0.039498  \n",
      "13            0.746320            0.753229          0.745028         0.010598  \n",
      "14            0.702191            0.715202          0.696555         0.025349  \n",
      "15            0.772071            0.778297          0.768664         0.013334  \n",
      "16            0.763550            0.763225          0.752399         0.018940  \n",
      "17            0.789397            0.787727          0.780362         0.013737  \n",
      "18            0.791001            0.789471          0.780657         0.016161  \n",
      "19            0.799452            0.799714          0.791994         0.013221  \n",
      "20            0.811571            0.801457          0.794321         0.018661  \n",
      "21            0.810079            0.800351          0.796513         0.012891  \n",
      "22            0.817423            0.805106          0.804122         0.010148  \n",
      "23            0.813497            0.806214          0.803344         0.009647  \n",
      "24            0.826160            0.813444          0.811554         0.011816  \n",
      "25            0.830919            0.808444          0.809905         0.013903  \n",
      "26            0.826388            0.819961          0.814813         0.012898  \n",
      "27            0.825524            0.813444          0.811395         0.011621  \n",
      "28            0.825748            0.814693          0.813454         0.009649  \n",
      "29            0.825524            0.816157          0.811969         0.013227  \n",
      "30            0.825748            0.816585          0.813618         0.011079  \n",
      "31            0.823401            0.813910          0.812476         0.008856  \n",
      "32            0.825748            0.816585          0.813618         0.011079  \n",
      "33            0.825748            0.814693          0.812672         0.010864  \n",
      "34            0.825748            0.816585          0.813145         0.010982  \n",
      "35            0.825748            0.814693          0.812672         0.010864  \n",
      "36            0.828082            0.814693          0.813256         0.011589  \n",
      "37            0.828082            0.814693          0.813256         0.011589  \n",
      "38            0.825748            0.814693          0.812672         0.010864  \n",
      "39            0.828082            0.814693          0.813256         0.011589  \n",
      "y_test_pred: [0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "test score: 0.6368456748258216\n",
      "logisticregression\n",
      "y_test shape: (380,)\n",
      "0.13636363636363635\n",
      "(1516, 109) (1516,)\n",
      "Fitting 4 folds for each of 40 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henryjiang/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0        0.008601      0.000287         0.002851        0.000068   \n",
      "1        0.011057      0.000237         0.002886        0.000059   \n",
      "2        0.010609      0.001404         0.003299        0.000296   \n",
      "3        0.011640      0.000541         0.002920        0.000223   \n",
      "4        0.009049      0.000154         0.003002        0.000238   \n",
      "5        0.012638      0.000864         0.002823        0.000124   \n",
      "6        0.009134      0.000586         0.003323        0.000549   \n",
      "7        0.015407      0.001618         0.003175        0.000688   \n",
      "8        0.009527      0.000460         0.002877        0.000060   \n",
      "9        0.017575      0.000960         0.003766        0.000979   \n",
      "10       0.011552      0.001343         0.003512        0.000943   \n",
      "11       0.026071      0.002867         0.003244        0.000595   \n",
      "12       0.017285      0.002547         0.004749        0.002091   \n",
      "13       0.035490      0.004265         0.008225        0.003700   \n",
      "14       0.026214      0.002185         0.005073        0.000670   \n",
      "15       0.039962      0.005199         0.006271        0.005133   \n",
      "16       0.040288      0.005279         0.004284        0.000802   \n",
      "17       0.058877      0.017678         0.006234        0.003411   \n",
      "18       0.206616      0.029731         0.004104        0.001068   \n",
      "19       0.060729      0.013637         0.006101        0.000975   \n",
      "20       0.314192      0.035895         0.003261        0.000166   \n",
      "21       0.074001      0.017364         0.004226        0.000632   \n",
      "22       1.092078      0.253227         0.004866        0.002226   \n",
      "23       0.095576      0.015588         0.004003        0.000278   \n",
      "24       2.066046      0.120148         0.003539        0.000368   \n",
      "25       0.117189      0.007575         0.003601        0.000743   \n",
      "26       2.299680      0.217767         0.003178        0.000209   \n",
      "27       0.104776      0.007254         0.003679        0.000891   \n",
      "28       2.461429      0.200512         0.003062        0.000203   \n",
      "29       0.145430      0.005825         0.003294        0.000337   \n",
      "30       2.406632      0.274125         0.003560        0.000770   \n",
      "31       0.124557      0.010412         0.003371        0.000355   \n",
      "32       2.459735      0.264259         0.003370        0.000623   \n",
      "33       0.140601      0.008005         0.004221        0.001303   \n",
      "34       2.516431      0.183276         0.003188        0.000246   \n",
      "35       0.141010      0.004978         0.004602        0.002864   \n",
      "36       2.660350      0.123206         0.004600        0.002138   \n",
      "37       0.136398      0.010293         0.002996        0.000095   \n",
      "38       2.653694      0.204032         0.003014        0.000061   \n",
      "39       0.132591      0.004211         0.004572        0.002765   \n",
      "\n",
      "   param_logisticregression__C param_logisticregression__penalty  \\\n",
      "0                       0.0001                                l1   \n",
      "1                       0.0001                                l2   \n",
      "2                     0.000264                                l1   \n",
      "3                     0.000264                                l2   \n",
      "4                     0.000695                                l1   \n",
      "5                     0.000695                                l2   \n",
      "6                     0.001833                                l1   \n",
      "7                     0.001833                                l2   \n",
      "8                     0.004833                                l1   \n",
      "9                     0.004833                                l2   \n",
      "10                    0.012743                                l1   \n",
      "11                    0.012743                                l2   \n",
      "12                    0.033598                                l1   \n",
      "13                    0.033598                                l2   \n",
      "14                    0.088587                                l1   \n",
      "15                    0.088587                                l2   \n",
      "16                    0.233572                                l1   \n",
      "17                    0.233572                                l2   \n",
      "18                    0.615848                                l1   \n",
      "19                    0.615848                                l2   \n",
      "20                    1.623777                                l1   \n",
      "21                    1.623777                                l2   \n",
      "22                    4.281332                                l1   \n",
      "23                    4.281332                                l2   \n",
      "24                   11.288379                                l1   \n",
      "25                   11.288379                                l2   \n",
      "26                   29.763514                                l1   \n",
      "27                   29.763514                                l2   \n",
      "28                   78.475997                                l1   \n",
      "29                   78.475997                                l2   \n",
      "30                  206.913808                                l1   \n",
      "31                  206.913808                                l2   \n",
      "32                  545.559478                                l1   \n",
      "33                  545.559478                                l2   \n",
      "34                 1438.449888                                l1   \n",
      "35                 1438.449888                                l2   \n",
      "36                 3792.690191                                l1   \n",
      "37                 3792.690191                                l2   \n",
      "38                     10000.0                                l1   \n",
      "39                     10000.0                                l2   \n",
      "\n",
      "   param_logisticregression__solver  \\\n",
      "0                         liblinear   \n",
      "1                         liblinear   \n",
      "2                         liblinear   \n",
      "3                         liblinear   \n",
      "4                         liblinear   \n",
      "5                         liblinear   \n",
      "6                         liblinear   \n",
      "7                         liblinear   \n",
      "8                         liblinear   \n",
      "9                         liblinear   \n",
      "10                        liblinear   \n",
      "11                        liblinear   \n",
      "12                        liblinear   \n",
      "13                        liblinear   \n",
      "14                        liblinear   \n",
      "15                        liblinear   \n",
      "16                        liblinear   \n",
      "17                        liblinear   \n",
      "18                        liblinear   \n",
      "19                        liblinear   \n",
      "20                        liblinear   \n",
      "21                        liblinear   \n",
      "22                        liblinear   \n",
      "23                        liblinear   \n",
      "24                        liblinear   \n",
      "25                        liblinear   \n",
      "26                        liblinear   \n",
      "27                        liblinear   \n",
      "28                        liblinear   \n",
      "29                        liblinear   \n",
      "30                        liblinear   \n",
      "31                        liblinear   \n",
      "32                        liblinear   \n",
      "33                        liblinear   \n",
      "34                        liblinear   \n",
      "35                        liblinear   \n",
      "36                        liblinear   \n",
      "37                        liblinear   \n",
      "38                        liblinear   \n",
      "39                        liblinear   \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "0   {'logisticregression__C': 0.0001, 'logisticreg...           0.409836   \n",
      "1   {'logisticregression__C': 0.0001, 'logisticreg...           0.628571   \n",
      "2   {'logisticregression__C': 0.000263665089873035...           0.409836   \n",
      "3   {'logisticregression__C': 0.000263665089873035...           0.699934   \n",
      "4   {'logisticregression__C': 0.000695192796177560...           0.409836   \n",
      "5   {'logisticregression__C': 0.000695192796177560...           0.666667   \n",
      "6   {'logisticregression__C': 0.001832980710832435...           0.409836   \n",
      "7   {'logisticregression__C': 0.001832980710832435...           0.609478   \n",
      "8   {'logisticregression__C': 0.004832930238571752...           0.409836   \n",
      "9   {'logisticregression__C': 0.004832930238571752...           0.600985   \n",
      "10  {'logisticregression__C': 0.012742749857031334...           0.409836   \n",
      "11  {'logisticregression__C': 0.012742749857031334...           0.621053   \n",
      "12  {'logisticregression__C': 0.03359818286283781,...           0.500000   \n",
      "13  {'logisticregression__C': 0.03359818286283781,...           0.650585   \n",
      "14  {'logisticregression__C': 0.08858667904100823,...           0.500000   \n",
      "15  {'logisticregression__C': 0.08858667904100823,...           0.629630   \n",
      "16  {'logisticregression__C': 0.23357214690901212,...           0.626536   \n",
      "17  {'logisticregression__C': 0.23357214690901212,...           0.629630   \n",
      "18  {'logisticregression__C': 0.615848211066026, '...           0.654458   \n",
      "19  {'logisticregression__C': 0.615848211066026, '...           0.584615   \n",
      "20  {'logisticregression__C': 1.623776739188721, '...           0.574545   \n",
      "21  {'logisticregression__C': 1.623776739188721, '...           0.573763   \n",
      "22  {'logisticregression__C': 4.281332398719396, '...           0.519226   \n",
      "23  {'logisticregression__C': 4.281332398719396, '...           0.563636   \n",
      "24  {'logisticregression__C': 11.288378916846883, ...           0.541818   \n",
      "25  {'logisticregression__C': 11.288378916846883, ...           0.541818   \n",
      "26  {'logisticregression__C': 29.763514416313132, ...           0.613319   \n",
      "27  {'logisticregression__C': 29.763514416313132, ...           0.563636   \n",
      "28  {'logisticregression__C': 78.47599703514607, '...           0.601702   \n",
      "29  {'logisticregression__C': 78.47599703514607, '...           0.613319   \n",
      "30  {'logisticregression__C': 206.913808111479, 'l...           0.601702   \n",
      "31  {'logisticregression__C': 206.913808111479, 'l...           0.601702   \n",
      "32  {'logisticregression__C': 545.5594781168514, '...           0.601702   \n",
      "33  {'logisticregression__C': 545.5594781168514, '...           0.601702   \n",
      "34  {'logisticregression__C': 1438.44988828766, 'l...           0.601702   \n",
      "35  {'logisticregression__C': 1438.44988828766, 'l...           0.601702   \n",
      "36  {'logisticregression__C': 3792.690190732246, '...           0.601702   \n",
      "37  {'logisticregression__C': 3792.690190732246, '...           0.601702   \n",
      "38  {'logisticregression__C': 10000.0, 'logisticre...           0.601702   \n",
      "39  {'logisticregression__C': 10000.0, 'logisticre...           0.601702   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
      "0            0.409836           0.409836           0.409836         0.409836   \n",
      "1            0.628571           0.628571           0.628571         0.628571   \n",
      "2            0.409836           0.409836           0.409836         0.409836   \n",
      "3            0.699934           0.699934           0.699934         0.699934   \n",
      "4            0.409836           0.409836           0.409836         0.409836   \n",
      "5            0.666667           0.666667           0.666667         0.666667   \n",
      "6            0.409836           0.409836           0.409836         0.409836   \n",
      "7            0.609478           0.609478           0.609478         0.609478   \n",
      "8            0.409836           0.409836           0.409836         0.409836   \n",
      "9            0.600985           0.600985           0.600985         0.600985   \n",
      "10           0.409836           0.409836           0.409836         0.409836   \n",
      "11           0.621053           0.621053           0.621053         0.621053   \n",
      "12           0.500000           0.500000           0.500000         0.500000   \n",
      "13           0.650585           0.650585           0.650585         0.650585   \n",
      "14           0.500000           0.500000           0.500000         0.500000   \n",
      "15           0.629630           0.629630           0.629630         0.629630   \n",
      "16           0.626536           0.626536           0.626536         0.626536   \n",
      "17           0.629630           0.629630           0.629630         0.629630   \n",
      "18           0.654458           0.654458           0.654458         0.654458   \n",
      "19           0.584615           0.584615           0.584615         0.584615   \n",
      "20           0.574545           0.574545           0.574545         0.574545   \n",
      "21           0.573763           0.573763           0.573763         0.573763   \n",
      "22           0.519226           0.519226           0.519226         0.519226   \n",
      "23           0.563636           0.563636           0.563636         0.563636   \n",
      "24           0.541818           0.541818           0.541818         0.541818   \n",
      "25           0.541818           0.541818           0.541818         0.541818   \n",
      "26           0.613319           0.613319           0.613319         0.613319   \n",
      "27           0.563636           0.563636           0.563636         0.563636   \n",
      "28           0.601702           0.601702           0.601702         0.601702   \n",
      "29           0.613319           0.613319           0.613319         0.613319   \n",
      "30           0.601702           0.601702           0.601702         0.601702   \n",
      "31           0.601702           0.601702           0.601702         0.601702   \n",
      "32           0.601702           0.601702           0.601702         0.601702   \n",
      "33           0.601702           0.601702           0.601702         0.601702   \n",
      "34           0.601702           0.601702           0.601702         0.601702   \n",
      "35           0.601702           0.601702           0.601702         0.601702   \n",
      "36           0.601702           0.601702           0.601702         0.601702   \n",
      "37           0.601702           0.601702           0.601702         0.601702   \n",
      "38           0.601702           0.601702           0.601702         0.601702   \n",
      "39           0.601702           0.601702           0.601702         0.601702   \n",
      "\n",
      "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
      "0              0.0               35            0.454683            0.454683   \n",
      "1              0.0                7            0.662637            0.662637   \n",
      "2              0.0               35            0.454683            0.454683   \n",
      "3              0.0                1            0.694887            0.694887   \n",
      "4              0.0               35            0.454683            0.454683   \n",
      "5              0.0                2            0.739614            0.739614   \n",
      "6              0.0               35            0.454683            0.454683   \n",
      "7              0.0               12            0.744563            0.744563   \n",
      "8              0.0               35            0.454683            0.454683   \n",
      "9              0.0               24            0.727868            0.727868   \n",
      "10             0.0               35            0.454683            0.454683   \n",
      "11             0.0                9            0.715752            0.715752   \n",
      "12             0.0               33            0.626581            0.626581   \n",
      "13             0.0                4            0.739394            0.739394   \n",
      "14             0.0               33            0.692422            0.692422   \n",
      "15             0.0                5            0.752181            0.752181   \n",
      "16             0.0                8            0.724810            0.724810   \n",
      "17             0.0                5            0.760247            0.760247   \n",
      "18             0.0                3            0.759202            0.759202   \n",
      "19             0.0               25            0.765009            0.765009   \n",
      "20             0.0               26            0.765968            0.765968   \n",
      "21             0.0               27            0.773473            0.773473   \n",
      "22             0.0               32            0.774323            0.774323   \n",
      "23             0.0               28            0.778939            0.778939   \n",
      "24             0.0               30            0.788940            0.788940   \n",
      "25             0.0               30            0.787973            0.787973   \n",
      "26             0.0               10            0.787010            0.787010   \n",
      "27             0.0               28            0.790727            0.790727   \n",
      "28             0.0               13            0.785220            0.785220   \n",
      "29             0.0               10            0.787830            0.787830   \n",
      "30             0.0               13            0.785220            0.785220   \n",
      "31             0.0               13            0.787010            0.787010   \n",
      "32             0.0               13            0.783422            0.782466   \n",
      "33             0.0               13            0.785220            0.785220   \n",
      "34             0.0               13            0.782466            0.783422   \n",
      "35             0.0               13            0.782466            0.782466   \n",
      "36             0.0               13            0.782466            0.782466   \n",
      "37             0.0               13            0.782466            0.782466   \n",
      "38             0.0               13            0.782466            0.782466   \n",
      "39             0.0               13            0.782466            0.782466   \n",
      "\n",
      "    split2_train_score  split3_train_score  mean_train_score  std_train_score  \n",
      "0             0.454683            0.454683          0.454683         0.000000  \n",
      "1             0.662637            0.662637          0.662637         0.000000  \n",
      "2             0.454683            0.454683          0.454683         0.000000  \n",
      "3             0.694887            0.694887          0.694887         0.000000  \n",
      "4             0.454683            0.454683          0.454683         0.000000  \n",
      "5             0.739614            0.739614          0.739614         0.000000  \n",
      "6             0.454683            0.454683          0.454683         0.000000  \n",
      "7             0.744563            0.744563          0.744563         0.000000  \n",
      "8             0.454683            0.454683          0.454683         0.000000  \n",
      "9             0.727868            0.727868          0.727868         0.000000  \n",
      "10            0.454683            0.454683          0.454683         0.000000  \n",
      "11            0.715752            0.715752          0.715752         0.000000  \n",
      "12            0.626581            0.626581          0.626581         0.000000  \n",
      "13            0.739394            0.739394          0.739394         0.000000  \n",
      "14            0.692422            0.692422          0.692422         0.000000  \n",
      "15            0.752181            0.752181          0.752181         0.000000  \n",
      "16            0.724810            0.724810          0.724810         0.000000  \n",
      "17            0.760247            0.760247          0.760247         0.000000  \n",
      "18            0.759202            0.759202          0.759202         0.000000  \n",
      "19            0.765009            0.765009          0.765009         0.000000  \n",
      "20            0.765968            0.765968          0.765968         0.000000  \n",
      "21            0.773473            0.773473          0.773473         0.000000  \n",
      "22            0.774323            0.774323          0.774323         0.000000  \n",
      "23            0.778939            0.778939          0.778939         0.000000  \n",
      "24            0.788940            0.788940          0.788940         0.000000  \n",
      "25            0.787973            0.787973          0.787973         0.000000  \n",
      "26            0.787010            0.787010          0.787010         0.000000  \n",
      "27            0.790727            0.790727          0.790727         0.000000  \n",
      "28            0.785220            0.785220          0.785220         0.000000  \n",
      "29            0.787830            0.787830          0.787830         0.000000  \n",
      "30            0.785220            0.785220          0.785220         0.000000  \n",
      "31            0.787010            0.787010          0.787010         0.000000  \n",
      "32            0.783422            0.783422          0.783183         0.000414  \n",
      "33            0.785220            0.785220          0.785220         0.000000  \n",
      "34            0.782466            0.782466          0.782705         0.000414  \n",
      "35            0.782466            0.782466          0.782466         0.000000  \n",
      "36            0.783422            0.782466          0.782705         0.000414  \n",
      "37            0.782466            0.782466          0.782466         0.000000  \n",
      "38            0.782466            0.782466          0.782466         0.000000  \n",
      "39            0.782466            0.782466          0.782466         0.000000  \n",
      "y_test_pred: [1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0\n",
      " 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 1\n",
      " 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1\n",
      " 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1\n",
      " 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0\n",
      " 1 1 0 1 0 0 0 1 0 0]\n",
      "test score: 0.6571926295378425\n",
      "the overall mean of all 10 models is: 0.6877950288788008 and the standard deviation is: 0.03459636092936912\n",
      "0.14664915540839632\n",
      "The model is on average 15.641699269330422 better than the baseline\n",
      "[0.6998275056020378, 0.7138517696008986, 0.6637426900584795, 0.7120867727399609, 0.738205467372134, 0.7198618709502191, 0.6357298474945534, 0.7006060606060607, 0.6368456748258216, 0.6571926295378425]\n",
      "0.14664915540839632\n"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "grid={\n",
    "     'logisticregression__penalty' : ['l1', 'l2'],\n",
    "    'logisticregression__C' : np.logspace(-4, 4, 20),\n",
    "    'logisticregression__solver' : ['liblinear']}\n",
    "    \n",
    "clf = LogisticRegression()\n",
    "\n",
    "lr_model_mean,lr_model_std, lr_test_scores_ls,lr_best_models,lr_avg_bl,lr_y_pred_ls,lr_y_act_ls,lr_X_train_ls,lr_X_test_ls=MLpipe_GroupShuffleSplit_F1(X_test,y_test,preprocessor, clf,grid)\n",
    "print(lr_test_scores_ls)\n",
    "print(lr_avg_bl)\n",
    "\n",
    "idx_best_lr_mod=np.argmax(lr_test_scores_ls)\n",
    "best_lr_model=lr_best_models[idx_best_lr_mod]\n",
    "\n",
    "lr_y_pred=lr_y_pred_ls[idx_best_lr_mod]\n",
    "lr_y_act=lr_y_act_ls[idx_best_lr_mod]\n",
    "\n",
    "lr_X_train_set=pd.DataFrame(lr_X_train_ls[idx_best_lr_mod])\n",
    "lr_X_test_set=pd.DataFrame(lr_X_test_ls[idx_best_lr_mod])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2b74879d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr_y_pred</th>\n",
       "      <th>lr_y_act</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lr_y_pred  lr_y_act\n",
       "0          0         0\n",
       "1          0         0\n",
       "2          0         0\n",
       "3          0         0\n",
       "4          0         0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a data frame of the predictions and actual\n",
    "lr_results_df=pd.DataFrame({'lr_y_pred':lr_y_pred,'lr_y_act':lr_y_act}).reset_index(drop=True)\n",
    "lr_results_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c0f7b6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr_y_pred</th>\n",
       "      <th>lr_y_act</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lr_y_pred  lr_y_act\n",
       "25          0         1\n",
       "52          0         1\n",
       "66          0         1\n",
       "69          0         1\n",
       "92          0         1"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get false negatives\n",
    "df_false_neg_lr=lr_results_df[(lr_results_df['lr_y_pred']==0) & (lr_results_df['lr_y_act']==1)]\n",
    "df_false_neg_lr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b57042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get matrix of weights/coeff of logistic regression\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(8, 6), dpi=80)\n",
    "clf.fit(X_test,y_test)\n",
    "importance = clf.coef_[0]\n",
    "#importance is a list so you can plot it. \n",
    "feat_importances = pd.Series(importance, index = X_test.columns)\n",
    "fig=feat_importances.abs().nlargest(20).plot(kind='barh',title = 'Feature Importance')\n",
    "plt.tick_params(axis='x',which='major', pad=15)\n",
    "figure(figsize=(8, 6), dpi=80)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45988a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get confusion matrix\n",
    "cm=confusion_matrix(lr_y_act_ls[idx_best_lr_mod],lr_y_pred_ls[idx_best_lr_mod])\n",
    "plt.figure(figsize = (10,7))\n",
    "ax = plt.axes()\n",
    "sns.heatmap(cm/np.sum(cm),fmt='.2%', annot=True, cmap='Blues',ax=ax)\n",
    "ax.set_title('Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "324d5ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iid</th>\n",
       "      <th>gender</th>\n",
       "      <th>condtn</th>\n",
       "      <th>wave</th>\n",
       "      <th>round</th>\n",
       "      <th>position</th>\n",
       "      <th>positin1</th>\n",
       "      <th>order</th>\n",
       "      <th>pid</th>\n",
       "      <th>int_corr</th>\n",
       "      <th>samerace</th>\n",
       "      <th>age_o</th>\n",
       "      <th>race_o</th>\n",
       "      <th>pf_o_att</th>\n",
       "      <th>pf_o_sin</th>\n",
       "      <th>pf_o_int</th>\n",
       "      <th>pf_o_fun</th>\n",
       "      <th>pf_o_amb</th>\n",
       "      <th>pf_o_sha</th>\n",
       "      <th>attr_o</th>\n",
       "      <th>sinc_o</th>\n",
       "      <th>intel_o</th>\n",
       "      <th>fun_o</th>\n",
       "      <th>amb_o</th>\n",
       "      <th>shar_o</th>\n",
       "      <th>like_o</th>\n",
       "      <th>prob_o</th>\n",
       "      <th>met_o</th>\n",
       "      <th>age</th>\n",
       "      <th>field_cd</th>\n",
       "      <th>mn_sat</th>\n",
       "      <th>tuition</th>\n",
       "      <th>race</th>\n",
       "      <th>imprace</th>\n",
       "      <th>imprelig</th>\n",
       "      <th>income</th>\n",
       "      <th>goal</th>\n",
       "      <th>date</th>\n",
       "      <th>go_out</th>\n",
       "      <th>career_c</th>\n",
       "      <th>sports</th>\n",
       "      <th>tvsports</th>\n",
       "      <th>exercise</th>\n",
       "      <th>dining</th>\n",
       "      <th>museums</th>\n",
       "      <th>art</th>\n",
       "      <th>hiking</th>\n",
       "      <th>gaming</th>\n",
       "      <th>clubbing</th>\n",
       "      <th>reading</th>\n",
       "      <th>tv</th>\n",
       "      <th>theater</th>\n",
       "      <th>movies</th>\n",
       "      <th>concerts</th>\n",
       "      <th>music</th>\n",
       "      <th>shopping</th>\n",
       "      <th>yoga</th>\n",
       "      <th>exphappy</th>\n",
       "      <th>expnum</th>\n",
       "      <th>attr1_1</th>\n",
       "      <th>sinc1_1</th>\n",
       "      <th>intel1_1</th>\n",
       "      <th>fun1_1</th>\n",
       "      <th>amb1_1</th>\n",
       "      <th>shar1_1</th>\n",
       "      <th>attr4_1</th>\n",
       "      <th>sinc4_1</th>\n",
       "      <th>intel4_1</th>\n",
       "      <th>fun4_1</th>\n",
       "      <th>amb4_1</th>\n",
       "      <th>shar4_1</th>\n",
       "      <th>attr2_1</th>\n",
       "      <th>sinc2_1</th>\n",
       "      <th>intel2_1</th>\n",
       "      <th>fun2_1</th>\n",
       "      <th>amb2_1</th>\n",
       "      <th>shar2_1</th>\n",
       "      <th>attr3_1</th>\n",
       "      <th>sinc3_1</th>\n",
       "      <th>fun3_1</th>\n",
       "      <th>intel3_1</th>\n",
       "      <th>amb3_1</th>\n",
       "      <th>attr5_1</th>\n",
       "      <th>sinc5_1</th>\n",
       "      <th>intel5_1</th>\n",
       "      <th>fun5_1</th>\n",
       "      <th>amb5_1</th>\n",
       "      <th>attr</th>\n",
       "      <th>sinc</th>\n",
       "      <th>intel</th>\n",
       "      <th>fun</th>\n",
       "      <th>amb</th>\n",
       "      <th>shar</th>\n",
       "      <th>like</th>\n",
       "      <th>prob</th>\n",
       "      <th>met</th>\n",
       "      <th>match_es</th>\n",
       "      <th>attr1_s</th>\n",
       "      <th>sinc1_s</th>\n",
       "      <th>intel1_s</th>\n",
       "      <th>fun1_s</th>\n",
       "      <th>amb1_s</th>\n",
       "      <th>shar1_s</th>\n",
       "      <th>attr3_s</th>\n",
       "      <th>sinc3_s</th>\n",
       "      <th>intel3_s</th>\n",
       "      <th>fun3_s</th>\n",
       "      <th>amb3_s</th>\n",
       "      <th>zip_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>230</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>17.0</td>\n",
       "      <td>20</td>\n",
       "      <td>206</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.58</td>\n",
       "      <td>16.67</td>\n",
       "      <td>20.83</td>\n",
       "      <td>18.75</td>\n",
       "      <td>16.67</td>\n",
       "      <td>12.50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39123.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>23.81</td>\n",
       "      <td>23.81</td>\n",
       "      <td>23.81</td>\n",
       "      <td>23.81</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.38</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.32</td>\n",
       "      <td>24.32</td>\n",
       "      <td>18.92</td>\n",
       "      <td>16.22</td>\n",
       "      <td>8.11</td>\n",
       "      <td>8.11</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>221</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8</td>\n",
       "      <td>198</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.38</td>\n",
       "      <td>19.23</td>\n",
       "      <td>17.31</td>\n",
       "      <td>19.23</td>\n",
       "      <td>9.62</td>\n",
       "      <td>19.23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>15.56</td>\n",
       "      <td>17.78</td>\n",
       "      <td>17.78</td>\n",
       "      <td>17.78</td>\n",
       "      <td>17.78</td>\n",
       "      <td>13.33</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.69</td>\n",
       "      <td>19.61</td>\n",
       "      <td>19.61</td>\n",
       "      <td>15.69</td>\n",
       "      <td>17.65</td>\n",
       "      <td>11.76</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>15.91</td>\n",
       "      <td>18.18</td>\n",
       "      <td>18.18</td>\n",
       "      <td>15.91</td>\n",
       "      <td>18.18</td>\n",
       "      <td>13.64</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>215</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>201</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.38</td>\n",
       "      <td>17.31</td>\n",
       "      <td>17.31</td>\n",
       "      <td>17.31</td>\n",
       "      <td>17.31</td>\n",
       "      <td>15.38</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53501.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>14.89</td>\n",
       "      <td>19.15</td>\n",
       "      <td>21.28</td>\n",
       "      <td>19.15</td>\n",
       "      <td>14.89</td>\n",
       "      <td>10.64</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.63</td>\n",
       "      <td>23.26</td>\n",
       "      <td>16.28</td>\n",
       "      <td>20.93</td>\n",
       "      <td>11.63</td>\n",
       "      <td>16.28</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>15.56</td>\n",
       "      <td>17.78</td>\n",
       "      <td>20.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>15.56</td>\n",
       "      <td>11.11</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>347</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15</td>\n",
       "      <td>369</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>27.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>10222.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19264.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>30.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>210</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>216</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.50</td>\n",
       "      <td>17.50</td>\n",
       "      <td>20.00</td>\n",
       "      <td>22.50</td>\n",
       "      <td>12.50</td>\n",
       "      <td>20.00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>46800.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>15.56</td>\n",
       "      <td>20.00</td>\n",
       "      <td>15.56</td>\n",
       "      <td>11.11</td>\n",
       "      <td>17.78</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.00</td>\n",
       "      <td>22.22</td>\n",
       "      <td>13.89</td>\n",
       "      <td>13.89</td>\n",
       "      <td>2.78</td>\n",
       "      <td>22.22</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.64</td>\n",
       "      <td>10.26</td>\n",
       "      <td>15.38</td>\n",
       "      <td>20.51</td>\n",
       "      <td>10.26</td>\n",
       "      <td>17.95</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     iid  gender  condtn  wave  round  position  positin1  order  pid  \\\n",
       "732  230       1       2     9     20        16      17.0     20  206   \n",
       "544  221       1       2     9     20        14       7.0      8  198   \n",
       "427  215       1       2     9     20         9       3.0      7  201   \n",
       "916  347       0       2    14     18        14      14.0     15  369   \n",
       "322  210       0       2     9     20         1       1.0      6  216   \n",
       "\n",
       "     int_corr  samerace  age_o  race_o  pf_o_att  pf_o_sin  pf_o_int  \\\n",
       "732      0.09         0   31.0     1.0     14.58     16.67     20.83   \n",
       "544      0.05         1   23.0     4.0     15.38     19.23     17.31   \n",
       "427      0.41         0   25.0     2.0     15.38     17.31     17.31   \n",
       "916     -0.19         0   25.0     2.0     17.00     18.00     28.00   \n",
       "322      0.37         1   27.0     2.0      7.50     17.50     20.00   \n",
       "\n",
       "     pf_o_fun  pf_o_amb  pf_o_sha  attr_o  sinc_o  intel_o  fun_o  amb_o  \\\n",
       "732     18.75     16.67     12.50     7.0     7.0      7.0    7.0    7.0   \n",
       "544     19.23      9.62     19.23     3.0     5.0      6.0    4.0    5.0   \n",
       "427     17.31     17.31     15.38     7.0     8.0      8.0    8.0    8.0   \n",
       "916     27.00      5.00      5.00     4.0     4.0      5.0    4.0    4.0   \n",
       "322     22.50     12.50     20.00     8.0     9.0      9.0    9.0    9.0   \n",
       "\n",
       "     shar_o  like_o  prob_o  met_o   age  field_cd  mn_sat  tuition  race  \\\n",
       "732     7.0     6.0     5.0    2.0  42.0      13.0    -1.0     -1.0   2.0   \n",
       "544     5.0     3.0     3.0    2.0  27.0       8.0    -1.0     -1.0   4.0   \n",
       "427     7.0     7.0     6.0    2.0  26.0       8.0    -1.0     -1.0   4.0   \n",
       "916     3.0     5.0     7.0    2.0  26.0       9.0    -1.0  10222.0   1.0   \n",
       "322     8.0     8.0     6.0    2.0  29.0      10.0    -1.0     -1.0   2.0   \n",
       "\n",
       "     imprace  imprelig   income  goal  date  go_out  career_c  sports  \\\n",
       "732      5.0       1.0  39123.0   2.0   7.0     3.0       7.0     1.0   \n",
       "544      6.0       1.0     -1.0   2.0   6.0     3.0       7.0     5.0   \n",
       "427      3.0       1.0  53501.0   1.0   4.0     1.0       7.0     8.0   \n",
       "916      1.0       1.0  19264.0   5.0   6.0     1.0      13.0     1.0   \n",
       "322     10.0       4.0  46800.0   2.0   6.0     3.0       2.0     9.0   \n",
       "\n",
       "     tvsports  exercise  dining  museums   art  hiking  gaming  clubbing  \\\n",
       "732       1.0       2.0     1.0     10.0  10.0    10.0     1.0       1.0   \n",
       "544       6.0       3.0     7.0      6.0   3.0     4.0     3.0       7.0   \n",
       "427       6.0       9.0     9.0      6.0   4.0     2.0     7.0       6.0   \n",
       "916       1.0       2.0    10.0      3.0   3.0     1.0     1.0       8.0   \n",
       "322       3.0       9.0     8.0      9.0   9.0     9.0     1.0       2.0   \n",
       "\n",
       "     reading    tv  theater  movies  concerts  music  shopping  yoga  \\\n",
       "732     10.0  10.0     10.0    10.0       5.0   10.0      10.0   6.0   \n",
       "544      8.0   7.0      8.0     8.0       4.0    4.0       4.0   1.0   \n",
       "427      9.0   5.0      2.0     9.0       2.0    7.0       9.0   2.0   \n",
       "916      2.0   5.0      3.0     8.0       1.0    7.0      10.0   1.0   \n",
       "322      9.0   5.0      7.0     7.0       7.0    7.0       5.0   1.0   \n",
       "\n",
       "     exphappy  expnum  attr1_1  sinc1_1  intel1_1  fun1_1  amb1_1  shar1_1  \\\n",
       "732       5.0    -1.0    23.81    23.81     23.81   23.81    2.38     2.38   \n",
       "544       6.0    -1.0    15.56    17.78     17.78   17.78   17.78    13.33   \n",
       "427       7.0    -1.0    14.89    19.15     21.28   19.15   14.89    10.64   \n",
       "916       5.0    -1.0    30.00     5.00     10.00   20.00   30.00     5.00   \n",
       "322       5.0    -1.0    20.00    15.56     20.00   15.56   11.11    17.78   \n",
       "\n",
       "     attr4_1  sinc4_1  intel4_1  fun4_1  amb4_1  shar4_1  attr2_1  sinc2_1  \\\n",
       "732     10.0     10.0      10.0    10.0    10.0     10.0    16.67    16.67   \n",
       "544      9.0      4.0       3.0    10.0     3.0      6.0    15.69    19.61   \n",
       "427     10.0      9.0       6.0    10.0     3.0      8.0    11.63    23.26   \n",
       "916     30.0      5.0      10.0    20.0    30.0      5.0    30.00     5.00   \n",
       "322      9.0      8.0       4.0     8.0     4.0      9.0    25.00    22.22   \n",
       "\n",
       "     intel2_1  fun2_1  amb2_1  shar2_1  attr3_1  sinc3_1  fun3_1  intel3_1  \\\n",
       "732     16.67   16.67   16.67    16.67      6.0      6.0     6.0       6.0   \n",
       "544     19.61   15.69   17.65    11.76      7.0      9.0     7.0       9.0   \n",
       "427     16.28   20.93   11.63    16.28      7.0      8.0     9.0       9.0   \n",
       "916     10.00   20.00   30.00     5.00      8.0      6.0    10.0      10.0   \n",
       "322     13.89   13.89    2.78    22.22      8.0      8.0     6.0       9.0   \n",
       "\n",
       "     amb3_1  attr5_1  sinc5_1  intel5_1  fun5_1  amb5_1  attr  sinc  intel  \\\n",
       "732     6.0     -1.0     -1.0      -1.0    -1.0    -1.0   6.0   8.0    9.0   \n",
       "544     9.0     -1.0     -1.0      -1.0    -1.0    -1.0   3.0   5.0    6.0   \n",
       "427     9.0     -1.0     -1.0      -1.0    -1.0    -1.0   6.0   8.0    9.0   \n",
       "916    10.0     10.0      2.0      10.0     8.0     9.0   7.0  10.0    9.0   \n",
       "322     9.0     -1.0     -1.0      -1.0    -1.0    -1.0   1.0   1.0    1.0   \n",
       "\n",
       "      fun  amb  shar  like  prob  met  match_es  attr1_s  sinc1_s  intel1_s  \\\n",
       "732   8.0  7.0   7.0   6.0   5.0  2.0       1.0    24.32    24.32     18.92   \n",
       "544   3.0  7.0   3.0   3.0   6.0  2.0      -1.0    15.91    18.18     18.18   \n",
       "427   5.0  8.0   5.0   6.0   5.0  2.0      -1.0    15.56    17.78     20.00   \n",
       "916  10.0  9.0   5.0   8.0   4.0  0.0       2.0    -1.00    -1.00     -1.00   \n",
       "322   1.0  1.0   1.0   1.0  -1.0  2.0       1.0    25.64    10.26     15.38   \n",
       "\n",
       "     fun1_s  amb1_s  shar1_s  attr3_s  sinc3_s  intel3_s  fun3_s  amb3_s  \\\n",
       "732   16.22    8.11     8.11      6.0      8.0       7.0     7.0     5.0   \n",
       "544   15.91   18.18    13.64      7.0      8.0       8.0     6.0     9.0   \n",
       "427   20.00   15.56    11.11      7.0      9.0       9.0     8.0     9.0   \n",
       "916   -1.00   -1.00    -1.00     -1.0     -1.0      -1.0    -1.0    -1.0   \n",
       "322   20.51   10.26    17.95      8.0      8.0       7.0     8.0     7.0   \n",
       "\n",
       "     zip_match  \n",
       "732          0  \n",
       "544          0  \n",
       "427          0  \n",
       "916          0  \n",
       "322          0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_X_train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "77e70871",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap global importance\n",
    "explainer = shap.Explainer(best_lr_model.best_estimator_.__getitem__(1),lr_X_train_set,features=lr_X_train_set.columns)\n",
    "shap_values = explainer(lr_X_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "99b94c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e6591d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id='i0HCD0G1FGDA9FT2BW85J'>\n",
       "<div style='color: #900; text-align: center;'>\n",
       "  <b>Visualization omitted, Javascript library not loaded!</b><br>\n",
       "  Have you run `initjs()` in this notebook? If this notebook was from another\n",
       "  user you must also trust this notebook (File -> Trust notebook). If you are viewing\n",
       "  this notebook on github the Javascript has been stripped for security. If you are using\n",
       "  JupyterLab this error is because a JupyterLab extension has not yet been written.\n",
       "</div></div>\n",
       " <script>\n",
       "   if (window.SHAP) SHAP.ReactDom.render(\n",
       "    SHAP.React.createElement(SHAP.AdditiveForceVisualizer, {\"outNames\": [\"f(x)\"], \"baseValue\": 341.21822638604567, \"outValue\": 7.321505954158226, \"link\": \"identity\", \"featureNames\": [\"iid\", \"gender\", \"condtn\", \"wave\", \"round\", \"position\", \"positin1\", \"order\", \"pid\", \"int_corr\", \"samerace\", \"age_o\", \"race_o\", \"pf_o_att\", \"pf_o_sin\", \"pf_o_int\", \"pf_o_fun\", \"pf_o_amb\", \"pf_o_sha\", \"attr_o\", \"sinc_o\", \"intel_o\", \"fun_o\", \"amb_o\", \"shar_o\", \"like_o\", \"prob_o\", \"met_o\", \"age\", \"field_cd\", \"mn_sat\", \"tuition\", \"race\", \"imprace\", \"imprelig\", \"income\", \"goal\", \"date\", \"go_out\", \"career_c\", \"sports\", \"tvsports\", \"exercise\", \"dining\", \"museums\", \"art\", \"hiking\", \"gaming\", \"clubbing\", \"reading\", \"tv\", \"theater\", \"movies\", \"concerts\", \"music\", \"shopping\", \"yoga\", \"exphappy\", \"expnum\", \"attr1_1\", \"sinc1_1\", \"intel1_1\", \"fun1_1\", \"amb1_1\", \"shar1_1\", \"attr4_1\", \"sinc4_1\", \"intel4_1\", \"fun4_1\", \"amb4_1\", \"shar4_1\", \"attr2_1\", \"sinc2_1\", \"intel2_1\", \"fun2_1\", \"amb2_1\", \"shar2_1\", \"attr3_1\", \"sinc3_1\", \"fun3_1\", \"intel3_1\", \"amb3_1\", \"attr5_1\", \"sinc5_1\", \"intel5_1\", \"fun5_1\", \"amb5_1\", \"attr\", \"sinc\", \"intel\", \"fun\", \"amb\", \"shar\", \"like\", \"prob\", \"met\", \"match_es\", \"attr1_s\", \"sinc1_s\", \"intel1_s\", \"fun1_s\", \"amb1_s\", \"shar1_s\", \"attr3_s\", \"sinc3_s\", \"intel3_s\", \"fun3_s\", \"amb3_s\", \"zip_match\"], \"features\": {\"0\": {\"effect\": -0.50400872920725, \"value\": 228.0}, \"1\": {\"effect\": -0.0038620688044277234, \"value\": 1.0}, \"2\": {\"effect\": -7.426203186884116e-05, \"value\": 2.0}, \"3\": {\"effect\": -0.014249446252921955, \"value\": 9.0}, \"4\": {\"effect\": 0.0027147948500360402, \"value\": 20.0}, \"5\": {\"effect\": 0.00037404397740981244, \"value\": 10.0}, \"6\": {\"effect\": 0.020116222806994394, \"value\": 9.0}, \"7\": {\"effect\": 0.07459603796610088, \"value\": 2.0}, \"8\": {\"effect\": -0.5897896255090846, \"value\": 212.0}, \"9\": {\"effect\": 0.0004287146151972327, \"value\": 0.14}, \"10\": {\"effect\": 0.008223517164459503, \"value\": 0.0}, \"11\": {\"effect\": 0.005982734602895339, \"value\": 27.0}, \"12\": {\"effect\": -0.007211314183571544, \"value\": 3.0}, \"13\": {\"effect\": 0.2017856777145588, \"value\": 14.29}, \"14\": {\"effect\": -0.002930043738742508, \"value\": 18.37}, \"15\": {\"effect\": -0.015444632366288978, \"value\": 18.37}, \"16\": {\"effect\": 0.005740484941870971, \"value\": 16.33}, \"17\": {\"effect\": 0.02774479569114781, \"value\": 18.37}, \"18\": {\"effect\": -0.018425781798659793, \"value\": 14.29}, \"19\": {\"effect\": 0.01002474884516364, \"value\": 6.0}, \"20\": {\"effect\": 0.027273784398347656, \"value\": 10.0}, \"21\": {\"effect\": 0.025595515351527945, \"value\": 10.0}, \"22\": {\"effect\": 0.0055476455949001555, \"value\": 6.0}, \"23\": {\"effect\": 0.012356941496228737, \"value\": 10.0}, \"24\": {\"effect\": 0.08973838870755371, \"value\": 6.0}, \"25\": {\"effect\": 0.018287248895583565, \"value\": 6.0}, \"26\": {\"effect\": 0.1643607372289491, \"value\": 8.0}, \"27\": {\"effect\": -0.017135977077946122, \"value\": 2.0}, \"28\": {\"effect\": -0.047153856492379395, \"value\": 24.0}, \"29\": {\"effect\": -0.04135573629058125, \"value\": 10.0}, \"30\": {\"effect\": 5.420975891313606, \"value\": -1.0}, \"31\": {\"effect\": 47.16644667317842, \"value\": -1.0}, \"32\": {\"effect\": -0.023918771734208276, \"value\": 4.0}, \"33\": {\"effect\": 0.19137707075512136, \"value\": 1.0}, \"34\": {\"effect\": -0.00209343119015206, \"value\": 4.0}, \"35\": {\"effect\": -387.156979298289, \"value\": -1.0}, \"36\": {\"effect\": 0.0026151510298064863, \"value\": 3.0}, \"37\": {\"effect\": -0.05131403732748237, \"value\": 7.0}, \"38\": {\"effect\": -0.0066277886744893115, \"value\": 3.0}, \"39\": {\"effect\": -0.03963578560324047, \"value\": 2.0}, \"40\": {\"effect\": 0.054448149151878664, \"value\": 3.0}, \"41\": {\"effect\": -0.0414805415101462, \"value\": 1.0}, \"42\": {\"effect\": -0.0028153493596711023, \"value\": 2.0}, \"43\": {\"effect\": -0.00021441277829953624, \"value\": 8.0}, \"44\": {\"effect\": -0.019995174024388218, \"value\": 4.0}, \"45\": {\"effect\": 0.0015656195519432512, \"value\": 5.0}, \"46\": {\"effect\": 0.029923876303710205, \"value\": 8.0}, \"47\": {\"effect\": 0.00016792446196227308, \"value\": 4.0}, \"48\": {\"effect\": -0.004744968600721118, \"value\": 6.0}, \"49\": {\"effect\": 0.02762577682600732, \"value\": 9.0}, \"50\": {\"effect\": -0.0025991443094644795, \"value\": 2.0}, \"51\": {\"effect\": -0.03198028610667615, \"value\": 4.0}, \"52\": {\"effect\": 0.01649718572465383, \"value\": 7.0}, \"53\": {\"effect\": 0.007823812852520123, \"value\": 8.0}, \"54\": {\"effect\": 0.005061657269638805, \"value\": 9.0}, \"55\": {\"effect\": 0.07121134965038167, \"value\": 2.0}, \"56\": {\"effect\": 0.025343145016811234, \"value\": 8.0}, \"57\": {\"effect\": -0.00202348620738722, \"value\": 7.0}, \"59\": {\"effect\": 0.18608515097855932, \"value\": 13.51}, \"60\": {\"effect\": -0.008627992394631818, \"value\": 18.92}, \"61\": {\"effect\": 0.06444488032667645, \"value\": 21.62}, \"62\": {\"effect\": -0.06441868079475375, \"value\": 13.51}, \"63\": {\"effect\": 0.008610582325703673, \"value\": 13.51}, \"64\": {\"effect\": -0.17284538889081685, \"value\": 18.92}, \"65\": {\"effect\": 0.5086975968466729, \"value\": 8.0}, \"66\": {\"effect\": 0.22378498743428735, \"value\": 3.0}, \"67\": {\"effect\": -0.003532594992066986, \"value\": 5.0}, \"68\": {\"effect\": 0.0036489006888220875, \"value\": 7.0}, \"69\": {\"effect\": 0.028193546364946642, \"value\": 4.0}, \"70\": {\"effect\": -0.025970180562623933, \"value\": 7.0}, \"71\": {\"effect\": 0.05429444347097526, \"value\": 15.38}, \"72\": {\"effect\": -0.0022274652335081283, \"value\": 10.26}, \"73\": {\"effect\": 0.1063865660040738, \"value\": 20.51}, \"74\": {\"effect\": 0.0033735018952273737, \"value\": 20.51}, \"75\": {\"effect\": -0.025518729982211576, \"value\": 15.38}, \"76\": {\"effect\": 0.03381329589879085, \"value\": 17.95}, \"77\": {\"effect\": 0.014643079568415797, \"value\": 5.0}, \"78\": {\"effect\": -0.0007622017305891669, \"value\": 8.0}, \"79\": {\"effect\": 0.1012445552101275, \"value\": 4.0}, \"80\": {\"effect\": 0.02732536982181938, \"value\": 7.0}, \"81\": {\"effect\": 0.00011351047391633066, \"value\": 8.0}, \"82\": {\"effect\": 0.022095881560416652, \"value\": -1.0}, \"83\": {\"effect\": -0.01426274428356584, \"value\": -1.0}, \"84\": {\"effect\": -0.031876272354943606, \"value\": -1.0}, \"85\": {\"effect\": -0.015467589096818179, \"value\": -1.0}, \"86\": {\"effect\": -0.06091625702247249, \"value\": -1.0}, \"87\": {\"effect\": 0.05111424326670492, \"value\": 7.0}, \"88\": {\"effect\": -0.012376031796163759, \"value\": 6.0}, \"90\": {\"effect\": 0.04889803609850125, \"value\": 7.0}, \"91\": {\"effect\": -0.0024400129215249792, \"value\": 6.0}, \"92\": {\"effect\": 0.07891784369658537, \"value\": 6.0}, \"93\": {\"effect\": -0.00990041402539187, \"value\": 6.0}, \"94\": {\"effect\": -0.0789946935102164, \"value\": 4.0}, \"95\": {\"effect\": -0.02673597438385007, \"value\": 2.0}, \"96\": {\"effect\": -0.013599897140514133, \"value\": 3.0}, \"97\": {\"effect\": 0.016209335529481065, \"value\": 17.24}, \"98\": {\"effect\": -0.0039802420358230105, \"value\": 10.34}, \"99\": {\"effect\": 0.019935232084817973, \"value\": 20.69}, \"100\": {\"effect\": -0.013258397679548648, \"value\": 13.79}, \"101\": {\"effect\": -0.03435596107607337, \"value\": 13.79}, \"102\": {\"effect\": -0.0014506140433914573, \"value\": 24.14}, \"103\": {\"effect\": 0.008371969206281823, \"value\": 5.0}, \"104\": {\"effect\": 0.013145376563079398, \"value\": 7.0}, \"105\": {\"effect\": 0.010477324331862166, \"value\": 8.0}, \"106\": {\"effect\": -0.00018594107790580883, \"value\": 4.0}, \"107\": {\"effect\": 0.01935844356806292, \"value\": 7.0}, \"108\": {\"effect\": -0.00011117653931326167, \"value\": 0.0}}, \"plot_cmap\": \"RdBu\", \"labelMargin\": 20}),\n",
       "    document.getElementById('i0HCD0G1FGDA9FT2BW85J')\n",
       "  );\n",
       "</script>"
      ],
      "text/plain": [
       "<shap.plots._force.AdditiveForceVisualizer at 0x181df6c80>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = 25\n",
    "shap.plots.force(shap_values[ind])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e3d85503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id='i2J8LWD2QF0RY8DFPPSTV'>\n",
       "<div style='color: #900; text-align: center;'>\n",
       "  <b>Visualization omitted, Javascript library not loaded!</b><br>\n",
       "  Have you run `initjs()` in this notebook? If this notebook was from another\n",
       "  user you must also trust this notebook (File -> Trust notebook). If you are viewing\n",
       "  this notebook on github the Javascript has been stripped for security. If you are using\n",
       "  JupyterLab this error is because a JupyterLab extension has not yet been written.\n",
       "</div></div>\n",
       " <script>\n",
       "   if (window.SHAP) SHAP.ReactDom.render(\n",
       "    SHAP.React.createElement(SHAP.AdditiveForceVisualizer, {\"outNames\": [\"f(x)\"], \"baseValue\": 341.21822638604567, \"outValue\": -93.50211624312766, \"link\": \"identity\", \"featureNames\": [\"iid\", \"gender\", \"condtn\", \"wave\", \"round\", \"position\", \"positin1\", \"order\", \"pid\", \"int_corr\", \"samerace\", \"age_o\", \"race_o\", \"pf_o_att\", \"pf_o_sin\", \"pf_o_int\", \"pf_o_fun\", \"pf_o_amb\", \"pf_o_sha\", \"attr_o\", \"sinc_o\", \"intel_o\", \"fun_o\", \"amb_o\", \"shar_o\", \"like_o\", \"prob_o\", \"met_o\", \"age\", \"field_cd\", \"mn_sat\", \"tuition\", \"race\", \"imprace\", \"imprelig\", \"income\", \"goal\", \"date\", \"go_out\", \"career_c\", \"sports\", \"tvsports\", \"exercise\", \"dining\", \"museums\", \"art\", \"hiking\", \"gaming\", \"clubbing\", \"reading\", \"tv\", \"theater\", \"movies\", \"concerts\", \"music\", \"shopping\", \"yoga\", \"exphappy\", \"expnum\", \"attr1_1\", \"sinc1_1\", \"intel1_1\", \"fun1_1\", \"amb1_1\", \"shar1_1\", \"attr4_1\", \"sinc4_1\", \"intel4_1\", \"fun4_1\", \"amb4_1\", \"shar4_1\", \"attr2_1\", \"sinc2_1\", \"intel2_1\", \"fun2_1\", \"amb2_1\", \"shar2_1\", \"attr3_1\", \"sinc3_1\", \"fun3_1\", \"intel3_1\", \"amb3_1\", \"attr5_1\", \"sinc5_1\", \"intel5_1\", \"fun5_1\", \"amb5_1\", \"attr\", \"sinc\", \"intel\", \"fun\", \"amb\", \"shar\", \"like\", \"prob\", \"met\", \"match_es\", \"attr1_s\", \"sinc1_s\", \"intel1_s\", \"fun1_s\", \"amb1_s\", \"shar1_s\", \"attr3_s\", \"sinc3_s\", \"intel3_s\", \"fun3_s\", \"amb3_s\", \"zip_match\"], \"features\": {\"0\": {\"effect\": 0.16960937024392647, \"value\": 351.0}, \"1\": {\"effect\": 0.004355098864567433, \"value\": 0.0}, \"2\": {\"effect\": -7.426203186884116e-05, \"value\": 2.0}, \"3\": {\"effect\": 0.005323968929663145, \"value\": 14.0}, \"4\": {\"effect\": -8.396272732070043e-05, \"value\": 18.0}, \"5\": {\"effect\": 0.009725143412655322, \"value\": 11.0}, \"6\": {\"effect\": -0.014868512509517585, \"value\": 11.0}, \"7\": {\"effect\": -0.04607402344965053, \"value\": 13.0}, \"8\": {\"effect\": 0.28253228880958053, \"value\": 369.0}, \"9\": {\"effect\": 0.010150495852011785, \"value\": 0.75}, \"10\": {\"effect\": 0.008223517164459503, \"value\": 0.0}, \"11\": {\"effect\": -0.006883361317309694, \"value\": 25.0}, \"12\": {\"effect\": 0.012820114104127197, \"value\": 2.0}, \"13\": {\"effect\": 0.11725995136685735, \"value\": 17.0}, \"14\": {\"effect\": -0.002450918718485855, \"value\": 18.0}, \"15\": {\"effect\": 0.21326797671777215, \"value\": 28.0}, \"16\": {\"effect\": -0.025634873284623928, \"value\": 27.0}, \"17\": {\"effect\": -0.032808243549897906, \"value\": 5.0}, \"18\": {\"effect\": 0.07877780304492324, \"value\": 5.0}, \"19\": {\"effect\": 0.01002474884516364, \"value\": 6.0}, \"20\": {\"effect\": 0.008783083111332296, \"value\": 8.0}, \"21\": {\"effect\": 0.00782085191296687, \"value\": 8.0}, \"22\": {\"effect\": -0.15295651425938936, \"value\": 4.0}, \"23\": {\"effect\": -0.004998313414204885, \"value\": 5.0}, \"24\": {\"effect\": -0.07342231803345309, \"value\": 4.0}, \"25\": {\"effect\": -0.1794127391647797, \"value\": 4.0}, \"26\": {\"effect\": 0.045042597679802746, \"value\": 6.0}, \"27\": {\"effect\": -0.017135977077946122, \"value\": 2.0}, \"28\": {\"effect\": 0.14932054555920135, \"value\": 35.0}, \"29\": {\"effect\": -0.018507263201862332, \"value\": 9.0}, \"30\": {\"effect\": -10.586159983176605, \"value\": 1290.0}, \"31\": {\"effect\": -39.624948993054154, \"value\": 15309.0}, \"32\": {\"effect\": -0.023918771734208276, \"value\": 4.0}, \"33\": {\"effect\": 0.19137707075512136, \"value\": 1.0}, \"34\": {\"effect\": 0.015351828727781756, \"value\": 3.0}, \"35\": {\"effect\": -387.156979298289, \"value\": -1.0}, \"36\": {\"effect\": -0.00010896462624193703, \"value\": 2.0}, \"37\": {\"effect\": -0.0005080597755196167, \"value\": 5.0}, \"38\": {\"effect\": -0.014613076234114989, \"value\": 4.0}, \"39\": {\"effect\": -0.03963578560324047, \"value\": 2.0}, \"40\": {\"effect\": 0.024531583683813463, \"value\": 5.0}, \"41\": {\"effect\": -0.03150925749328413, \"value\": 2.0}, \"42\": {\"effect\": -0.000785049340677519, \"value\": 5.0}, \"43\": {\"effect\": 0.005911666601687188, \"value\": 10.0}, \"44\": {\"effect\": 0.019995174024388218, \"value\": 10.0}, \"45\": {\"effect\": -0.002688781404424279, \"value\": 10.0}, \"46\": {\"effect\": 0.017025653759007532, \"value\": 7.0}, \"47\": {\"effect\": 0.0016281371746776883, \"value\": 2.0}, \"48\": {\"effect\": 0.04797690474062472, \"value\": 3.0}, \"49\": {\"effect\": 0.04824202818869935, \"value\": 10.0}, \"50\": {\"effect\": -0.0018091308415421454, \"value\": 3.0}, \"51\": {\"effect\": 0.04070218231758784, \"value\": 10.0}, \"52\": {\"effect\": -0.04695352860093781, \"value\": 10.0}, \"53\": {\"effect\": 0.02131314535686516, \"value\": 10.0}, \"54\": {\"effect\": 0.010073199120766336, \"value\": 10.0}, \"55\": {\"effect\": 0.01315861895713574, \"value\": 5.0}, \"56\": {\"effect\": 0.004513162811212962, \"value\": 5.0}, \"57\": {\"effect\": -0.003479231680327666, \"value\": 8.0}, \"59\": {\"effect\": 0.15119564865591514, \"value\": 15.0}, \"60\": {\"effect\": 0.008324406075097381, \"value\": 15.0}, \"61\": {\"effect\": 0.2921499429979878, \"value\": 30.0}, \"62\": {\"effect\": -0.04087014439039089, \"value\": 15.0}, \"63\": {\"effect\": 0.014142323655112749, \"value\": 15.0}, \"64\": {\"effect\": 0.06857844201114145, \"value\": 10.0}, \"65\": {\"effect\": 1.0143879394415352, \"value\": 30.0}, \"66\": {\"effect\": 0.001587127570455961, \"value\": 10.0}, \"67\": {\"effect\": -0.0009502887113162651, \"value\": 10.0}, \"68\": {\"effect\": -0.008056080600286421, \"value\": 30.0}, \"69\": {\"effect\": -0.00598044922892808, \"value\": 10.0}, \"70\": {\"effect\": -0.006733009775495097, \"value\": 10.0}, \"71\": {\"effect\": -0.008836621899774404, \"value\": 30.0}, \"72\": {\"effect\": -0.0024052085231720975, \"value\": 10.0}, \"73\": {\"effect\": -0.05454072367681509, \"value\": 10.0}, \"74\": {\"effect\": 0.030385426452861294, \"value\": 30.0}, \"75\": {\"effect\": 0.023950072419077706, \"value\": 10.0}, \"76\": {\"effect\": -0.01460364587015916, \"value\": 10.0}, \"77\": {\"effect\": -0.005600809235845674, \"value\": 8.0}, \"78\": {\"effect\": 0.010963978740013331, \"value\": 10.0}, \"79\": {\"effect\": 0.021103746336675656, \"value\": 7.0}, \"80\": {\"effect\": 0.008216719596770857, \"value\": 8.0}, \"81\": {\"effect\": 0.0007109340208443856, \"value\": 9.0}, \"82\": {\"effect\": -0.01775640983111639, \"value\": 8.0}, \"83\": {\"effect\": 0.010094883525180379, \"value\": 8.0}, \"84\": {\"effect\": 0.020667033724633764, \"value\": 8.0}, \"85\": {\"effect\": 0.01099786753271863, \"value\": 8.0}, \"86\": {\"effect\": 0.04391095391486066, \"value\": 8.0}, \"87\": {\"effect\": 0.05111424326670492, \"value\": 7.0}, \"88\": {\"effect\": 0.010125844196861265, \"value\": 8.0}, \"89\": {\"effect\": 0.0074794904913330264, \"value\": 8.0}, \"90\": {\"effect\": 0.04889803609850125, \"value\": 7.0}, \"91\": {\"effect\": 0.005973824738905982, \"value\": 7.0}, \"92\": {\"effect\": 0.07891784369658537, \"value\": 6.0}, \"93\": {\"effect\": -0.00990041402539187, \"value\": 6.0}, \"94\": {\"effect\": 0.04540639863185671, \"value\": 6.0}, \"95\": {\"effect\": 0.028389745995222242, \"value\": 0.0}, \"96\": {\"effect\": -0.14011056821506415, \"value\": -1.0}, \"97\": {\"effect\": -0.03172488626976258, \"value\": -1.0}, \"98\": {\"effect\": 0.1634379504205303, \"value\": -1.0}, \"99\": {\"effect\": -0.022461856163655036, \"value\": -1.0}, \"100\": {\"effect\": 0.02843840033025925, \"value\": -1.0}, \"101\": {\"effect\": 0.030805770199670766, \"value\": -1.0}, \"102\": {\"effect\": 0.0007889529500351738, \"value\": -1.0}, \"103\": {\"effect\": -0.028563189056726227, \"value\": -1.0}, \"104\": {\"effect\": -0.025948680130093528, \"value\": -1.0}, \"105\": {\"effect\": -0.014272260704032716, \"value\": -1.0}, \"106\": {\"effect\": 0.030804238573062533, \"value\": -1.0}, \"107\": {\"effect\": -0.028293109830245806, \"value\": -1.0}, \"108\": {\"effect\": -0.00011117653931326167, \"value\": 0.0}}, \"plot_cmap\": \"RdBu\", \"labelMargin\": 20}),\n",
       "    document.getElementById('i2J8LWD2QF0RY8DFPPSTV')\n",
       "  );\n",
       "</script>"
      ],
      "text/plain": [
       "<shap.plots._force.AdditiveForceVisualizer at 0x18240aa40>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = 52\n",
    "shap.plots.force(shap_values[ind])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f24ad79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc\n",
      "y_test shape: (1676,)\n",
      "0.14051282051282052\n",
      "(6702, 109) (6702,)\n",
      "Fitting 4 folds for each of 25 candidates, totalling 100 fits\n",
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_svc__C  \\\n",
      "0        2.047716      0.326965         2.118279        0.302888          0.1   \n",
      "1        1.953284      0.331389         2.120596        0.289691          0.1   \n",
      "2        0.831193      0.100622         0.834710        0.115435          0.1   \n",
      "3        0.684475      0.137839         0.740035        0.128646          0.1   \n",
      "4        0.661617      0.125782         0.712637        0.111358          0.1   \n",
      "5        1.984093      0.383127         2.153704        0.314391            1   \n",
      "6        1.858667      0.305855         2.062514        0.328162            1   \n",
      "7        0.755840      0.145174         0.815517        0.138988            1   \n",
      "8        0.698852      0.119677         0.716976        0.104980            1   \n",
      "9        0.676722      0.136669         0.730139        0.106671            1   \n",
      "10       2.281878      0.434393         2.089670        0.314140           10   \n",
      "11       2.133976      0.366778         2.078267        0.278801           10   \n",
      "12       0.892043      0.122079         0.815075        0.139434           10   \n",
      "13       0.741745      0.143472         0.698154        0.120718           10   \n",
      "14       0.667600      0.109506         0.679735        0.105992           10   \n",
      "15       2.224621      0.419068         2.046169        0.293246          100   \n",
      "16       2.103459      0.366291         2.022580        0.262027          100   \n",
      "17       0.942862      0.187625         0.819185        0.123055          100   \n",
      "18       0.963288      0.144871         0.691663        0.126424          100   \n",
      "19       0.715454      0.111665         0.677480        0.096448          100   \n",
      "20       2.257115      0.467989         2.042875        0.263784         1000   \n",
      "21       2.081382      0.351544         2.052690        0.292496         1000   \n",
      "22       0.955148      0.184317         0.819597        0.109737         1000   \n",
      "23       1.731106      0.284880         0.644858        0.101217         1000   \n",
      "24       0.958236      0.163974         0.642947        0.138039         1000   \n",
      "\n",
      "   param_svc__gamma param_svc__kernel  \\\n",
      "0                 1               rbf   \n",
      "1               0.1               rbf   \n",
      "2              0.01               rbf   \n",
      "3             0.001               rbf   \n",
      "4            0.0001               rbf   \n",
      "5                 1               rbf   \n",
      "6               0.1               rbf   \n",
      "7              0.01               rbf   \n",
      "8             0.001               rbf   \n",
      "9            0.0001               rbf   \n",
      "10                1               rbf   \n",
      "11              0.1               rbf   \n",
      "12             0.01               rbf   \n",
      "13            0.001               rbf   \n",
      "14           0.0001               rbf   \n",
      "15                1               rbf   \n",
      "16              0.1               rbf   \n",
      "17             0.01               rbf   \n",
      "18            0.001               rbf   \n",
      "19           0.0001               rbf   \n",
      "20                1               rbf   \n",
      "21              0.1               rbf   \n",
      "22             0.01               rbf   \n",
      "23            0.001               rbf   \n",
      "24           0.0001               rbf   \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "0   {'svc__C': 0.1, 'svc__gamma': 1, 'svc__kernel'...           0.452744   \n",
      "1   {'svc__C': 0.1, 'svc__gamma': 0.1, 'svc__kerne...           0.452744   \n",
      "2   {'svc__C': 0.1, 'svc__gamma': 0.01, 'svc__kern...           0.452744   \n",
      "3   {'svc__C': 0.1, 'svc__gamma': 0.001, 'svc__ker...           0.452744   \n",
      "4   {'svc__C': 0.1, 'svc__gamma': 0.0001, 'svc__ke...           0.452744   \n",
      "5   {'svc__C': 1, 'svc__gamma': 1, 'svc__kernel': ...           0.452744   \n",
      "6   {'svc__C': 1, 'svc__gamma': 0.1, 'svc__kernel'...           0.452744   \n",
      "7   {'svc__C': 1, 'svc__gamma': 0.01, 'svc__kernel...           0.582922   \n",
      "8   {'svc__C': 1, 'svc__gamma': 0.001, 'svc__kerne...           0.452744   \n",
      "9   {'svc__C': 1, 'svc__gamma': 0.0001, 'svc__kern...           0.452744   \n",
      "10  {'svc__C': 10, 'svc__gamma': 1, 'svc__kernel':...           0.452744   \n",
      "11  {'svc__C': 10, 'svc__gamma': 0.1, 'svc__kernel...           0.452744   \n",
      "12  {'svc__C': 10, 'svc__gamma': 0.01, 'svc__kerne...           0.677846   \n",
      "13  {'svc__C': 10, 'svc__gamma': 0.001, 'svc__kern...           0.657101   \n",
      "14  {'svc__C': 10, 'svc__gamma': 0.0001, 'svc__ker...           0.452611   \n",
      "15  {'svc__C': 100, 'svc__gamma': 1, 'svc__kernel'...           0.452744   \n",
      "16  {'svc__C': 100, 'svc__gamma': 0.1, 'svc__kerne...           0.452744   \n",
      "17  {'svc__C': 100, 'svc__gamma': 0.01, 'svc__kern...           0.671102   \n",
      "18  {'svc__C': 100, 'svc__gamma': 0.001, 'svc__ker...           0.686116   \n",
      "19  {'svc__C': 100, 'svc__gamma': 0.0001, 'svc__ke...           0.638825   \n",
      "20  {'svc__C': 1000, 'svc__gamma': 1, 'svc__kernel...           0.452744   \n",
      "21  {'svc__C': 1000, 'svc__gamma': 0.1, 'svc__kern...           0.452744   \n",
      "22  {'svc__C': 1000, 'svc__gamma': 0.01, 'svc__ker...           0.671102   \n",
      "23  {'svc__C': 1000, 'svc__gamma': 0.001, 'svc__ke...           0.667440   \n",
      "24  {'svc__C': 1000, 'svc__gamma': 0.0001, 'svc__k...           0.673845   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
      "0            0.458154           0.453341           0.457155         0.455348   \n",
      "1            0.458154           0.453341           0.457155         0.455348   \n",
      "2            0.458154           0.453341           0.457155         0.455348   \n",
      "3            0.458154           0.453341           0.457155         0.455348   \n",
      "4            0.458154           0.453341           0.457155         0.455348   \n",
      "5            0.458154           0.453341           0.457155         0.455348   \n",
      "6            0.458154           0.453341           0.457155         0.455348   \n",
      "7            0.588264           0.583602           0.622661         0.594362   \n",
      "8            0.463753           0.459336           0.465674         0.460376   \n",
      "9            0.458154           0.453341           0.457155         0.455348   \n",
      "10           0.458154           0.453341           0.457155         0.455348   \n",
      "11           0.458154           0.453341           0.457155         0.455348   \n",
      "12           0.668629           0.645175           0.671251         0.665725   \n",
      "13           0.626168           0.636837           0.660852         0.645239   \n",
      "14           0.472043           0.462357           0.484092         0.467776   \n",
      "15           0.458154           0.453341           0.457155         0.455348   \n",
      "16           0.458154           0.453341           0.457155         0.455348   \n",
      "17           0.656573           0.636768           0.653856         0.654575   \n",
      "18           0.645035           0.664924           0.677569         0.668411   \n",
      "19           0.635200           0.633629           0.643434         0.637772   \n",
      "20           0.458154           0.453341           0.457155         0.455348   \n",
      "21           0.458154           0.453341           0.457155         0.455348   \n",
      "22           0.656573           0.636768           0.653856         0.654575   \n",
      "23           0.643640           0.651228           0.661179         0.655872   \n",
      "24           0.635901           0.657357           0.665900         0.658251   \n",
      "\n",
      "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
      "0         0.002342               12            0.456194            0.453289   \n",
      "1         0.002342               12            0.456194            0.453289   \n",
      "2         0.002342               12            0.456194            0.453289   \n",
      "3         0.002342               12            0.456194            0.453289   \n",
      "4         0.002342               12            0.456194            0.453289   \n",
      "5         0.002342               12            1.000000            1.000000   \n",
      "6         0.002342               12            0.971413            0.976369   \n",
      "7         0.016467                9            0.739670            0.763557   \n",
      "8         0.004970               11            0.467707            0.475264   \n",
      "9         0.002342               12            0.456194            0.453289   \n",
      "10        0.002342               12            1.000000            1.000000   \n",
      "11        0.002342               12            1.000000            1.000000   \n",
      "12        0.012331                2            0.976611            0.981842   \n",
      "13        0.014307                7            0.712362            0.725231   \n",
      "14        0.011659               10            0.471874            0.476418   \n",
      "15        0.002342               12            1.000000            1.000000   \n",
      "16        0.002342               12            1.000000            1.000000   \n",
      "17        0.012194                5            1.000000            1.000000   \n",
      "18        0.015459                1            0.846368            0.855582   \n",
      "19        0.003773                8            0.676795            0.699036   \n",
      "20        0.002342               12            1.000000            1.000000   \n",
      "21        0.002342               12            1.000000            1.000000   \n",
      "22        0.012194                5            1.000000            1.000000   \n",
      "23        0.009126                4            0.974209            0.976412   \n",
      "24        0.014160                3            0.746182            0.747325   \n",
      "\n",
      "    split2_train_score  split3_train_score  mean_train_score  std_train_score  \n",
      "0             0.455679            0.454509          0.454918         0.001121  \n",
      "1             0.455679            0.454509          0.454918         0.001121  \n",
      "2             0.455679            0.454509          0.454918         0.001121  \n",
      "3             0.455679            0.454509          0.454918         0.001121  \n",
      "4             0.455679            0.454509          0.454918         0.001121  \n",
      "5             1.000000            1.000000          1.000000         0.000000  \n",
      "6             0.978977            0.975466          0.975557         0.002717  \n",
      "7             0.760193            0.780049          0.760867         0.014361  \n",
      "8             0.476375            0.492224          0.477892         0.008921  \n",
      "9             0.455679            0.454509          0.454918         0.001121  \n",
      "10            1.000000            1.000000          1.000000         0.000000  \n",
      "11            1.000000            1.000000          1.000000         0.000000  \n",
      "12            0.982836            0.979675          0.980241         0.002387  \n",
      "13            0.724087            0.735059          0.724185         0.008049  \n",
      "14            0.483841            0.513106          0.486310         0.016050  \n",
      "15            1.000000            1.000000          1.000000         0.000000  \n",
      "16            1.000000            1.000000          1.000000         0.000000  \n",
      "17            1.000000            1.000000          1.000000         0.000000  \n",
      "18            0.848527            0.853907          0.851096         0.003774  \n",
      "19            0.681588            0.697746          0.688791         0.009759  \n",
      "20            1.000000            1.000000          1.000000         0.000000  \n",
      "21            1.000000            1.000000          1.000000         0.000000  \n",
      "22            1.000000            1.000000          1.000000         0.000000  \n",
      "23            0.973969            0.967716          0.973077         0.003238  \n",
      "24            0.743535            0.755789          0.748208         0.004588  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test_pred: [0 0 0 ... 0 0 0]\n",
      "test score: 0.7073765778656882\n",
      "svc\n",
      "y_test shape: (1676,)\n",
      "0.14489795918367346\n",
      "(6702, 109) (6702,)\n",
      "Fitting 4 folds for each of 25 candidates, totalling 100 fits\n",
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_svc__C  \\\n",
      "0        2.194814      0.266294         2.459679        0.210029          0.1   \n",
      "1        2.165303      0.258540         2.407603        0.198118          0.1   \n",
      "2        0.774808      0.156516         1.027853        0.158795          0.1   \n",
      "3        0.628265      0.084642         0.785950        0.045517          0.1   \n",
      "4        0.607904      0.094929         0.756481        0.053503          0.1   \n",
      "5        1.974212      0.202878         2.553294        0.124320            1   \n",
      "6        1.984874      0.247487         2.348707        0.175787            1   \n",
      "7        0.759128      0.109095         0.951617        0.092996            1   \n",
      "8        0.654304      0.057618         0.777419        0.062635            1   \n",
      "9        0.630319      0.090954         0.742047        0.058763            1   \n",
      "10       1.993009      0.237950         2.209956        0.184173           10   \n",
      "11       1.942966      0.236141         2.292277        0.155036           10   \n",
      "12       0.858767      0.125530         0.892568        0.048433           10   \n",
      "13       0.670551      0.078164         0.722939        0.049614           10   \n",
      "14       0.649590      0.067699         0.743742        0.049005           10   \n",
      "15       2.077120      0.275968         2.236327        0.128614          100   \n",
      "16       2.032140      0.201443         2.174545        0.140001          100   \n",
      "17       0.887484      0.101340         0.853622        0.066490          100   \n",
      "18       0.864331      0.133944         0.738421        0.066536          100   \n",
      "19       0.670993      0.078992         0.706678        0.062326          100   \n",
      "20       2.020187      0.241253         2.147360        0.191152         1000   \n",
      "21       1.920071      0.233414         2.234184        0.189373         1000   \n",
      "22       0.863544      0.118886         0.843883        0.083374         1000   \n",
      "23       1.591354      0.233598         0.627994        0.107623         1000   \n",
      "24       0.909759      0.109192         0.617195        0.059969         1000   \n",
      "\n",
      "   param_svc__gamma param_svc__kernel  \\\n",
      "0                 1               rbf   \n",
      "1               0.1               rbf   \n",
      "2              0.01               rbf   \n",
      "3             0.001               rbf   \n",
      "4            0.0001               rbf   \n",
      "5                 1               rbf   \n",
      "6               0.1               rbf   \n",
      "7              0.01               rbf   \n",
      "8             0.001               rbf   \n",
      "9            0.0001               rbf   \n",
      "10                1               rbf   \n",
      "11              0.1               rbf   \n",
      "12             0.01               rbf   \n",
      "13            0.001               rbf   \n",
      "14           0.0001               rbf   \n",
      "15                1               rbf   \n",
      "16              0.1               rbf   \n",
      "17             0.01               rbf   \n",
      "18            0.001               rbf   \n",
      "19           0.0001               rbf   \n",
      "20                1               rbf   \n",
      "21              0.1               rbf   \n",
      "22             0.01               rbf   \n",
      "23            0.001               rbf   \n",
      "24           0.0001               rbf   \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "0   {'svc__C': 0.1, 'svc__gamma': 1, 'svc__kernel'...           0.451541   \n",
      "1   {'svc__C': 0.1, 'svc__gamma': 0.1, 'svc__kerne...           0.451541   \n",
      "2   {'svc__C': 0.1, 'svc__gamma': 0.01, 'svc__kern...           0.451541   \n",
      "3   {'svc__C': 0.1, 'svc__gamma': 0.001, 'svc__ker...           0.451541   \n",
      "4   {'svc__C': 0.1, 'svc__gamma': 0.0001, 'svc__ke...           0.451541   \n",
      "5   {'svc__C': 1, 'svc__gamma': 1, 'svc__kernel': ...           0.451541   \n",
      "6   {'svc__C': 1, 'svc__gamma': 0.1, 'svc__kernel'...           0.451541   \n",
      "7   {'svc__C': 1, 'svc__gamma': 0.01, 'svc__kernel...           0.522301   \n",
      "8   {'svc__C': 1, 'svc__gamma': 0.001, 'svc__kerne...           0.451541   \n",
      "9   {'svc__C': 1, 'svc__gamma': 0.0001, 'svc__kern...           0.451541   \n",
      "10  {'svc__C': 10, 'svc__gamma': 1, 'svc__kernel':...           0.451541   \n",
      "11  {'svc__C': 10, 'svc__gamma': 0.1, 'svc__kernel...           0.451541   \n",
      "12  {'svc__C': 10, 'svc__gamma': 0.01, 'svc__kerne...           0.639135   \n",
      "13  {'svc__C': 10, 'svc__gamma': 0.001, 'svc__kern...           0.553694   \n",
      "14  {'svc__C': 10, 'svc__gamma': 0.0001, 'svc__ker...           0.451541   \n",
      "15  {'svc__C': 100, 'svc__gamma': 1, 'svc__kernel'...           0.451541   \n",
      "16  {'svc__C': 100, 'svc__gamma': 0.1, 'svc__kerne...           0.451541   \n",
      "17  {'svc__C': 100, 'svc__gamma': 0.01, 'svc__kern...           0.639253   \n",
      "18  {'svc__C': 100, 'svc__gamma': 0.001, 'svc__ker...           0.641888   \n",
      "19  {'svc__C': 100, 'svc__gamma': 0.0001, 'svc__ke...           0.562757   \n",
      "20  {'svc__C': 1000, 'svc__gamma': 1, 'svc__kernel...           0.451541   \n",
      "21  {'svc__C': 1000, 'svc__gamma': 0.1, 'svc__kern...           0.451541   \n",
      "22  {'svc__C': 1000, 'svc__gamma': 0.01, 'svc__ker...           0.638467   \n",
      "23  {'svc__C': 1000, 'svc__gamma': 0.001, 'svc__ke...           0.641950   \n",
      "24  {'svc__C': 1000, 'svc__gamma': 0.0001, 'svc__k...           0.603299   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
      "0            0.450605           0.455207           0.456461         0.453453   \n",
      "1            0.450605           0.455207           0.456461         0.453453   \n",
      "2            0.450605           0.455207           0.456461         0.453453   \n",
      "3            0.450605           0.455207           0.456461         0.453453   \n",
      "4            0.450605           0.455207           0.456461         0.453453   \n",
      "5            0.450605           0.455207           0.456461         0.453453   \n",
      "6            0.450605           0.455207           0.456461         0.453453   \n",
      "7            0.555539           0.558722           0.561735         0.549574   \n",
      "8            0.450605           0.455207           0.456461         0.453453   \n",
      "9            0.450605           0.455207           0.456461         0.453453   \n",
      "10           0.450605           0.455207           0.456461         0.453453   \n",
      "11           0.450605           0.455207           0.456461         0.453453   \n",
      "12           0.662385           0.660007           0.662186         0.655928   \n",
      "13           0.599659           0.596682           0.644506         0.598635   \n",
      "14           0.450605           0.455207           0.456461         0.453453   \n",
      "15           0.450605           0.455207           0.456461         0.453453   \n",
      "16           0.450605           0.455207           0.456461         0.453453   \n",
      "17           0.670782           0.672228           0.649281         0.657886   \n",
      "18           0.676525           0.644455           0.669733         0.658151   \n",
      "19           0.578418           0.590220           0.631047         0.590611   \n",
      "20           0.450605           0.455207           0.456461         0.453453   \n",
      "21           0.450605           0.455207           0.456461         0.453453   \n",
      "22           0.671352           0.669684           0.649281         0.657196   \n",
      "23           0.656590           0.649263           0.662042         0.652461   \n",
      "24           0.632397           0.609051           0.649478         0.623556   \n",
      "\n",
      "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
      "0         0.002444               10            0.457751            0.457329   \n",
      "1         0.002444               10            0.457751            0.457329   \n",
      "2         0.002444               10            0.457751            0.457329   \n",
      "3         0.002444               10            0.457751            0.457329   \n",
      "4         0.002444               10            0.457751            0.457329   \n",
      "5         0.002444               10            1.000000            1.000000   \n",
      "6         0.002444               10            0.959649            0.969952   \n",
      "7         0.015898                9            0.714480            0.738007   \n",
      "8         0.002444               10            0.457751            0.465459   \n",
      "9         0.002444               10            0.457751            0.457329   \n",
      "10        0.002444               10            1.000000            1.000000   \n",
      "11        0.002444               10            1.000000            1.000000   \n",
      "12        0.009740                4            0.973679            0.979783   \n",
      "13        0.032128                7            0.667564            0.678581   \n",
      "14        0.002444               10            0.457751            0.457329   \n",
      "15        0.002444               10            1.000000            1.000000   \n",
      "16        0.002444               10            1.000000            1.000000   \n",
      "17        0.014082                2            1.000000            1.000000   \n",
      "18        0.015197                1            0.848488            0.848141   \n",
      "19        0.025297                8            0.639926            0.641451   \n",
      "20        0.002444               10            1.000000            1.000000   \n",
      "21        0.002444               10            1.000000            1.000000   \n",
      "22        0.013872                3            1.000000            1.000000   \n",
      "23        0.007575                5            0.972756            0.974317   \n",
      "24        0.018513                6            0.720954            0.716680   \n",
      "\n",
      "    split2_train_score  split3_train_score  mean_train_score  std_train_score  \n",
      "0             0.455581            0.454953          0.456403         0.001168  \n",
      "1             0.455581            0.454953          0.456403         0.001168  \n",
      "2             0.455581            0.454953          0.456403         0.001168  \n",
      "3             0.455581            0.454953          0.456403         0.001168  \n",
      "4             0.455581            0.454953          0.456403         0.001168  \n",
      "5             1.000000            1.000000          1.000000         0.000000  \n",
      "6             0.967948            0.968512          0.966515         0.004031  \n",
      "7             0.755582            0.751236          0.739826         0.016001  \n",
      "8             0.467336            0.454953          0.461375         0.005162  \n",
      "9             0.455581            0.454953          0.456403         0.001168  \n",
      "10            1.000000            1.000000          1.000000         0.000000  \n",
      "11            1.000000            1.000000          1.000000         0.000000  \n",
      "12            0.979560            0.981961          0.978746         0.003072  \n",
      "13            0.699688            0.704849          0.687671         0.015218  \n",
      "14            0.469678            0.454953          0.459928         0.005729  \n",
      "15            1.000000            1.000000          1.000000         0.000000  \n",
      "16            1.000000            1.000000          1.000000         0.000000  \n",
      "17            1.000000            1.000000          1.000000         0.000000  \n",
      "18            0.839957            0.851370          0.846989         0.004249  \n",
      "19            0.667253            0.661880          0.652628         0.012101  \n",
      "20            1.000000            1.000000          1.000000         0.000000  \n",
      "21            1.000000            1.000000          1.000000         0.000000  \n",
      "22            1.000000            1.000000          1.000000         0.000000  \n",
      "23            0.968827            0.973916          0.972454         0.002171  \n",
      "24            0.729940            0.733746          0.725330         0.006820  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test_pred: [0 0 0 ... 0 0 0]\n",
      "test score: 0.7201058417050461\n",
      "svc\n",
      "y_test shape: (1676,)\n",
      "0.14314928425357873\n",
      "(6702, 109) (6702,)\n",
      "Fitting 4 folds for each of 25 candidates, totalling 100 fits\n",
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_svc__C  \\\n",
      "0        2.329939      0.398557         1.894926        0.358987          0.1   \n",
      "1        2.301638      0.355320         1.819097        0.363129          0.1   \n",
      "2        1.004257      0.310493         0.757699        0.136272          0.1   \n",
      "3        0.785752      0.136998         0.624628        0.111129          0.1   \n",
      "4        0.769251      0.180416         0.609732        0.110570          0.1   \n",
      "5        2.209841      0.405841         1.882356        0.384341            1   \n",
      "6        2.205231      0.385871         1.888133        0.375361            1   \n",
      "7        0.939418      0.210294         0.727139        0.161393            1   \n",
      "8        0.815145      0.163727         0.633386        0.107520            1   \n",
      "9        0.788166      0.154524         0.592917        0.094192            1   \n",
      "10       2.672822      0.445041         1.930087        0.390837           10   \n",
      "11       2.519422      0.423076         1.878893        0.342941           10   \n",
      "12       1.059977      0.249547         0.764818        0.140132           10   \n",
      "13       0.826216      0.178520         0.607383        0.121589           10   \n",
      "14       0.773707      0.157851         0.636095        0.103200           10   \n",
      "15       2.716036      0.461461         1.922886        0.368341          100   \n",
      "16       2.516459      0.410373         1.934929        0.407683          100   \n",
      "17       1.121975      0.240956         0.780123        0.169524          100   \n",
      "18       1.080593      0.215854         0.610866        0.099048          100   \n",
      "19       0.840928      0.171278         0.618144        0.123878          100   \n",
      "20       2.714751      0.476423         1.868452        0.365155         1000   \n",
      "21       2.562340      0.422958         1.888216        0.339143         1000   \n",
      "22       1.145263      0.253456         0.743790        0.169707         1000   \n",
      "23       2.133905      0.451487         0.589275        0.113106         1000   \n",
      "24       1.194726      0.213366         0.598867        0.098438         1000   \n",
      "\n",
      "   param_svc__gamma param_svc__kernel  \\\n",
      "0                 1               rbf   \n",
      "1               0.1               rbf   \n",
      "2              0.01               rbf   \n",
      "3             0.001               rbf   \n",
      "4            0.0001               rbf   \n",
      "5                 1               rbf   \n",
      "6               0.1               rbf   \n",
      "7              0.01               rbf   \n",
      "8             0.001               rbf   \n",
      "9            0.0001               rbf   \n",
      "10                1               rbf   \n",
      "11              0.1               rbf   \n",
      "12             0.01               rbf   \n",
      "13            0.001               rbf   \n",
      "14           0.0001               rbf   \n",
      "15                1               rbf   \n",
      "16              0.1               rbf   \n",
      "17             0.01               rbf   \n",
      "18            0.001               rbf   \n",
      "19           0.0001               rbf   \n",
      "20                1               rbf   \n",
      "21              0.1               rbf   \n",
      "22             0.01               rbf   \n",
      "23            0.001               rbf   \n",
      "24           0.0001               rbf   \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "0   {'svc__C': 0.1, 'svc__gamma': 1, 'svc__kernel'...           0.458123   \n",
      "1   {'svc__C': 0.1, 'svc__gamma': 0.1, 'svc__kerne...           0.458123   \n",
      "2   {'svc__C': 0.1, 'svc__gamma': 0.01, 'svc__kern...           0.458123   \n",
      "3   {'svc__C': 0.1, 'svc__gamma': 0.001, 'svc__ker...           0.458123   \n",
      "4   {'svc__C': 0.1, 'svc__gamma': 0.0001, 'svc__ke...           0.458123   \n",
      "5   {'svc__C': 1, 'svc__gamma': 1, 'svc__kernel': ...           0.458123   \n",
      "6   {'svc__C': 1, 'svc__gamma': 0.1, 'svc__kernel'...           0.458123   \n",
      "7   {'svc__C': 1, 'svc__gamma': 0.01, 'svc__kernel...           0.580217   \n",
      "8   {'svc__C': 1, 'svc__gamma': 0.001, 'svc__kerne...           0.458123   \n",
      "9   {'svc__C': 1, 'svc__gamma': 0.0001, 'svc__kern...           0.458123   \n",
      "10  {'svc__C': 10, 'svc__gamma': 1, 'svc__kernel':...           0.458123   \n",
      "11  {'svc__C': 10, 'svc__gamma': 0.1, 'svc__kernel...           0.458123   \n",
      "12  {'svc__C': 10, 'svc__gamma': 0.01, 'svc__kerne...           0.692265   \n",
      "13  {'svc__C': 10, 'svc__gamma': 0.001, 'svc__kern...           0.623947   \n",
      "14  {'svc__C': 10, 'svc__gamma': 0.0001, 'svc__ker...           0.458123   \n",
      "15  {'svc__C': 100, 'svc__gamma': 1, 'svc__kernel'...           0.458123   \n",
      "16  {'svc__C': 100, 'svc__gamma': 0.1, 'svc__kerne...           0.458123   \n",
      "17  {'svc__C': 100, 'svc__gamma': 0.01, 'svc__kern...           0.693472   \n",
      "18  {'svc__C': 100, 'svc__gamma': 0.001, 'svc__ker...           0.678185   \n",
      "19  {'svc__C': 100, 'svc__gamma': 0.0001, 'svc__ke...           0.644675   \n",
      "20  {'svc__C': 1000, 'svc__gamma': 1, 'svc__kernel...           0.458123   \n",
      "21  {'svc__C': 1000, 'svc__gamma': 0.1, 'svc__kern...           0.458123   \n",
      "22  {'svc__C': 1000, 'svc__gamma': 0.01, 'svc__ker...           0.691791   \n",
      "23  {'svc__C': 1000, 'svc__gamma': 0.001, 'svc__ke...           0.668320   \n",
      "24  {'svc__C': 1000, 'svc__gamma': 0.0001, 'svc__k...           0.669820   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
      "0            0.453502           0.448202           0.450546         0.452593   \n",
      "1            0.453502           0.448202           0.450546         0.452593   \n",
      "2            0.453502           0.448202           0.450546         0.452593   \n",
      "3            0.453502           0.448202           0.450546         0.452593   \n",
      "4            0.453502           0.448202           0.450546         0.452593   \n",
      "5            0.453502           0.448202           0.450546         0.452593   \n",
      "6            0.453502           0.448202           0.450546         0.452593   \n",
      "7            0.649175           0.584319           0.551614         0.591331   \n",
      "8            0.453502           0.448202           0.450546         0.452593   \n",
      "9            0.453502           0.448202           0.450546         0.452593   \n",
      "10           0.453502           0.448202           0.450546         0.452593   \n",
      "11           0.453502           0.448202           0.450546         0.452593   \n",
      "12           0.707270           0.699315           0.649903         0.687188   \n",
      "13           0.666260           0.624137           0.606569         0.630228   \n",
      "14           0.453502           0.448202           0.450546         0.452593   \n",
      "15           0.453502           0.448202           0.450546         0.452593   \n",
      "16           0.453502           0.448202           0.450546         0.452593   \n",
      "17           0.698420           0.697774           0.643481         0.683287   \n",
      "18           0.714866           0.696888           0.641654         0.682898   \n",
      "19           0.668683           0.607521           0.603021         0.630975   \n",
      "20           0.453502           0.448202           0.450546         0.452593   \n",
      "21           0.453502           0.448202           0.450546         0.452593   \n",
      "22           0.694872           0.702693           0.642388         0.682936   \n",
      "23           0.683511           0.679694           0.641759         0.668321   \n",
      "24           0.684087           0.672293           0.630468         0.664167   \n",
      "\n",
      "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
      "0         0.003704               10            0.454316            0.455596   \n",
      "1         0.003704               10            0.454316            0.455596   \n",
      "2         0.003704               10            0.454316            0.455596   \n",
      "3         0.003704               10            0.454316            0.455596   \n",
      "4         0.003704               10            0.454316            0.455596   \n",
      "5         0.003704               10            1.000000            1.000000   \n",
      "6         0.003704               10            0.974710            0.975334   \n",
      "7         0.035693                9            0.739677            0.747505   \n",
      "8         0.003704               10            0.462988            0.455596   \n",
      "9         0.003704               10            0.454316            0.455596   \n",
      "10        0.003704               10            1.000000            1.000000   \n",
      "11        0.003704               10            1.000000            1.000000   \n",
      "12        0.022172                1            0.978414            0.980939   \n",
      "13        0.021992                8            0.689743            0.697323   \n",
      "14        0.003704               10            0.455564            0.455596   \n",
      "15        0.003704               10            1.000000            1.000000   \n",
      "16        0.003704               10            1.000000            1.000000   \n",
      "17        0.023060                2            1.000000            1.000000   \n",
      "18        0.027115                4            0.845554            0.842586   \n",
      "19        0.027116                7            0.644142            0.651216   \n",
      "20        0.003704               10            1.000000            1.000000   \n",
      "21        0.003704               10            1.000000            1.000000   \n",
      "22        0.023745                3            1.000000            1.000000   \n",
      "23        0.016322                5            0.971097            0.969145   \n",
      "24        0.020189                6            0.725039            0.723559   \n",
      "\n",
      "    split2_train_score  split3_train_score  mean_train_score  std_train_score  \n",
      "0             0.458382            0.457172          0.456366         0.001542  \n",
      "1             0.458382            0.457172          0.456366         0.001542  \n",
      "2             0.458382            0.457172          0.456366         0.001542  \n",
      "3             0.458382            0.457172          0.456366         0.001542  \n",
      "4             0.458382            0.457172          0.456366         0.001542  \n",
      "5             1.000000            1.000000          1.000000         0.000000  \n",
      "6             0.972537            0.973996          0.974144         0.001042  \n",
      "7             0.703046            0.758862          0.737273         0.020905  \n",
      "8             0.458382            0.469403          0.461592         0.005226  \n",
      "9             0.458382            0.457172          0.456366         0.001542  \n",
      "10            1.000000            1.000000          1.000000         0.000000  \n",
      "11            1.000000            1.000000          1.000000         0.000000  \n",
      "12            0.981950            0.982497          0.980950         0.001567  \n",
      "13            0.651059            0.706372          0.686124         0.021084  \n",
      "14            0.458382            0.466709          0.459063         0.004561  \n",
      "15            1.000000            1.000000          1.000000         0.000000  \n",
      "16            1.000000            1.000000          1.000000         0.000000  \n",
      "17            1.000000            1.000000          1.000000         0.000000  \n",
      "18            0.841858            0.852633          0.845657         0.004259  \n",
      "19            0.594017            0.669871          0.639812         0.028060  \n",
      "20            1.000000            1.000000          1.000000         0.000000  \n",
      "21            1.000000            1.000000          1.000000         0.000000  \n",
      "22            1.000000            1.000000          1.000000         0.000000  \n",
      "23            0.976018            0.976027          0.973072         0.003030  \n",
      "24            0.689748            0.740529          0.719719         0.018536  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test_pred: [0 0 0 ... 0 0 0]\n",
      "test score: 0.7136984436932495\n",
      "svc\n",
      "y_test shape: (1676,)\n",
      "0.14051282051282052\n",
      "(6702, 109) (6702,)\n",
      "Fitting 4 folds for each of 25 candidates, totalling 100 fits\n",
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_svc__C  \\\n",
      "0        2.019042      0.212949         2.176117        0.148002          0.1   \n",
      "1        1.914689      0.202905         2.141165        0.148981          0.1   \n",
      "2        0.826647      0.091595         0.868979        0.054489          0.1   \n",
      "3        0.698320      0.081324         0.725184        0.066626          0.1   \n",
      "4        0.674421      0.073872         0.740097        0.040399          0.1   \n",
      "5        1.932444      0.213630         2.150903        0.222260            1   \n",
      "6        1.818981      0.196678         2.146880        0.170860            1   \n",
      "7        0.815947      0.097772         0.872003        0.073298            1   \n",
      "8        0.741251      0.086212         0.746600        0.053420            1   \n",
      "9        0.713248      0.087302         0.724078        0.033004            1   \n",
      "10       2.248922      0.275260         2.137024        0.218945           10   \n",
      "11       2.138407      0.223962         2.103026        0.128904           10   \n",
      "12       0.925823      0.104857         0.860472        0.066368           10   \n",
      "13       0.731695      0.103796         0.730570        0.070127           10   \n",
      "14       0.716132      0.076111         0.727378        0.042188           10   \n",
      "15       2.213329      0.219504         2.276239        0.126059          100   \n",
      "16       2.201600      0.251304         2.257660        0.215182          100   \n",
      "17       0.961194      0.122566         0.886724        0.081972          100   \n",
      "18       1.062047      0.124872         0.830542        0.076393          100   \n",
      "19       0.788298      0.100405         0.797422        0.063652          100   \n",
      "20       2.375915      0.231350         2.302927        0.208222         1000   \n",
      "21       2.224678      0.269881         2.393865        0.157017         1000   \n",
      "22       1.018821      0.120876         0.908561        0.055263         1000   \n",
      "23       1.898187      0.275399         0.621882        0.070749         1000   \n",
      "24       1.073327      0.136748         0.639288        0.070003         1000   \n",
      "\n",
      "   param_svc__gamma param_svc__kernel  \\\n",
      "0                 1               rbf   \n",
      "1               0.1               rbf   \n",
      "2              0.01               rbf   \n",
      "3             0.001               rbf   \n",
      "4            0.0001               rbf   \n",
      "5                 1               rbf   \n",
      "6               0.1               rbf   \n",
      "7              0.01               rbf   \n",
      "8             0.001               rbf   \n",
      "9            0.0001               rbf   \n",
      "10                1               rbf   \n",
      "11              0.1               rbf   \n",
      "12             0.01               rbf   \n",
      "13            0.001               rbf   \n",
      "14           0.0001               rbf   \n",
      "15                1               rbf   \n",
      "16              0.1               rbf   \n",
      "17             0.01               rbf   \n",
      "18            0.001               rbf   \n",
      "19           0.0001               rbf   \n",
      "20                1               rbf   \n",
      "21              0.1               rbf   \n",
      "22             0.01               rbf   \n",
      "23            0.001               rbf   \n",
      "24           0.0001               rbf   \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "0   {'svc__C': 0.1, 'svc__gamma': 1, 'svc__kernel'...           0.451157   \n",
      "1   {'svc__C': 0.1, 'svc__gamma': 0.1, 'svc__kerne...           0.451157   \n",
      "2   {'svc__C': 0.1, 'svc__gamma': 0.01, 'svc__kern...           0.451157   \n",
      "3   {'svc__C': 0.1, 'svc__gamma': 0.001, 'svc__ker...           0.451157   \n",
      "4   {'svc__C': 0.1, 'svc__gamma': 0.0001, 'svc__ke...           0.451157   \n",
      "5   {'svc__C': 1, 'svc__gamma': 1, 'svc__kernel': ...           0.451157   \n",
      "6   {'svc__C': 1, 'svc__gamma': 0.1, 'svc__kernel'...           0.451157   \n",
      "7   {'svc__C': 1, 'svc__gamma': 0.01, 'svc__kernel...           0.533355   \n",
      "8   {'svc__C': 1, 'svc__gamma': 0.001, 'svc__kerne...           0.451157   \n",
      "9   {'svc__C': 1, 'svc__gamma': 0.0001, 'svc__kern...           0.451157   \n",
      "10  {'svc__C': 10, 'svc__gamma': 1, 'svc__kernel':...           0.451157   \n",
      "11  {'svc__C': 10, 'svc__gamma': 0.1, 'svc__kernel...           0.451157   \n",
      "12  {'svc__C': 10, 'svc__gamma': 0.01, 'svc__kerne...           0.633313   \n",
      "13  {'svc__C': 10, 'svc__gamma': 0.001, 'svc__kern...           0.582150   \n",
      "14  {'svc__C': 10, 'svc__gamma': 0.0001, 'svc__ker...           0.451157   \n",
      "15  {'svc__C': 100, 'svc__gamma': 1, 'svc__kernel'...           0.451157   \n",
      "16  {'svc__C': 100, 'svc__gamma': 0.1, 'svc__kerne...           0.451157   \n",
      "17  {'svc__C': 100, 'svc__gamma': 0.01, 'svc__kern...           0.613474   \n",
      "18  {'svc__C': 100, 'svc__gamma': 0.001, 'svc__ker...           0.620159   \n",
      "19  {'svc__C': 100, 'svc__gamma': 0.0001, 'svc__ke...           0.553768   \n",
      "20  {'svc__C': 1000, 'svc__gamma': 1, 'svc__kernel...           0.451157   \n",
      "21  {'svc__C': 1000, 'svc__gamma': 0.1, 'svc__kern...           0.451157   \n",
      "22  {'svc__C': 1000, 'svc__gamma': 0.01, 'svc__ker...           0.615845   \n",
      "23  {'svc__C': 1000, 'svc__gamma': 0.001, 'svc__ke...           0.641581   \n",
      "24  {'svc__C': 1000, 'svc__gamma': 0.0001, 'svc__k...           0.597175   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
      "0            0.455666           0.454663           0.459753         0.455310   \n",
      "1            0.455666           0.454663           0.459753         0.455310   \n",
      "2            0.455666           0.454663           0.459753         0.455310   \n",
      "3            0.455666           0.454663           0.459753         0.455310   \n",
      "4            0.455666           0.454663           0.459753         0.455310   \n",
      "5            0.455666           0.454663           0.459753         0.455310   \n",
      "6            0.455666           0.454663           0.459753         0.455310   \n",
      "7            0.582567           0.619927           0.561659         0.574377   \n",
      "8            0.455666           0.454663           0.459753         0.455310   \n",
      "9            0.455666           0.454663           0.459753         0.455310   \n",
      "10           0.455666           0.454663           0.459753         0.455310   \n",
      "11           0.455666           0.454663           0.459753         0.455310   \n",
      "12           0.660092           0.707338           0.636464         0.659302   \n",
      "13           0.641797           0.670888           0.604780         0.624904   \n",
      "14           0.455666           0.454663           0.459753         0.455310   \n",
      "15           0.455666           0.454663           0.459753         0.455310   \n",
      "16           0.455666           0.454663           0.459753         0.455310   \n",
      "17           0.662600           0.709945           0.620774         0.651698   \n",
      "18           0.662684           0.682418           0.643711         0.652243   \n",
      "19           0.638075           0.664608           0.593632         0.612521   \n",
      "20           0.455666           0.454663           0.459753         0.455310   \n",
      "21           0.455666           0.454663           0.459753         0.455310   \n",
      "22           0.662600           0.709945           0.619069         0.651865   \n",
      "23           0.646651           0.669209           0.639659         0.649275   \n",
      "24           0.639924           0.666393           0.631316         0.633702   \n",
      "\n",
      "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
      "0         0.003063               10            0.456975            0.454799   \n",
      "1         0.003063               10            0.456975            0.454799   \n",
      "2         0.003063               10            0.456975            0.454799   \n",
      "3         0.003063               10            0.456975            0.454799   \n",
      "4         0.003063               10            0.456975            0.454799   \n",
      "5         0.003063               10            1.000000            1.000000   \n",
      "6         0.003063               10            0.967873            0.972003   \n",
      "7         0.031569                9            0.730557            0.734969   \n",
      "8         0.003063               10            0.456975            0.454799   \n",
      "9         0.003063               10            0.456975            0.454799   \n",
      "10        0.003063               10            1.000000            1.000000   \n",
      "11        0.003063               10            1.000000            1.000000   \n",
      "12        0.029602                1            0.979833            0.976672   \n",
      "13        0.034032                7            0.680921            0.684809   \n",
      "14        0.003063               10            0.456975            0.454799   \n",
      "15        0.003063               10            1.000000            1.000000   \n",
      "16        0.003063               10            1.000000            1.000000   \n",
      "17        0.038500                4            1.000000            1.000000   \n",
      "18        0.023031                2            0.839267            0.848086   \n",
      "19        0.042352                8            0.631220            0.643222   \n",
      "20        0.003063               10            1.000000            1.000000   \n",
      "21        0.003063               10            1.000000            1.000000   \n",
      "22        0.038280                3            1.000000            1.000000   \n",
      "23        0.011789                5            0.972523            0.968690   \n",
      "24        0.024735                6            0.715544            0.711433   \n",
      "\n",
      "    split2_train_score  split3_train_score  mean_train_score  std_train_score  \n",
      "0             0.455157            0.452641          0.454893         0.001540  \n",
      "1             0.455157            0.452641          0.454893         0.001540  \n",
      "2             0.455157            0.452641          0.454893         0.001540  \n",
      "3             0.455157            0.452641          0.454893         0.001540  \n",
      "4             0.455157            0.452641          0.454893         0.001540  \n",
      "5             1.000000            1.000000          1.000000         0.000000  \n",
      "6             0.974513            0.974532          0.972230         0.002718  \n",
      "7             0.734075            0.759368          0.739742         0.011450  \n",
      "8             0.455157            0.455332          0.455566         0.000836  \n",
      "9             0.455157            0.452641          0.454893         0.001540  \n",
      "10            1.000000            1.000000          1.000000         0.000000  \n",
      "11            1.000000            1.000000          1.000000         0.000000  \n",
      "12            0.979368            0.983334          0.979802         0.002370  \n",
      "13            0.678899            0.714601          0.689808         0.014471  \n",
      "14            0.455157            0.453988          0.455230         0.001093  \n",
      "15            1.000000            1.000000          1.000000         0.000000  \n",
      "16            1.000000            1.000000          1.000000         0.000000  \n",
      "17            1.000000            1.000000          1.000000         0.000000  \n",
      "18            0.846904            0.857774          0.848008         0.006577  \n",
      "19            0.623924            0.657985          0.639088         0.012904  \n",
      "20            1.000000            1.000000          1.000000         0.000000  \n",
      "21            1.000000            1.000000          1.000000         0.000000  \n",
      "22            1.000000            1.000000          1.000000         0.000000  \n",
      "23            0.967062            0.976192          0.971117         0.003538  \n",
      "24            0.708286            0.743306          0.719643         0.013903  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test_pred: [0 0 0 ... 0 0 0]\n",
      "test score: 0.701590091293123\n",
      "svc\n",
      "y_test shape: (1676,)\n",
      "0.13250517598343686\n",
      "(6702, 109) (6702,)\n",
      "Fitting 4 folds for each of 25 candidates, totalling 100 fits\n",
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_svc__C  \\\n",
      "0        1.945674      0.604678         2.098284        0.442221          0.1   \n",
      "1        1.877088      0.590147         2.118445        0.452679          0.1   \n",
      "2        0.799876      0.262178         0.860962        0.202338          0.1   \n",
      "3        0.695270      0.223633         0.736011        0.147803          0.1   \n",
      "4        0.673838      0.206079         0.711509        0.163968          0.1   \n",
      "5        1.853933      0.583227         2.080492        0.426329            1   \n",
      "6        1.807365      0.570847         2.101743        0.383845            1   \n",
      "7        0.789925      0.253352         0.850757        0.178589            1   \n",
      "8        0.743123      0.252651         0.828533        0.178431            1   \n",
      "9        0.721143      0.225593         0.770934        0.166824            1   \n",
      "10       2.372194      0.764768         2.122898        0.430319           10   \n",
      "11       2.180198      0.679858         2.101881        0.408548           10   \n",
      "12       0.915583      0.296204         0.852258        0.153661           10   \n",
      "13       0.702146      0.229350         0.730711        0.147403           10   \n",
      "14       0.700476      0.238067         0.718740        0.159560           10   \n",
      "15       2.328671      0.757919         2.264153        0.469424          100   \n",
      "16       2.210084      0.720472         2.268113        0.472168          100   \n",
      "17       0.997184      0.317813         0.890717        0.182195          100   \n",
      "18       0.878639      0.254297         0.697498        0.128420          100   \n",
      "19       0.713165      0.219512         0.688630        0.143504          100   \n",
      "20       2.257916      0.766178         2.254709        0.459193         1000   \n",
      "21       2.161641      0.694392         2.213489        0.493509         1000   \n",
      "22       0.972613      0.303768         0.901666        0.222943         1000   \n",
      "23       1.756932      0.574341         0.679409        0.153129         1000   \n",
      "24       0.999553      0.300952         0.637976        0.127098         1000   \n",
      "\n",
      "   param_svc__gamma param_svc__kernel  \\\n",
      "0                 1               rbf   \n",
      "1               0.1               rbf   \n",
      "2              0.01               rbf   \n",
      "3             0.001               rbf   \n",
      "4            0.0001               rbf   \n",
      "5                 1               rbf   \n",
      "6               0.1               rbf   \n",
      "7              0.01               rbf   \n",
      "8             0.001               rbf   \n",
      "9            0.0001               rbf   \n",
      "10                1               rbf   \n",
      "11              0.1               rbf   \n",
      "12             0.01               rbf   \n",
      "13            0.001               rbf   \n",
      "14           0.0001               rbf   \n",
      "15                1               rbf   \n",
      "16              0.1               rbf   \n",
      "17             0.01               rbf   \n",
      "18            0.001               rbf   \n",
      "19           0.0001               rbf   \n",
      "20                1               rbf   \n",
      "21              0.1               rbf   \n",
      "22             0.01               rbf   \n",
      "23            0.001               rbf   \n",
      "24           0.0001               rbf   \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "0   {'svc__C': 0.1, 'svc__gamma': 1, 'svc__kernel'...           0.458256   \n",
      "1   {'svc__C': 0.1, 'svc__gamma': 0.1, 'svc__kerne...           0.458256   \n",
      "2   {'svc__C': 0.1, 'svc__gamma': 0.01, 'svc__kern...           0.458256   \n",
      "3   {'svc__C': 0.1, 'svc__gamma': 0.001, 'svc__ker...           0.458256   \n",
      "4   {'svc__C': 0.1, 'svc__gamma': 0.0001, 'svc__ke...           0.458256   \n",
      "5   {'svc__C': 1, 'svc__gamma': 1, 'svc__kernel': ...           0.458256   \n",
      "6   {'svc__C': 1, 'svc__gamma': 0.1, 'svc__kernel'...           0.458256   \n",
      "7   {'svc__C': 1, 'svc__gamma': 0.01, 'svc__kernel...           0.568119   \n",
      "8   {'svc__C': 1, 'svc__gamma': 0.001, 'svc__kerne...           0.467167   \n",
      "9   {'svc__C': 1, 'svc__gamma': 0.0001, 'svc__kern...           0.458256   \n",
      "10  {'svc__C': 10, 'svc__gamma': 1, 'svc__kernel':...           0.458256   \n",
      "11  {'svc__C': 10, 'svc__gamma': 0.1, 'svc__kernel...           0.458256   \n",
      "12  {'svc__C': 10, 'svc__gamma': 0.01, 'svc__kerne...           0.615342   \n",
      "13  {'svc__C': 10, 'svc__gamma': 0.001, 'svc__kern...           0.603928   \n",
      "14  {'svc__C': 10, 'svc__gamma': 0.0001, 'svc__ker...           0.475851   \n",
      "15  {'svc__C': 100, 'svc__gamma': 1, 'svc__kernel'...           0.458256   \n",
      "16  {'svc__C': 100, 'svc__gamma': 0.1, 'svc__kerne...           0.458256   \n",
      "17  {'svc__C': 100, 'svc__gamma': 0.01, 'svc__kern...           0.624196   \n",
      "18  {'svc__C': 100, 'svc__gamma': 0.001, 'svc__ker...           0.652809   \n",
      "19  {'svc__C': 100, 'svc__gamma': 0.0001, 'svc__ke...           0.616627   \n",
      "20  {'svc__C': 1000, 'svc__gamma': 1, 'svc__kernel...           0.458256   \n",
      "21  {'svc__C': 1000, 'svc__gamma': 0.1, 'svc__kern...           0.458256   \n",
      "22  {'svc__C': 1000, 'svc__gamma': 0.01, 'svc__ker...           0.624196   \n",
      "23  {'svc__C': 1000, 'svc__gamma': 0.001, 'svc__ke...           0.635078   \n",
      "24  {'svc__C': 1000, 'svc__gamma': 0.0001, 'svc__k...           0.621783   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
      "0            0.452434           0.451460           0.458050         0.455050   \n",
      "1            0.452434           0.451460           0.458050         0.455050   \n",
      "2            0.452434           0.451460           0.458050         0.455050   \n",
      "3            0.452434           0.451460           0.458050         0.455050   \n",
      "4            0.452434           0.451460           0.458050         0.455050   \n",
      "5            0.452434           0.451460           0.458050         0.455050   \n",
      "6            0.452434           0.451460           0.458050         0.455050   \n",
      "7            0.550823           0.576874           0.601274         0.574272   \n",
      "8            0.452434           0.451460           0.465417         0.459120   \n",
      "9            0.452434           0.451460           0.458050         0.455050   \n",
      "10           0.452434           0.451460           0.458050         0.455050   \n",
      "11           0.452434           0.451460           0.458050         0.455050   \n",
      "12           0.656326           0.660715           0.679237         0.652905   \n",
      "13           0.607190           0.608692           0.665272         0.621271   \n",
      "14           0.452434           0.451460           0.472543         0.463072   \n",
      "15           0.452434           0.451460           0.458050         0.455050   \n",
      "16           0.452434           0.451460           0.458050         0.455050   \n",
      "17           0.658688           0.661384           0.665148         0.652354   \n",
      "18           0.672955           0.668173           0.679428         0.668341   \n",
      "19           0.600843           0.614833           0.669212         0.625379   \n",
      "20           0.452434           0.451460           0.458050         0.455050   \n",
      "21           0.452434           0.451460           0.458050         0.455050   \n",
      "22           0.658688           0.661384           0.665527         0.652449   \n",
      "23           0.662757           0.663444           0.647785         0.652266   \n",
      "24           0.653539           0.629038           0.684216         0.647144   \n",
      "\n",
      "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
      "0         0.003123               12            0.453104            0.455659   \n",
      "1         0.003123               12            0.453104            0.455659   \n",
      "2         0.003123               12            0.453104            0.455659   \n",
      "3         0.003123               12            0.453104            0.455659   \n",
      "4         0.003123               12            0.453104            0.455659   \n",
      "5         0.003123               12            1.000000            1.000000   \n",
      "6         0.003123               12            0.976128            0.974625   \n",
      "7         0.018190                9            0.773830            0.762436   \n",
      "8         0.007207               11            0.474383            0.469179   \n",
      "9         0.003123               12            0.453104            0.455659   \n",
      "10        0.003123               12            1.000000            1.000000   \n",
      "11        0.003123               12            1.000000            1.000000   \n",
      "12        0.023329                2            0.976475            0.983649   \n",
      "13        0.025463                8            0.736986            0.728020   \n",
      "14        0.011192               10            0.477619            0.464148   \n",
      "15        0.003123               12            1.000000            1.000000   \n",
      "16        0.003123               12            1.000000            1.000000   \n",
      "17        0.016418                4            1.000000            1.000000   \n",
      "18        0.009817                1            0.856310            0.869971   \n",
      "19        0.026034                7            0.692033            0.675704   \n",
      "20        0.003123               12            1.000000            1.000000   \n",
      "21        0.003123               12            1.000000            1.000000   \n",
      "22        0.016493                3            1.000000            1.000000   \n",
      "23        0.011732                5            0.969490            0.983165   \n",
      "24        0.024425                6            0.754800            0.748895   \n",
      "\n",
      "    split2_train_score  split3_train_score  mean_train_score  std_train_score  \n",
      "0             0.454883            0.451608          0.453814         0.001575  \n",
      "1             0.454883            0.451608          0.453814         0.001575  \n",
      "2             0.454883            0.451608          0.453814         0.001575  \n",
      "3             0.454883            0.451608          0.453814         0.001575  \n",
      "4             0.454883            0.451608          0.453814         0.001575  \n",
      "5             1.000000            1.000000          1.000000         0.000000  \n",
      "6             0.975197            0.970007          0.973989         0.002361  \n",
      "7             0.765899            0.755248          0.764353         0.006685  \n",
      "8             0.467488            0.474614          0.471416         0.003141  \n",
      "9             0.454883            0.451608          0.453814         0.001575  \n",
      "10            1.000000            1.000000          1.000000         0.000000  \n",
      "11            1.000000            1.000000          1.000000         0.000000  \n",
      "12            0.975585            0.976741          0.978112         0.003225  \n",
      "13            0.723883            0.730106          0.729749         0.004741  \n",
      "14            0.467284            0.482697          0.472937         0.007523  \n",
      "15            1.000000            1.000000          1.000000         0.000000  \n",
      "16            1.000000            1.000000          1.000000         0.000000  \n",
      "17            1.000000            1.000000          1.000000         0.000000  \n",
      "18            0.850594            0.846139          0.855753         0.008965  \n",
      "19            0.670012            0.694233          0.682996         0.010364  \n",
      "20            1.000000            1.000000          1.000000         0.000000  \n",
      "21            1.000000            1.000000          1.000000         0.000000  \n",
      "22            1.000000            1.000000          1.000000         0.000000  \n",
      "23            0.967114            0.975918          0.973922         0.006233  \n",
      "24            0.739054            0.753694          0.749111         0.006216  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test_pred: [1 1 0 ... 0 0 1]\n",
      "test score: 0.7084772657918827\n",
      "svc\n",
      "y_test shape: (1676,)\n",
      "0.14314928425357873\n",
      "(6702, 109) (6702,)\n",
      "Fitting 4 folds for each of 25 candidates, totalling 100 fits\n",
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_svc__C  \\\n",
      "0        2.018661      0.586007         1.975463        0.549855          0.1   \n",
      "1        1.990639      0.598679         1.974771        0.561692          0.1   \n",
      "2        0.858668      0.222093         0.793219        0.235119          0.1   \n",
      "3        0.686153      0.181246         0.693766        0.204198          0.1   \n",
      "4        0.683863      0.186120         0.698308        0.206677          0.1   \n",
      "5        1.965826      0.667300         2.031384        0.586754            1   \n",
      "6        1.931931      0.604593         1.950217        0.562543            1   \n",
      "7        0.792645      0.229546         0.784876        0.246786            1   \n",
      "8        0.711661      0.234004         0.677610        0.219508            1   \n",
      "9        0.724878      0.224874         0.734983        0.233705            1   \n",
      "10       2.261005      0.689455         1.971243        0.542881           10   \n",
      "11       2.160160      0.683758         1.962753        0.549085           10   \n",
      "12       0.969226      0.290303         0.832873        0.232322           10   \n",
      "13       0.724262      0.207095         0.667535        0.195226           10   \n",
      "14       0.705699      0.220162         0.676810        0.195753           10   \n",
      "15       2.336466      0.745535         1.950143        0.545080          100   \n",
      "16       2.154037      0.676026         1.909616        0.504831          100   \n",
      "17       0.990929      0.301698         0.798185        0.238065          100   \n",
      "18       0.937493      0.272576         0.680969        0.209697          100   \n",
      "19       0.735607      0.227768         0.649577        0.196736          100   \n",
      "20       2.292032      0.713702         2.029273        0.548691         1000   \n",
      "21       2.201493      0.685420         1.968969        0.558793         1000   \n",
      "22       0.995167      0.290243         0.806021        0.239463         1000   \n",
      "23       1.892181      0.556424         0.614921        0.208959         1000   \n",
      "24       1.000312      0.262452         0.619490        0.205582         1000   \n",
      "\n",
      "   param_svc__gamma param_svc__kernel  \\\n",
      "0                 1               rbf   \n",
      "1               0.1               rbf   \n",
      "2              0.01               rbf   \n",
      "3             0.001               rbf   \n",
      "4            0.0001               rbf   \n",
      "5                 1               rbf   \n",
      "6               0.1               rbf   \n",
      "7              0.01               rbf   \n",
      "8             0.001               rbf   \n",
      "9            0.0001               rbf   \n",
      "10                1               rbf   \n",
      "11              0.1               rbf   \n",
      "12             0.01               rbf   \n",
      "13            0.001               rbf   \n",
      "14           0.0001               rbf   \n",
      "15                1               rbf   \n",
      "16              0.1               rbf   \n",
      "17             0.01               rbf   \n",
      "18            0.001               rbf   \n",
      "19           0.0001               rbf   \n",
      "20                1               rbf   \n",
      "21              0.1               rbf   \n",
      "22             0.01               rbf   \n",
      "23            0.001               rbf   \n",
      "24           0.0001               rbf   \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "0   {'svc__C': 0.1, 'svc__gamma': 1, 'svc__kernel'...           0.456615   \n",
      "1   {'svc__C': 0.1, 'svc__gamma': 0.1, 'svc__kerne...           0.456615   \n",
      "2   {'svc__C': 0.1, 'svc__gamma': 0.01, 'svc__kern...           0.456615   \n",
      "3   {'svc__C': 0.1, 'svc__gamma': 0.001, 'svc__ker...           0.456615   \n",
      "4   {'svc__C': 0.1, 'svc__gamma': 0.0001, 'svc__ke...           0.456615   \n",
      "5   {'svc__C': 1, 'svc__gamma': 1, 'svc__kernel': ...           0.456615   \n",
      "6   {'svc__C': 1, 'svc__gamma': 0.1, 'svc__kernel'...           0.456615   \n",
      "7   {'svc__C': 1, 'svc__gamma': 0.01, 'svc__kernel...           0.539870   \n",
      "8   {'svc__C': 1, 'svc__gamma': 0.001, 'svc__kerne...           0.460289   \n",
      "9   {'svc__C': 1, 'svc__gamma': 0.0001, 'svc__kern...           0.456615   \n",
      "10  {'svc__C': 10, 'svc__gamma': 1, 'svc__kernel':...           0.456615   \n",
      "11  {'svc__C': 10, 'svc__gamma': 0.1, 'svc__kernel...           0.456615   \n",
      "12  {'svc__C': 10, 'svc__gamma': 0.01, 'svc__kerne...           0.596788   \n",
      "13  {'svc__C': 10, 'svc__gamma': 0.001, 'svc__kern...           0.555775   \n",
      "14  {'svc__C': 10, 'svc__gamma': 0.0001, 'svc__ker...           0.471164   \n",
      "15  {'svc__C': 100, 'svc__gamma': 1, 'svc__kernel'...           0.456615   \n",
      "16  {'svc__C': 100, 'svc__gamma': 0.1, 'svc__kerne...           0.456615   \n",
      "17  {'svc__C': 100, 'svc__gamma': 0.01, 'svc__kern...           0.584101   \n",
      "18  {'svc__C': 100, 'svc__gamma': 0.001, 'svc__ker...           0.555092   \n",
      "19  {'svc__C': 100, 'svc__gamma': 0.0001, 'svc__ke...           0.578306   \n",
      "20  {'svc__C': 1000, 'svc__gamma': 1, 'svc__kernel...           0.456615   \n",
      "21  {'svc__C': 1000, 'svc__gamma': 0.1, 'svc__kern...           0.456615   \n",
      "22  {'svc__C': 1000, 'svc__gamma': 0.01, 'svc__ker...           0.584101   \n",
      "23  {'svc__C': 1000, 'svc__gamma': 0.001, 'svc__ke...           0.556759   \n",
      "24  {'svc__C': 1000, 'svc__gamma': 0.0001, 'svc__k...           0.560200   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
      "0            0.459319           0.436275           0.454916         0.451781   \n",
      "1            0.459319           0.436275           0.454916         0.451781   \n",
      "2            0.459319           0.436275           0.454916         0.451781   \n",
      "3            0.459319           0.436275           0.454916         0.451781   \n",
      "4            0.459319           0.436275           0.454916         0.451781   \n",
      "5            0.459319           0.436275           0.454916         0.451781   \n",
      "6            0.459319           0.436275           0.454916         0.451781   \n",
      "7            0.541111           0.519677           0.534220         0.533719   \n",
      "8            0.459319           0.436275           0.454916         0.452700   \n",
      "9            0.459319           0.436275           0.454916         0.451781   \n",
      "10           0.459319           0.436275           0.454916         0.451781   \n",
      "11           0.459319           0.436275           0.454916         0.451781   \n",
      "12           0.650064           0.651615           0.665639         0.641026   \n",
      "13           0.609398           0.607344           0.572608         0.586281   \n",
      "14           0.459319           0.436275           0.454916         0.455418   \n",
      "15           0.459319           0.436275           0.454916         0.451781   \n",
      "16           0.459319           0.436275           0.454916         0.451781   \n",
      "17           0.648677           0.642503           0.655368         0.632662   \n",
      "18           0.635877           0.662640           0.646719         0.625082   \n",
      "19           0.625161           0.621368           0.574079         0.599728   \n",
      "20           0.459319           0.436275           0.454916         0.451781   \n",
      "21           0.459319           0.436275           0.454916         0.451781   \n",
      "22           0.648677           0.643646           0.657149         0.633393   \n",
      "23           0.631536           0.660089           0.654947         0.625833   \n",
      "24           0.640551           0.661053           0.607303         0.617277   \n",
      "\n",
      "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
      "0         0.009089               12            0.454827            0.452357   \n",
      "1         0.009089               12            0.454827            0.452357   \n",
      "2         0.009089               12            0.454827            0.452357   \n",
      "3         0.009089               12            0.454827            0.452357   \n",
      "4         0.009089               12            0.454827            0.452357   \n",
      "5         0.009089               12            1.000000            1.000000   \n",
      "6         0.009089               12            0.980063            0.977917   \n",
      "7         0.008514                9            0.768362            0.755013   \n",
      "8         0.009697               11            0.467332            0.461525   \n",
      "9         0.009089               12            0.454827            0.452357   \n",
      "10        0.009089               12            1.000000            1.000000   \n",
      "11        0.009089               12            1.000000            1.000000   \n",
      "12        0.026252                1            0.978774            0.978727   \n",
      "13        0.022889                8            0.731560            0.733207   \n",
      "14        0.012549               10            0.468344            0.460007   \n",
      "15        0.009089               12            1.000000            1.000000   \n",
      "16        0.009089               12            1.000000            1.000000   \n",
      "17        0.028404                3            1.000000            1.000000   \n",
      "18        0.041515                5            0.851519            0.851728   \n",
      "19        0.023622                7            0.687110            0.697207   \n",
      "20        0.009089               12            1.000000            1.000000   \n",
      "21        0.009089               12            1.000000            1.000000   \n",
      "22        0.028865                2            1.000000            1.000000   \n",
      "23        0.041306                4            0.968841            0.971786   \n",
      "24        0.038129                6            0.750826            0.746317   \n",
      "\n",
      "    split2_train_score  split3_train_score  mean_train_score  std_train_score  \n",
      "0             0.457797            0.455513          0.455124         0.001939  \n",
      "1             0.457797            0.455513          0.455124         0.001939  \n",
      "2             0.457797            0.455513          0.455124         0.001939  \n",
      "3             0.457797            0.455513          0.455124         0.001939  \n",
      "4             0.457797            0.455513          0.455124         0.001939  \n",
      "5             1.000000            1.000000          1.000000         0.000000  \n",
      "6             0.972541            0.969865          0.975097         0.004078  \n",
      "7             0.745892            0.724737          0.748501         0.015877  \n",
      "8             0.457797            0.458470          0.461281         0.003765  \n",
      "9             0.457797            0.455513          0.455124         0.001939  \n",
      "10            1.000000            1.000000          1.000000         0.000000  \n",
      "11            1.000000            1.000000          1.000000         0.000000  \n",
      "12            0.975018            0.972694          0.976303         0.002581  \n",
      "13            0.693330            0.694543          0.713160         0.019237  \n",
      "14            0.457797            0.456994          0.460786         0.004501  \n",
      "15            1.000000            1.000000          1.000000         0.000000  \n",
      "16            1.000000            1.000000          1.000000         0.000000  \n",
      "17            1.000000            1.000000          1.000000         0.000000  \n",
      "18            0.837028            0.835002          0.843819         0.007837  \n",
      "19            0.648781            0.659156          0.673063         0.019769  \n",
      "20            1.000000            1.000000          1.000000         0.000000  \n",
      "21            1.000000            1.000000          1.000000         0.000000  \n",
      "22            1.000000            1.000000          1.000000         0.000000  \n",
      "23            0.960116            0.961242          0.965496         0.004945  \n",
      "24            0.723491            0.717712          0.734586         0.014223  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test_pred: [0 0 0 ... 0 0 0]\n",
      "test score: 0.7129416628523337\n",
      "svc\n",
      "y_test shape: (1676,)\n",
      "0.1391884951206985\n",
      "(6702, 109) (6702,)\n",
      "Fitting 4 folds for each of 25 candidates, totalling 100 fits\n",
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_svc__C  \\\n",
      "0        1.629391      0.322016         2.252030        0.198923          0.1   \n",
      "1        1.561028      0.316935         2.235906        0.220381          0.1   \n",
      "2        0.703845      0.157992         0.943435        0.120964          0.1   \n",
      "3        0.638058      0.103220         0.861870        0.085348          0.1   \n",
      "4        0.634803      0.098605         0.863595        0.087704          0.1   \n",
      "5        1.756168      0.345398         2.353551        0.200940            1   \n",
      "6        1.638705      0.303510         2.290337        0.204665            1   \n",
      "7        0.712137      0.147045         0.947285        0.113131            1   \n",
      "8        0.630583      0.143440         0.904840        0.098877            1   \n",
      "9        0.642828      0.150013         0.837212        0.084939            1   \n",
      "10       1.857515      0.415883         2.318970        0.204121           10   \n",
      "11       1.777323      0.381261         2.413637        0.195368           10   \n",
      "12       0.800911      0.173185         0.989849        0.125055           10   \n",
      "13       0.642440      0.109854         0.833182        0.102226           10   \n",
      "14       0.616681      0.112452         0.878277        0.083331           10   \n",
      "15       1.969113      0.445288         2.405803        0.227532          100   \n",
      "16       1.895335      0.373099         2.309570        0.217197          100   \n",
      "17       0.864823      0.205213         0.978700        0.115547          100   \n",
      "18       0.867841      0.171146         0.848184        0.086543          100   \n",
      "19       0.705467      0.159200         0.863546        0.063518          100   \n",
      "20       2.000200      0.410875         2.378700        0.237101         1000   \n",
      "21       1.815971      0.390509         2.388722        0.231806         1000   \n",
      "22       0.860483      0.175895         1.014348        0.101624         1000   \n",
      "23       1.614430      0.360394         0.749634        0.104156         1000   \n",
      "24       0.869027      0.191256         0.733873        0.113947         1000   \n",
      "\n",
      "   param_svc__gamma param_svc__kernel  \\\n",
      "0                 1               rbf   \n",
      "1               0.1               rbf   \n",
      "2              0.01               rbf   \n",
      "3             0.001               rbf   \n",
      "4            0.0001               rbf   \n",
      "5                 1               rbf   \n",
      "6               0.1               rbf   \n",
      "7              0.01               rbf   \n",
      "8             0.001               rbf   \n",
      "9            0.0001               rbf   \n",
      "10                1               rbf   \n",
      "11              0.1               rbf   \n",
      "12             0.01               rbf   \n",
      "13            0.001               rbf   \n",
      "14           0.0001               rbf   \n",
      "15                1               rbf   \n",
      "16              0.1               rbf   \n",
      "17             0.01               rbf   \n",
      "18            0.001               rbf   \n",
      "19           0.0001               rbf   \n",
      "20                1               rbf   \n",
      "21              0.1               rbf   \n",
      "22             0.01               rbf   \n",
      "23            0.001               rbf   \n",
      "24           0.0001               rbf   \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "0   {'svc__C': 0.1, 'svc__gamma': 1, 'svc__kernel'...           0.463723   \n",
      "1   {'svc__C': 0.1, 'svc__gamma': 0.1, 'svc__kerne...           0.463723   \n",
      "2   {'svc__C': 0.1, 'svc__gamma': 0.01, 'svc__kern...           0.463723   \n",
      "3   {'svc__C': 0.1, 'svc__gamma': 0.001, 'svc__ker...           0.463723   \n",
      "4   {'svc__C': 0.1, 'svc__gamma': 0.0001, 'svc__ke...           0.463723   \n",
      "5   {'svc__C': 1, 'svc__gamma': 1, 'svc__kernel': ...           0.463723   \n",
      "6   {'svc__C': 1, 'svc__gamma': 0.1, 'svc__kernel'...           0.463723   \n",
      "7   {'svc__C': 1, 'svc__gamma': 0.01, 'svc__kernel...           0.617175   \n",
      "8   {'svc__C': 1, 'svc__gamma': 0.001, 'svc__kerne...           0.471677   \n",
      "9   {'svc__C': 1, 'svc__gamma': 0.0001, 'svc__kern...           0.463723   \n",
      "10  {'svc__C': 10, 'svc__gamma': 1, 'svc__kernel':...           0.463723   \n",
      "11  {'svc__C': 10, 'svc__gamma': 0.1, 'svc__kernel...           0.463723   \n",
      "12  {'svc__C': 10, 'svc__gamma': 0.01, 'svc__kerne...           0.678877   \n",
      "13  {'svc__C': 10, 'svc__gamma': 0.001, 'svc__kern...           0.649908   \n",
      "14  {'svc__C': 10, 'svc__gamma': 0.0001, 'svc__ker...           0.475609   \n",
      "15  {'svc__C': 100, 'svc__gamma': 1, 'svc__kernel'...           0.463723   \n",
      "16  {'svc__C': 100, 'svc__gamma': 0.1, 'svc__kerne...           0.463723   \n",
      "17  {'svc__C': 100, 'svc__gamma': 0.01, 'svc__kern...           0.657543   \n",
      "18  {'svc__C': 100, 'svc__gamma': 0.001, 'svc__ker...           0.694891   \n",
      "19  {'svc__C': 100, 'svc__gamma': 0.0001, 'svc__ke...           0.654458   \n",
      "20  {'svc__C': 1000, 'svc__gamma': 1, 'svc__kernel...           0.463723   \n",
      "21  {'svc__C': 1000, 'svc__gamma': 0.1, 'svc__kern...           0.463723   \n",
      "22  {'svc__C': 1000, 'svc__gamma': 0.01, 'svc__ker...           0.659853   \n",
      "23  {'svc__C': 1000, 'svc__gamma': 0.001, 'svc__ke...           0.619706   \n",
      "24  {'svc__C': 1000, 'svc__gamma': 0.0001, 'svc__k...           0.675517   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
      "0            0.463338           0.460779           0.453801         0.460410   \n",
      "1            0.463338           0.460779           0.453801         0.460410   \n",
      "2            0.463338           0.460779           0.453801         0.460410   \n",
      "3            0.463338           0.460779           0.453801         0.460410   \n",
      "4            0.463338           0.460779           0.453801         0.460410   \n",
      "5            0.463338           0.460779           0.453801         0.460410   \n",
      "6            0.463338           0.460779           0.453801         0.460410   \n",
      "7            0.567074           0.599044           0.560537         0.585958   \n",
      "8            0.463338           0.468182           0.453801         0.464249   \n",
      "9            0.463338           0.460779           0.453801         0.460410   \n",
      "10           0.463338           0.460779           0.453801         0.460410   \n",
      "11           0.463338           0.460779           0.453801         0.460410   \n",
      "12           0.664025           0.657604           0.675144         0.668912   \n",
      "13           0.600217           0.614944           0.617925         0.620748   \n",
      "14           0.463338           0.482687           0.453801         0.468859   \n",
      "15           0.463338           0.460779           0.453801         0.460410   \n",
      "16           0.463338           0.460779           0.453801         0.460410   \n",
      "17           0.646152           0.663360           0.667555         0.658653   \n",
      "18           0.643613           0.642463           0.684512         0.666370   \n",
      "19           0.609485           0.595732           0.634045         0.623430   \n",
      "20           0.463338           0.460779           0.453801         0.460410   \n",
      "21           0.463338           0.460779           0.453801         0.460410   \n",
      "22           0.645044           0.663360           0.665579         0.658459   \n",
      "23           0.628881           0.644212           0.654994         0.636948   \n",
      "24           0.620609           0.616530           0.651128         0.640946   \n",
      "\n",
      "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
      "0         0.003980               12            0.451310            0.448946   \n",
      "1         0.003980               12            0.451310            0.448946   \n",
      "2         0.003980               12            0.451310            0.448946   \n",
      "3         0.003980               12            0.451310            0.448946   \n",
      "4         0.003980               12            0.451310            0.448946   \n",
      "5         0.003980               12            1.000000            1.000000   \n",
      "6         0.003980               12            0.978354            0.971514   \n",
      "7         0.023176                9            0.751971            0.744710   \n",
      "8         0.006720               11            0.457340            0.454600   \n",
      "9         0.003980               12            0.451310            0.448946   \n",
      "10        0.003980               12            1.000000            1.000000   \n",
      "11        0.003980               12            1.000000            1.000000   \n",
      "12        0.008513                1            0.978621            0.977947   \n",
      "13        0.018121                8            0.707458            0.712007   \n",
      "14        0.011113               10            0.462016            0.456005   \n",
      "15        0.003980               12            1.000000            1.000000   \n",
      "16        0.003980               12            1.000000            1.000000   \n",
      "17        0.008045                3            1.000000            1.000000   \n",
      "18        0.023622                2            0.845768            0.853417   \n",
      "19        0.022567                7            0.661448            0.664809   \n",
      "20        0.003980               12            1.000000            1.000000   \n",
      "21        0.003980               12            1.000000            1.000000   \n",
      "22        0.008010                4            1.000000            1.000000   \n",
      "23        0.013609                6            0.970159            0.970249   \n",
      "24        0.024024                5            0.738055            0.735700   \n",
      "\n",
      "    split2_train_score  split3_train_score  mean_train_score  std_train_score  \n",
      "0             0.452520            0.455720          0.452124         0.002442  \n",
      "1             0.452520            0.455720          0.452124         0.002442  \n",
      "2             0.452520            0.455720          0.452124         0.002442  \n",
      "3             0.452520            0.455720          0.452124         0.002442  \n",
      "4             0.452520            0.455720          0.452124         0.002442  \n",
      "5             1.000000            1.000000          1.000000         0.000000  \n",
      "6             0.978285            0.971194          0.974837         0.003485  \n",
      "7             0.766757            0.715315          0.744688         0.018727  \n",
      "8             0.469566            0.455720          0.459307         0.006003  \n",
      "9             0.452520            0.455720          0.452124         0.002442  \n",
      "10            1.000000            1.000000          1.000000         0.000000  \n",
      "11            1.000000            1.000000          1.000000         0.000000  \n",
      "12            0.978224            0.976607          0.977850         0.000756  \n",
      "13            0.725351            0.691426          0.709061         0.012120  \n",
      "14            0.474217            0.455720          0.461989         0.007494  \n",
      "15            1.000000            1.000000          1.000000         0.000000  \n",
      "16            1.000000            1.000000          1.000000         0.000000  \n",
      "17            1.000000            1.000000          1.000000         0.000000  \n",
      "18            0.856321            0.849033          0.851135         0.004041  \n",
      "19            0.674841            0.661038          0.665534         0.005569  \n",
      "20            1.000000            1.000000          1.000000         0.000000  \n",
      "21            1.000000            1.000000          1.000000         0.000000  \n",
      "22            1.000000            1.000000          1.000000         0.000000  \n",
      "23            0.969276            0.971883          0.970392         0.000941  \n",
      "24            0.747807            0.727308          0.737217         0.007303  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test_pred: [0 0 1 ... 0 0 0]\n",
      "test score: 0.7229431701037328\n",
      "svc\n",
      "y_test shape: (1676,)\n",
      "0.15010141987829617\n",
      "(6702, 109) (6702,)\n",
      "Fitting 4 folds for each of 25 candidates, totalling 100 fits\n",
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_svc__C  \\\n",
      "0        1.863489      0.146344         2.075330        0.089241          0.1   \n",
      "1        1.853721      0.146864         2.070969        0.120099          0.1   \n",
      "2        0.740879      0.100358         0.807353        0.062804          0.1   \n",
      "3        0.670988      0.052705         0.721774        0.051320          0.1   \n",
      "4        0.642289      0.071319         0.699485        0.035500          0.1   \n",
      "5        1.771718      0.144205         2.170717        0.124960            1   \n",
      "6        1.797083      0.111758         2.128962        0.098867            1   \n",
      "7        0.743643      0.077687         0.812615        0.056309            1   \n",
      "8        0.654887      0.045530         0.722888        0.046736            1   \n",
      "9        0.639932      0.056243         0.684935        0.039081            1   \n",
      "10       2.115544      0.123814         2.171106        0.123733           10   \n",
      "11       2.085167      0.145700         2.173104        0.109384           10   \n",
      "12       0.860138      0.086302         0.850078        0.041656           10   \n",
      "13       0.674451      0.048631         0.669225        0.031435           10   \n",
      "14       0.651602      0.057092         0.714347        0.061683           10   \n",
      "15       2.369460      0.138773         2.282453        0.134809          100   \n",
      "16       2.277006      0.189457         2.175273        0.112321          100   \n",
      "17       0.975907      0.084479         0.848090        0.011440          100   \n",
      "18       0.917771      0.077179         0.767229        0.053905          100   \n",
      "19       0.725682      0.064453         0.780573        0.047568          100   \n",
      "20       2.227727      0.165586         2.240134        0.149455         1000   \n",
      "21       2.063488      0.145538         2.236370        0.096444         1000   \n",
      "22       0.930310      0.070969         0.900822        0.108078         1000   \n",
      "23       1.676314      0.141400         0.585582        0.032842         1000   \n",
      "24       0.923646      0.121797         0.576296        0.030791         1000   \n",
      "\n",
      "   param_svc__gamma param_svc__kernel  \\\n",
      "0                 1               rbf   \n",
      "1               0.1               rbf   \n",
      "2              0.01               rbf   \n",
      "3             0.001               rbf   \n",
      "4            0.0001               rbf   \n",
      "5                 1               rbf   \n",
      "6               0.1               rbf   \n",
      "7              0.01               rbf   \n",
      "8             0.001               rbf   \n",
      "9            0.0001               rbf   \n",
      "10                1               rbf   \n",
      "11              0.1               rbf   \n",
      "12             0.01               rbf   \n",
      "13            0.001               rbf   \n",
      "14           0.0001               rbf   \n",
      "15                1               rbf   \n",
      "16              0.1               rbf   \n",
      "17             0.01               rbf   \n",
      "18            0.001               rbf   \n",
      "19           0.0001               rbf   \n",
      "20                1               rbf   \n",
      "21              0.1               rbf   \n",
      "22             0.01               rbf   \n",
      "23            0.001               rbf   \n",
      "24           0.0001               rbf   \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "0   {'svc__C': 0.1, 'svc__gamma': 1, 'svc__kernel'...           0.453037   \n",
      "1   {'svc__C': 0.1, 'svc__gamma': 0.1, 'svc__kerne...           0.453037   \n",
      "2   {'svc__C': 0.1, 'svc__gamma': 0.01, 'svc__kern...           0.453037   \n",
      "3   {'svc__C': 0.1, 'svc__gamma': 0.001, 'svc__ker...           0.453037   \n",
      "4   {'svc__C': 0.1, 'svc__gamma': 0.0001, 'svc__ke...           0.453037   \n",
      "5   {'svc__C': 1, 'svc__gamma': 1, 'svc__kernel': ...           0.453037   \n",
      "6   {'svc__C': 1, 'svc__gamma': 0.1, 'svc__kernel'...           0.453037   \n",
      "7   {'svc__C': 1, 'svc__gamma': 0.01, 'svc__kernel...           0.648627   \n",
      "8   {'svc__C': 1, 'svc__gamma': 0.001, 'svc__kerne...           0.453037   \n",
      "9   {'svc__C': 1, 'svc__gamma': 0.0001, 'svc__kern...           0.453037   \n",
      "10  {'svc__C': 10, 'svc__gamma': 1, 'svc__kernel':...           0.453037   \n",
      "11  {'svc__C': 10, 'svc__gamma': 0.1, 'svc__kernel...           0.453037   \n",
      "12  {'svc__C': 10, 'svc__gamma': 0.01, 'svc__kerne...           0.680724   \n",
      "13  {'svc__C': 10, 'svc__gamma': 0.001, 'svc__kern...           0.656147   \n",
      "14  {'svc__C': 10, 'svc__gamma': 0.0001, 'svc__ker...           0.453037   \n",
      "15  {'svc__C': 100, 'svc__gamma': 1, 'svc__kernel'...           0.453037   \n",
      "16  {'svc__C': 100, 'svc__gamma': 0.1, 'svc__kerne...           0.453037   \n",
      "17  {'svc__C': 100, 'svc__gamma': 0.01, 'svc__kern...           0.671614   \n",
      "18  {'svc__C': 100, 'svc__gamma': 0.001, 'svc__ker...           0.688856   \n",
      "19  {'svc__C': 100, 'svc__gamma': 0.0001, 'svc__ke...           0.638302   \n",
      "20  {'svc__C': 1000, 'svc__gamma': 1, 'svc__kernel...           0.453037   \n",
      "21  {'svc__C': 1000, 'svc__gamma': 0.1, 'svc__kern...           0.453037   \n",
      "22  {'svc__C': 1000, 'svc__gamma': 0.01, 'svc__ker...           0.669124   \n",
      "23  {'svc__C': 1000, 'svc__gamma': 0.001, 'svc__ke...           0.668424   \n",
      "24  {'svc__C': 1000, 'svc__gamma': 0.0001, 'svc__k...           0.687354   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
      "0            0.456890           0.453358           0.452191         0.453869   \n",
      "1            0.456890           0.453358           0.452191         0.453869   \n",
      "2            0.456890           0.453358           0.452191         0.453869   \n",
      "3            0.456890           0.453358           0.452191         0.453869   \n",
      "4            0.456890           0.453358           0.452191         0.453869   \n",
      "5            0.456890           0.453358           0.452191         0.453869   \n",
      "6            0.456890           0.453358           0.452191         0.453869   \n",
      "7            0.570941           0.603000           0.564888         0.596864   \n",
      "8            0.462780           0.453358           0.452191         0.455341   \n",
      "9            0.456890           0.453358           0.452191         0.453869   \n",
      "10           0.456890           0.453358           0.452191         0.453869   \n",
      "11           0.456890           0.453358           0.452191         0.453869   \n",
      "12           0.637094           0.668103           0.649146         0.658767   \n",
      "13           0.600301           0.648760           0.637267         0.635618   \n",
      "14           0.462631           0.459166           0.452191         0.456756   \n",
      "15           0.456890           0.453358           0.452191         0.453869   \n",
      "16           0.456890           0.453358           0.452191         0.453869   \n",
      "17           0.628098           0.651598           0.639818         0.647782   \n",
      "18           0.644690           0.664857           0.660373         0.664694   \n",
      "19           0.591235           0.647037           0.650722         0.631824   \n",
      "20           0.456890           0.453358           0.452191         0.453869   \n",
      "21           0.456890           0.453358           0.452191         0.453869   \n",
      "22           0.628098           0.651598           0.639818         0.647160   \n",
      "23           0.649163           0.664874           0.647169         0.657407   \n",
      "24           0.631797           0.666092           0.663670         0.662228   \n",
      "\n",
      "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
      "0         0.001796               12            0.457046            0.455570   \n",
      "1         0.001796               12            0.457046            0.455570   \n",
      "2         0.001796               12            0.457046            0.455570   \n",
      "3         0.001796               12            0.457046            0.455570   \n",
      "4         0.001796               12            0.457046            0.455570   \n",
      "5         0.001796               12            1.000000            1.000000   \n",
      "6         0.001796               12            0.974917            0.975575   \n",
      "7         0.033210                9            0.744031            0.763021   \n",
      "8         0.004316               11            0.457046            0.466809   \n",
      "9         0.001796               12            0.457046            0.455570   \n",
      "10        0.001796               12            1.000000            1.000000   \n",
      "11        0.001796               12            1.000000            1.000000   \n",
      "12        0.016819                3            0.983479            0.981021   \n",
      "13        0.021472                7            0.711373            0.732713   \n",
      "14        0.004330               10            0.457046            0.469582   \n",
      "15        0.001796               12            1.000000            1.000000   \n",
      "16        0.001796               12            1.000000            1.000000   \n",
      "17        0.016073                5            1.000000            1.000000   \n",
      "18        0.015832                1            0.860043            0.869059   \n",
      "19        0.023864                8            0.665569            0.696022   \n",
      "20        0.001796               12            1.000000            1.000000   \n",
      "21        0.001796               12            1.000000            1.000000   \n",
      "22        0.015161                6            1.000000            1.000000   \n",
      "23        0.009353                4            0.972293            0.976469   \n",
      "24        0.019839                2            0.739409            0.754324   \n",
      "\n",
      "    split2_train_score  split3_train_score  mean_train_score  std_train_score  \n",
      "0             0.457200            0.457600          0.456854         0.000768  \n",
      "1             0.457200            0.457600          0.456854         0.000768  \n",
      "2             0.457200            0.457600          0.456854         0.000768  \n",
      "3             0.457200            0.457600          0.456854         0.000768  \n",
      "4             0.457200            0.457600          0.456854         0.000768  \n",
      "5             1.000000            1.000000          1.000000         0.000000  \n",
      "6             0.968024            0.975040          0.973389         0.003107  \n",
      "7             0.739278            0.741263          0.746898         0.009460  \n",
      "8             0.464300            0.461812          0.462492         0.003607  \n",
      "9             0.457200            0.457600          0.456854         0.000768  \n",
      "10            1.000000            1.000000          1.000000         0.000000  \n",
      "11            1.000000            1.000000          1.000000         0.000000  \n",
      "12            0.978834            0.982924          0.981564         0.001821  \n",
      "13            0.699054            0.702155          0.711324         0.013154  \n",
      "14            0.465709            0.457600          0.462484         0.005344  \n",
      "15            1.000000            1.000000          1.000000         0.000000  \n",
      "16            1.000000            1.000000          1.000000         0.000000  \n",
      "17            1.000000            1.000000          1.000000         0.000000  \n",
      "18            0.861160            0.853720          0.860995         0.005452  \n",
      "19            0.677757            0.662656          0.675501         0.013132  \n",
      "20            1.000000            1.000000          1.000000         0.000000  \n",
      "21            1.000000            1.000000          1.000000         0.000000  \n",
      "22            1.000000            1.000000          1.000000         0.000000  \n",
      "23            0.973393            0.974706          0.974215         0.001557  \n",
      "24            0.737143            0.733959          0.741209         0.007816  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test_pred: [1 0 0 ... 0 0 0]\n",
      "test score: 0.7037052443298218\n",
      "svc\n",
      "y_test shape: (1676,)\n",
      "0.13519091847265222\n",
      "(6702, 109) (6702,)\n",
      "Fitting 4 folds for each of 25 candidates, totalling 100 fits\n",
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_svc__C  \\\n",
      "0        2.206021      0.465772         2.094696        0.433049          0.1   \n",
      "1        2.140641      0.482284         2.101058        0.376856          0.1   \n",
      "2        0.919360      0.295542         0.784300        0.172963          0.1   \n",
      "3        0.729273      0.154816         0.669074        0.156590          0.1   \n",
      "4        0.717076      0.124204         0.753432        0.222944          0.1   \n",
      "5        2.273898      0.483427         2.086749        0.415357            1   \n",
      "6        2.186149      0.520576         2.088369        0.408055            1   \n",
      "7        0.858017      0.192512         0.820037        0.170417            1   \n",
      "8        0.739490      0.162989         0.731183        0.175735            1   \n",
      "9        0.755876      0.134970         0.755818        0.159336            1   \n",
      "10       2.688147      0.611614         2.318471        0.378259           10   \n",
      "11       2.637442      0.607889         2.549694        0.513364           10   \n",
      "12       1.169671      0.306600         0.996914        0.236933           10   \n",
      "13       0.910070      0.254999         0.791466        0.144258           10   \n",
      "14       0.821464      0.179724         0.800124        0.194964           10   \n",
      "15       2.601335      0.678174         2.295088        0.381328          100   \n",
      "16       2.541792      0.494203         2.142077        0.405319          100   \n",
      "17       1.203761      0.347407         0.883758        0.163287          100   \n",
      "18       1.066153      0.197191         0.722394        0.182195          100   \n",
      "19       0.824862      0.153488         0.703573        0.130373          100   \n",
      "20       2.535843      0.658513         2.275795        0.447568         1000   \n",
      "21       2.575058      0.534875         2.111189        0.413148         1000   \n",
      "22       1.171387      0.381095         0.861412        0.144559         1000   \n",
      "23       2.118476      0.358248         0.646437        0.106832         1000   \n",
      "24       1.094978      0.268005         0.603454        0.096410         1000   \n",
      "\n",
      "   param_svc__gamma param_svc__kernel  \\\n",
      "0                 1               rbf   \n",
      "1               0.1               rbf   \n",
      "2              0.01               rbf   \n",
      "3             0.001               rbf   \n",
      "4            0.0001               rbf   \n",
      "5                 1               rbf   \n",
      "6               0.1               rbf   \n",
      "7              0.01               rbf   \n",
      "8             0.001               rbf   \n",
      "9            0.0001               rbf   \n",
      "10                1               rbf   \n",
      "11              0.1               rbf   \n",
      "12             0.01               rbf   \n",
      "13            0.001               rbf   \n",
      "14           0.0001               rbf   \n",
      "15                1               rbf   \n",
      "16              0.1               rbf   \n",
      "17             0.01               rbf   \n",
      "18            0.001               rbf   \n",
      "19           0.0001               rbf   \n",
      "20                1               rbf   \n",
      "21              0.1               rbf   \n",
      "22             0.01               rbf   \n",
      "23            0.001               rbf   \n",
      "24           0.0001               rbf   \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "0   {'svc__C': 0.1, 'svc__gamma': 1, 'svc__kernel'...           0.444262   \n",
      "1   {'svc__C': 0.1, 'svc__gamma': 0.1, 'svc__kerne...           0.444262   \n",
      "2   {'svc__C': 0.1, 'svc__gamma': 0.01, 'svc__kern...           0.444262   \n",
      "3   {'svc__C': 0.1, 'svc__gamma': 0.001, 'svc__ker...           0.444262   \n",
      "4   {'svc__C': 0.1, 'svc__gamma': 0.0001, 'svc__ke...           0.444262   \n",
      "5   {'svc__C': 1, 'svc__gamma': 1, 'svc__kernel': ...           0.444262   \n",
      "6   {'svc__C': 1, 'svc__gamma': 0.1, 'svc__kernel'...           0.444262   \n",
      "7   {'svc__C': 1, 'svc__gamma': 0.01, 'svc__kernel...           0.566979   \n",
      "8   {'svc__C': 1, 'svc__gamma': 0.001, 'svc__kerne...           0.444262   \n",
      "9   {'svc__C': 1, 'svc__gamma': 0.0001, 'svc__kern...           0.444262   \n",
      "10  {'svc__C': 10, 'svc__gamma': 1, 'svc__kernel':...           0.444262   \n",
      "11  {'svc__C': 10, 'svc__gamma': 0.1, 'svc__kernel...           0.444262   \n",
      "12  {'svc__C': 10, 'svc__gamma': 0.01, 'svc__kerne...           0.662308   \n",
      "13  {'svc__C': 10, 'svc__gamma': 0.001, 'svc__kern...           0.629414   \n",
      "14  {'svc__C': 10, 'svc__gamma': 0.0001, 'svc__ker...           0.444262   \n",
      "15  {'svc__C': 100, 'svc__gamma': 1, 'svc__kernel'...           0.444262   \n",
      "16  {'svc__C': 100, 'svc__gamma': 0.1, 'svc__kerne...           0.444262   \n",
      "17  {'svc__C': 100, 'svc__gamma': 0.01, 'svc__kern...           0.652927   \n",
      "18  {'svc__C': 100, 'svc__gamma': 0.001, 'svc__ker...           0.675496   \n",
      "19  {'svc__C': 100, 'svc__gamma': 0.0001, 'svc__ke...           0.631182   \n",
      "20  {'svc__C': 1000, 'svc__gamma': 1, 'svc__kernel...           0.444262   \n",
      "21  {'svc__C': 1000, 'svc__gamma': 0.1, 'svc__kern...           0.444262   \n",
      "22  {'svc__C': 1000, 'svc__gamma': 0.01, 'svc__ker...           0.651531   \n",
      "23  {'svc__C': 1000, 'svc__gamma': 0.001, 'svc__ke...           0.647579   \n",
      "24  {'svc__C': 1000, 'svc__gamma': 0.0001, 'svc__k...           0.650087   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
      "0            0.447471           0.457038           0.456637         0.451352   \n",
      "1            0.447471           0.457038           0.456637         0.451352   \n",
      "2            0.447471           0.457038           0.456637         0.451352   \n",
      "3            0.447471           0.457038           0.456637         0.451352   \n",
      "4            0.447471           0.457038           0.456637         0.451352   \n",
      "5            0.447471           0.457038           0.456637         0.451352   \n",
      "6            0.447471           0.457038           0.456637         0.451352   \n",
      "7            0.565690           0.581829           0.584207         0.574676   \n",
      "8            0.447471           0.457038           0.456637         0.451352   \n",
      "9            0.447471           0.457038           0.456637         0.451352   \n",
      "10           0.447471           0.457038           0.456637         0.451352   \n",
      "11           0.447471           0.457038           0.456637         0.451352   \n",
      "12           0.635331           0.658252           0.664467         0.655090   \n",
      "13           0.638374           0.621817           0.634152         0.630939   \n",
      "14           0.447471           0.457038           0.456637         0.451352   \n",
      "15           0.447471           0.457038           0.456637         0.451352   \n",
      "16           0.447471           0.457038           0.456637         0.451352   \n",
      "17           0.627192           0.661899           0.655204         0.649306   \n",
      "18           0.651074           0.675998           0.665893         0.667115   \n",
      "19           0.632540           0.620000           0.643709         0.631858   \n",
      "20           0.447471           0.457038           0.456637         0.451352   \n",
      "21           0.447471           0.457038           0.456637         0.451352   \n",
      "22           0.626504           0.661899           0.655204         0.648785   \n",
      "23           0.612735           0.653110           0.667855         0.645320   \n",
      "24           0.649857           0.650132           0.643985         0.648515   \n",
      "\n",
      "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
      "0         0.005604               10            0.457027            0.456132   \n",
      "1         0.005604               10            0.457027            0.456132   \n",
      "2         0.005604               10            0.457027            0.456132   \n",
      "3         0.005604               10            0.457027            0.456132   \n",
      "4         0.005604               10            0.457027            0.456132   \n",
      "5         0.005604               10            1.000000            1.000000   \n",
      "6         0.005604               10            0.969294            0.969846   \n",
      "7         0.008396                9            0.724310            0.751213   \n",
      "8         0.005604               10            0.457027            0.463217   \n",
      "9         0.005604               10            0.457027            0.456132   \n",
      "10        0.005604               10            1.000000            1.000000   \n",
      "11        0.005604               10            1.000000            1.000000   \n",
      "12        0.011624                2            0.975912            0.976238   \n",
      "13        0.006147                8            0.679463            0.701558   \n",
      "14        0.005604               10            0.457027            0.468845   \n",
      "15        0.005604               10            1.000000            1.000000   \n",
      "16        0.005604               10            1.000000            1.000000   \n",
      "17        0.013186                3            1.000000            1.000000   \n",
      "18        0.010099                1            0.835506            0.844173   \n",
      "19        0.008396                7            0.633506            0.662589   \n",
      "20        0.005604               10            1.000000            1.000000   \n",
      "21        0.005604               10            1.000000            1.000000   \n",
      "22        0.013390                4            1.000000            1.000000   \n",
      "23        0.020220                6            0.962747            0.963077   \n",
      "24        0.002617                5            0.701602            0.722875   \n",
      "\n",
      "    split2_train_score  split3_train_score  mean_train_score  std_train_score  \n",
      "0             0.453097            0.453258          0.454878         0.001731  \n",
      "1             0.453097            0.453258          0.454878         0.001731  \n",
      "2             0.453097            0.453258          0.454878         0.001731  \n",
      "3             0.453097            0.453258          0.454878         0.001731  \n",
      "4             0.453097            0.453258          0.454878         0.001731  \n",
      "5             1.000000            1.000000          1.000000         0.000000  \n",
      "6             0.977380            0.976881          0.973350         0.003789  \n",
      "7             0.740381            0.724029          0.734983         0.011472  \n",
      "8             0.457296            0.453258          0.457699         0.003563  \n",
      "9             0.453097            0.453258          0.454878         0.001731  \n",
      "10            1.000000            1.000000          1.000000         0.000000  \n",
      "11            1.000000            1.000000          1.000000         0.000000  \n",
      "12            0.980028            0.979682          0.977965         0.001897  \n",
      "13            0.703004            0.684559          0.692146         0.010307  \n",
      "14            0.455900            0.453258          0.458757         0.005983  \n",
      "15            1.000000            1.000000          1.000000         0.000000  \n",
      "16            1.000000            1.000000          1.000000         0.000000  \n",
      "17            1.000000            1.000000          1.000000         0.000000  \n",
      "18            0.835039            0.835812          0.837632         0.003786  \n",
      "19            0.665165            0.664767          0.656507         0.013316  \n",
      "20            1.000000            1.000000          1.000000         0.000000  \n",
      "21            1.000000            1.000000          1.000000         0.000000  \n",
      "22            1.000000            1.000000          1.000000         0.000000  \n",
      "23            0.970008            0.969420          0.966313         0.003409  \n",
      "24            0.730980            0.712027          0.716871         0.011087  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test_pred: [0 0 0 ... 0 0 0]\n",
      "test score: 0.7172118491635229\n",
      "svc\n",
      "y_test shape: (1676,)\n",
      "0.13786008230452676\n",
      "(6702, 109) (6702,)\n",
      "Fitting 4 folds for each of 25 candidates, totalling 100 fits\n",
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_svc__C  \\\n",
      "0        1.936465      0.494312         2.136580        0.288893          0.1   \n",
      "1        1.875818      0.479496         2.130255        0.296512          0.1   \n",
      "2        0.818246      0.240943         0.873629        0.149800          0.1   \n",
      "3        0.679085      0.182676         0.750487        0.125274          0.1   \n",
      "4        0.655136      0.154620         0.746303        0.143891          0.1   \n",
      "5        1.873640      0.487259         2.118399        0.291392            1   \n",
      "6        1.781430      0.463827         2.065198        0.290221            1   \n",
      "7        0.752540      0.167502         0.858273        0.166586            1   \n",
      "8        0.694758      0.179027         0.743238        0.130273            1   \n",
      "9        0.674363      0.156000         0.742191        0.122482            1   \n",
      "10       2.176183      0.554714         2.223078        0.295523           10   \n",
      "11       2.105208      0.554023         2.065162        0.268866           10   \n",
      "12       0.943058      0.237521         0.865000        0.132588           10   \n",
      "13       0.697055      0.172358         0.720117        0.115777           10   \n",
      "14       0.681390      0.165589         0.803662        0.130222           10   \n",
      "15       2.273020      0.551467         2.246772        0.297906          100   \n",
      "16       2.085747      0.538900         2.244845        0.302994          100   \n",
      "17       1.017131      0.232707         0.902484        0.137128          100   \n",
      "18       0.958313      0.215209         0.796122        0.162314          100   \n",
      "19       0.833457      0.190761         0.770970        0.150546          100   \n",
      "20       2.367057      0.604202         2.177070        0.272418         1000   \n",
      "21       2.116670      0.548369         2.130336        0.257875         1000   \n",
      "22       1.060416      0.235620         0.929262        0.213167         1000   \n",
      "23       2.132517      0.478715         0.732932        0.075823         1000   \n",
      "24       1.149892      0.281333         0.647494        0.123365         1000   \n",
      "\n",
      "   param_svc__gamma param_svc__kernel  \\\n",
      "0                 1               rbf   \n",
      "1               0.1               rbf   \n",
      "2              0.01               rbf   \n",
      "3             0.001               rbf   \n",
      "4            0.0001               rbf   \n",
      "5                 1               rbf   \n",
      "6               0.1               rbf   \n",
      "7              0.01               rbf   \n",
      "8             0.001               rbf   \n",
      "9            0.0001               rbf   \n",
      "10                1               rbf   \n",
      "11              0.1               rbf   \n",
      "12             0.01               rbf   \n",
      "13            0.001               rbf   \n",
      "14           0.0001               rbf   \n",
      "15                1               rbf   \n",
      "16              0.1               rbf   \n",
      "17             0.01               rbf   \n",
      "18            0.001               rbf   \n",
      "19           0.0001               rbf   \n",
      "20                1               rbf   \n",
      "21              0.1               rbf   \n",
      "22             0.01               rbf   \n",
      "23            0.001               rbf   \n",
      "24           0.0001               rbf   \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "0   {'svc__C': 0.1, 'svc__gamma': 1, 'svc__kernel'...           0.458832   \n",
      "1   {'svc__C': 0.1, 'svc__gamma': 0.1, 'svc__kerne...           0.458832   \n",
      "2   {'svc__C': 0.1, 'svc__gamma': 0.01, 'svc__kern...           0.458832   \n",
      "3   {'svc__C': 0.1, 'svc__gamma': 0.001, 'svc__ker...           0.458832   \n",
      "4   {'svc__C': 0.1, 'svc__gamma': 0.0001, 'svc__ke...           0.458832   \n",
      "5   {'svc__C': 1, 'svc__gamma': 1, 'svc__kernel': ...           0.458832   \n",
      "6   {'svc__C': 1, 'svc__gamma': 0.1, 'svc__kernel'...           0.458832   \n",
      "7   {'svc__C': 1, 'svc__gamma': 0.01, 'svc__kernel...           0.630984   \n",
      "8   {'svc__C': 1, 'svc__gamma': 0.001, 'svc__kerne...           0.458091   \n",
      "9   {'svc__C': 1, 'svc__gamma': 0.0001, 'svc__kern...           0.458832   \n",
      "10  {'svc__C': 10, 'svc__gamma': 1, 'svc__kernel':...           0.458832   \n",
      "11  {'svc__C': 10, 'svc__gamma': 0.1, 'svc__kernel...           0.458832   \n",
      "12  {'svc__C': 10, 'svc__gamma': 0.01, 'svc__kerne...           0.674543   \n",
      "13  {'svc__C': 10, 'svc__gamma': 0.001, 'svc__kern...           0.674084   \n",
      "14  {'svc__C': 10, 'svc__gamma': 0.0001, 'svc__ker...           0.470658   \n",
      "15  {'svc__C': 100, 'svc__gamma': 1, 'svc__kernel'...           0.458832   \n",
      "16  {'svc__C': 100, 'svc__gamma': 0.1, 'svc__kerne...           0.458832   \n",
      "17  {'svc__C': 100, 'svc__gamma': 0.01, 'svc__kern...           0.665592   \n",
      "18  {'svc__C': 100, 'svc__gamma': 0.001, 'svc__ker...           0.697360   \n",
      "19  {'svc__C': 100, 'svc__gamma': 0.0001, 'svc__ke...           0.657666   \n",
      "20  {'svc__C': 1000, 'svc__gamma': 1, 'svc__kernel...           0.458832   \n",
      "21  {'svc__C': 1000, 'svc__gamma': 0.1, 'svc__kern...           0.458832   \n",
      "22  {'svc__C': 1000, 'svc__gamma': 0.01, 'svc__ker...           0.662591   \n",
      "23  {'svc__C': 1000, 'svc__gamma': 0.001, 'svc__ke...           0.677275   \n",
      "24  {'svc__C': 1000, 'svc__gamma': 0.0001, 'svc__k...           0.683588   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
      "0            0.455679           0.449350           0.458462         0.455581   \n",
      "1            0.455679           0.449350           0.458462         0.455581   \n",
      "2            0.455679           0.449350           0.458462         0.455581   \n",
      "3            0.455679           0.449350           0.458462         0.455581   \n",
      "4            0.455679           0.449350           0.458462         0.455581   \n",
      "5            0.455679           0.449350           0.458462         0.455581   \n",
      "6            0.455679           0.449350           0.458462         0.455581   \n",
      "7            0.580565           0.579803           0.549599         0.585238   \n",
      "8            0.455679           0.449350           0.458462         0.455395   \n",
      "9            0.455679           0.449350           0.458462         0.455581   \n",
      "10           0.455679           0.449350           0.458462         0.455581   \n",
      "11           0.455679           0.449350           0.458462         0.455581   \n",
      "12           0.654875           0.678611           0.664318         0.668087   \n",
      "13           0.633269           0.648608           0.603244         0.639801   \n",
      "14           0.455679           0.449350           0.460603         0.459072   \n",
      "15           0.455679           0.449350           0.458462         0.455581   \n",
      "16           0.455679           0.449350           0.458462         0.455581   \n",
      "17           0.653204           0.660756           0.659768         0.659830   \n",
      "18           0.659060           0.670725           0.662880         0.672506   \n",
      "19           0.605454           0.654465           0.621945         0.634882   \n",
      "20           0.455679           0.449350           0.458462         0.455581   \n",
      "21           0.455679           0.449350           0.458462         0.455581   \n",
      "22           0.654287           0.660756           0.663891         0.660381   \n",
      "23           0.648770           0.675621           0.660056         0.665430   \n",
      "24           0.661846           0.670384           0.649565         0.666346   \n",
      "\n",
      "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
      "0         0.003798               11            0.453497            0.454388   \n",
      "1         0.003798               11            0.453497            0.454388   \n",
      "2         0.003798               11            0.453497            0.454388   \n",
      "3         0.003798               11            0.453497            0.454388   \n",
      "4         0.003798               11            0.453497            0.454388   \n",
      "5         0.003798               11            1.000000            1.000000   \n",
      "6         0.003798               11            0.977924            0.976138   \n",
      "7         0.029216                9            0.768516            0.740879   \n",
      "8         0.003650               25            0.475356            0.459623   \n",
      "9         0.003798               11            0.453497            0.454388   \n",
      "10        0.003798               11            1.000000            1.000000   \n",
      "11        0.003798               11            1.000000            1.000000   \n",
      "12        0.009236                2            0.976062            0.978028   \n",
      "13        0.025651                7            0.725931            0.708603   \n",
      "14        0.007788               10            0.486658            0.457012   \n",
      "15        0.003798               11            1.000000            1.000000   \n",
      "16        0.003798               11            1.000000            1.000000   \n",
      "17        0.004415                6            1.000000            1.000000   \n",
      "18        0.014953                1            0.843384            0.841685   \n",
      "19        0.022000                8            0.687188            0.656777   \n",
      "20        0.003798               11            1.000000            1.000000   \n",
      "21        0.003798               11            1.000000            1.000000   \n",
      "22        0.003691                5            1.000000            1.000000   \n",
      "23        0.011732                4            0.966351            0.967603   \n",
      "24        0.012404                3            0.755074            0.729744   \n",
      "\n",
      "    split2_train_score  split3_train_score  mean_train_score  std_train_score  \n",
      "0             0.456457            0.451438          0.453945         0.001802  \n",
      "1             0.456457            0.451438          0.453945         0.001802  \n",
      "2             0.456457            0.451438          0.453945         0.001802  \n",
      "3             0.456457            0.451438          0.453945         0.001802  \n",
      "4             0.456457            0.451438          0.453945         0.001802  \n",
      "5             1.000000            1.000000          1.000000         0.000000  \n",
      "6             0.975515            0.971953          0.975383         0.002168  \n",
      "7             0.741317            0.727653          0.744591         0.014865  \n",
      "8             0.460266            0.458047          0.463323         0.006994  \n",
      "9             0.456457            0.451438          0.453945         0.001802  \n",
      "10            1.000000            1.000000          1.000000         0.000000  \n",
      "11            1.000000            1.000000          1.000000         0.000000  \n",
      "12            0.977420            0.972282          0.975948         0.002233  \n",
      "13            0.693075            0.709079          0.709172         0.011622  \n",
      "14            0.460266            0.462951          0.466722         0.011701  \n",
      "15            1.000000            1.000000          1.000000         0.000000  \n",
      "16            1.000000            1.000000          1.000000         0.000000  \n",
      "17            1.000000            1.000000          1.000000         0.000000  \n",
      "18            0.834088            0.845056          0.841053         0.004194  \n",
      "19            0.667225            0.678054          0.672311         0.011418  \n",
      "20            1.000000            1.000000          1.000000         0.000000  \n",
      "21            1.000000            1.000000          1.000000         0.000000  \n",
      "22            1.000000            1.000000          1.000000         0.000000  \n",
      "23            0.966887            0.969510          0.967588         0.001195  \n",
      "24            0.729890            0.737092          0.737950         0.010323  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test_pred: [0 0 0 ... 0 0 0]\n",
      "test score: 0.697528952356184\n",
      "the overall mean of all 10 models is: 0.7105579099154584 and the standard deviation is: 0.007848326538270615\n",
      "0.14070682604760826\n",
      "The model is on average 72.60797331623478 better than the baseline\n",
      "[0.7073765778656882, 0.7201058417050461, 0.7136984436932495, 0.701590091293123, 0.7084772657918827, 0.7129416628523337, 0.7229431701037328, 0.7037052443298218, 0.7172118491635229, 0.697528952356184]\n"
     ]
    }
   ],
   "source": [
    "#SVC\n",
    "from sklearn.svm import SVC\n",
    "# defining parameter range\n",
    "grid = {'svc__C': [0.1, 1, 10, 100, 1000], \n",
    "              'svc__gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'svc__kernel': ['rbf']} \n",
    "clf = SVC()\n",
    "svc_model_mean,svc_model_std, svc_test_scores_ls,svc_best_models,svc_avg_bl,svc_y_pred_ls,svc_y_act_ls,svc_X_train_ls,svc_X_test_ls=MLpipe_GroupShuffleSplit_F1(X,y,preprocessor, clf,grid)\n",
    "print(svc_test_scores_ls)\n",
    "\n",
    "idx_best_svc_mod=np.argmax(svc_test_scores_ls)\n",
    "best_svc_model=svc_best_models[idx_best_svc_mod]\n",
    "svc_X_train=svc_X_train_ls[idx_best_svc_mod]\n",
    "svc_X_test=svc_X_train_ls[idx_best_svc_mod]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f2fdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cm=confusion_matrix(svc_y_act_ls[idx_best_svc_mod],svc_y_pred_ls[idx_best_svc_mod])\n",
    "plt.figure(figsize = (10,7))\n",
    "ax = plt.axes()\n",
    "sns.heatmap(cm/np.sum(cm),fmt='.2%', annot=True, cmap='Blues',ax=ax)\n",
    "\n",
    "ax.set_title('SVC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2bab8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 6702 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5942bbc0e75741a497b999859c05434a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6702 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#global shap value\n",
    "# explainer = shap.KernelExplainer(best_svc_model.best_estimator_.__getitem__(1).predict,svc_X_train,features=svc_X_train.columns)\n",
    "# shap_values = explainer.shap_values(svc_X_test)\n",
    "# shap.force_plot(explainer.expected_value[0], shap_values[0], svc_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "247121e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Kernel in module shap.explainers._kernel:\n",
      "\n",
      "class Kernel(shap.explainers._explainer.Explainer)\n",
      " |  Kernel(model, data, link=<shap.utils._legacy.IdentityLink object at 0x171b7fb80>, **kwargs)\n",
      " |  \n",
      " |  Uses the Kernel SHAP method to explain the output of any function.\n",
      " |  \n",
      " |  Kernel SHAP is a method that uses a special weighted linear regression\n",
      " |  to compute the importance of each feature. The computed importance values\n",
      " |  are Shapley values from game theory and also coefficents from a local linear\n",
      " |  regression.\n",
      " |  \n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  model : function or iml.Model\n",
      " |      User supplied function that takes a matrix of samples (# samples x # features) and\n",
      " |      computes a the output of the model for those samples. The output can be a vector\n",
      " |      (# samples) or a matrix (# samples x # model outputs).\n",
      " |  \n",
      " |  data : numpy.array or pandas.DataFrame or shap.common.DenseData or any scipy.sparse matrix\n",
      " |      The background dataset to use for integrating out features. To determine the impact\n",
      " |      of a feature, that feature is set to \"missing\" and the change in the model output\n",
      " |      is observed. Since most models aren't designed to handle arbitrary missing data at test\n",
      " |      time, we simulate \"missing\" by replacing the feature with the values it takes in the\n",
      " |      background dataset. So if the background dataset is a simple sample of all zeros, then\n",
      " |      we would approximate a feature being missing by setting it to zero. For small problems\n",
      " |      this background dataset can be the whole training set, but for larger problems consider\n",
      " |      using a single reference value or using the kmeans function to summarize the dataset.\n",
      " |      Note: for sparse case we accept any sparse matrix but convert to lil format for\n",
      " |      performance.\n",
      " |  \n",
      " |  link : \"identity\" or \"logit\"\n",
      " |      A generalized linear model link to connect the feature importance values to the model\n",
      " |      output. Since the feature importance values, phi, sum up to the model output, it often makes\n",
      " |      sense to connect them to the output with a link function where link(output) = sum(phi).\n",
      " |      If the model output is a probability then the LogitLink link function makes the feature\n",
      " |      importance values have log-odds units.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  See :ref:`Kernel Explainer Examples <kernel_explainer_examples>`\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Kernel\n",
      " |      shap.explainers._explainer.Explainer\n",
      " |      shap._serializable.Serializable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, model, data, link=<shap.utils._legacy.IdentityLink object at 0x171b7fb80>, **kwargs)\n",
      " |      Build a new explainer for the passed model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      model : object or function\n",
      " |          User supplied function or model object that takes a dataset of samples and\n",
      " |          computes the output of the model for those samples.\n",
      " |      \n",
      " |      masker : function, numpy.array, pandas.DataFrame, tokenizer, None, or a list of these for each model input\n",
      " |          The function used to \"mask\" out hidden features of the form `masked_args = masker(*model_args, mask=mask)`.\n",
      " |          It takes input in the same form as the model, but for just a single sample with a binary\n",
      " |          mask, then returns an iterable of masked samples. These\n",
      " |          masked samples will then be evaluated using the model function and the outputs averaged.\n",
      " |          As a shortcut for the standard masking using by SHAP you can pass a background data matrix\n",
      " |          instead of a function and that matrix will be used for masking. Domain specific masking\n",
      " |          functions are available in shap such as shap.ImageMasker for images and shap.TokenMasker\n",
      " |          for text. In addition to determining how to replace hidden features, the masker can also\n",
      " |          constrain the rules of the cooperative game used to explain the model. For example\n",
      " |          shap.TabularMasker(data, hclustering=\"correlation\") will enforce a hierarchial clustering\n",
      " |          of coalitions for the game (in this special case the attributions are known as the Owen values).\n",
      " |      \n",
      " |      link : function\n",
      " |          The link function used to map between the output units of the model and the SHAP value units. By\n",
      " |          default it is shap.links.identity, but shap.links.logit can be useful so that expectations are\n",
      " |          computed in probability units while explanations remain in the (more naturally additive) log-odds\n",
      " |          units. For more details on how link functions work see any overview of link functions for generalized\n",
      " |          linear models.\n",
      " |      \n",
      " |      algorithm : \"auto\", \"permutation\", \"partition\", \"tree\", \"kernel\", \"sampling\", \"linear\", \"deep\", or \"gradient\"\n",
      " |          The algorithm used to estimate the Shapley values. There are many different algorithms that\n",
      " |          can be used to estimate the Shapley values (and the related value for constrained games), each\n",
      " |          of these algorithms have various tradeoffs and are preferrable in different situations. By\n",
      " |          default the \"auto\" options attempts to make the best choice given the passed model and masker,\n",
      " |          but this choice can always be overriden by passing the name of a specific algorithm. The type of\n",
      " |          algorithm used will determine what type of subclass object is returned by this constructor, and\n",
      " |          you can also build those subclasses directly if you prefer or need more fine grained control over\n",
      " |          their options.\n",
      " |      \n",
      " |      output_names : None or list of strings\n",
      " |          The names of the model outputs. For example if the model is an image classifier, then output_names would\n",
      " |          be the names of all the output classes. This parameter is optional. When output_names is None then\n",
      " |          the Explanation objects produced by this explainer will not have any output_names, which could effect\n",
      " |          downstream plots.\n",
      " |  \n",
      " |  addsample(self, x, m, w)\n",
      " |  \n",
      " |  allocate(self)\n",
      " |  \n",
      " |  explain(self, incoming_instance, **kwargs)\n",
      " |  \n",
      " |  run(self)\n",
      " |  \n",
      " |  shap_values(self, X, **kwargs)\n",
      " |      Estimate the SHAP values for a set of samples.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : numpy.array or pandas.DataFrame or any scipy.sparse matrix\n",
      " |          A matrix of samples (# samples x # features) on which to explain the model's output.\n",
      " |      \n",
      " |      nsamples : \"auto\" or int\n",
      " |          Number of times to re-evaluate the model when explaining each prediction. More samples\n",
      " |          lead to lower variance estimates of the SHAP values. The \"auto\" setting uses\n",
      " |          `nsamples = 2 * X.shape[1] + 2048`.\n",
      " |      \n",
      " |      l1_reg : \"num_features(int)\", \"auto\" (default for now, but deprecated), \"aic\", \"bic\", or float\n",
      " |          The l1 regularization to use for feature selection (the estimation procedure is based on\n",
      " |          a debiased lasso). The auto option currently uses \"aic\" when less that 20% of the possible sample\n",
      " |          space is enumerated, otherwise it uses no regularization. THE BEHAVIOR OF \"auto\" WILL CHANGE\n",
      " |          in a future version to be based on num_features instead of AIC.\n",
      " |          The \"aic\" and \"bic\" options use the AIC and BIC rules for regularization.\n",
      " |          Using \"num_features(int)\" selects a fix number of top features. Passing a float directly sets the\n",
      " |          \"alpha\" parameter of the sklearn.linear_model.Lasso model used for feature selection.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array or list\n",
      " |          For models with a single output this returns a matrix of SHAP values\n",
      " |          (# samples x # features). Each row sums to the difference between the model output for that\n",
      " |          sample and the expected value of the model output (which is stored as expected_value\n",
      " |          attribute of the explainer). For models with vector outputs this returns a list\n",
      " |          of such matrices, one for each output.\n",
      " |  \n",
      " |  solve(self, fraction_evaluated, dim)\n",
      " |  \n",
      " |  varying_groups(self, x)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  not_equal(i, j)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from shap.explainers._explainer.Explainer:\n",
      " |  \n",
      " |  __call__(self, *args, max_evals='auto', main_effects=False, error_bounds=False, batch_size='auto', outputs=None, silent=False, **kwargs)\n",
      " |      Explains the output of model(*args), where args is a list of parallel iteratable datasets.\n",
      " |      \n",
      " |      Note this default version could be an abstract method that is implemented by each algorithm-specific\n",
      " |      subclass of Explainer. Descriptions of each subclasses' __call__ arguments\n",
      " |      are available in their respective doc-strings.\n",
      " |  \n",
      " |  explain_row(self, *row_args, max_evals, main_effects, error_bounds, outputs, silent, **kwargs)\n",
      " |      Explains a single row and returns the tuple (row_values, row_expected_values, row_mask_shapes, main_effects).\n",
      " |      \n",
      " |      This is an abstract method meant to be implemented by each subclass.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      tuple\n",
      " |          A tuple of (row_values, row_expected_values, row_mask_shapes), where row_values is an array of the\n",
      " |          attribution values for each sample, row_expected_values is an array (or single value) representing\n",
      " |          the expected value of the model for each sample (which is the same for all samples unless there\n",
      " |          are fixed inputs present, like labels when explaining the loss), and row_mask_shapes is a list\n",
      " |          of all the input shapes (since the row_values is always flattened),\n",
      " |  \n",
      " |  save(self, out_file, model_saver='.save', masker_saver='.save')\n",
      " |      Write the explainer to the given file stream.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from shap.explainers._explainer.Explainer:\n",
      " |  \n",
      " |  load(in_file, model_loader=<bound method Model.load of <class 'shap.models._model.Model'>>, masker_loader=<bound method Serializable.load of <class 'shap.maskers._masker.Masker'>>, instantiate=True) from builtins.type\n",
      " |      Load an Explainer from the given file stream.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      in_file : The file stream to load objects from.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from shap.explainers._explainer.Explainer:\n",
      " |  \n",
      " |  supports_model_with_masker(model, masker)\n",
      " |      Determines if this explainer can handle the given model.\n",
      " |      \n",
      " |      This is an abstract static method meant to be implemented by each subclass.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from shap._serializable.Serializable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(shap.KernelExplainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f942ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "grid={\n",
    "    'kneighborsclassifier__leaf_size':[1,3,5,10,20,50],\n",
    "    'kneighborsclassifier__n_neighbors':[1,3,5,10,20],\n",
    "    'kneighborsclassifier__p':[1,2]\n",
    "    \n",
    "}\n",
    "clf= KNeighborsClassifier()\n",
    "knn_model_mean,knn_model_std, knn_test_scores_ls,knn_best_models,knn_avg_bl,knn_y_pred_ls,knn_y_act_ls,knn_X_train_ls,knn_X_test_ls=MLpipe_GroupShuffleSplit_F1(X,y,preprocessor, clf,grid)\n",
    "print(knn_test_scores_ls)\n",
    "\n",
    "\n",
    "idx_best_knn_mod=np.argmax(knn_test_scores_ls)\n",
    "best_knn_model=knn_best_models[idx_best_knn_mod]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4295a39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparison of all models mean and std f1 score with baseline \n",
    "model_ls=[\"XG_Boost\",\"Logistic_Regression\",\"SVC\",\"KNN\"]\n",
    "avg_ls=[]\n",
    "std_ls=[]\n",
    "bl_ls=[]\n",
    "avg_ls.extend([XGB_model_mean, lr_model_mean, svc_model_mean,knn_model_mean])\n",
    "std_ls.extend([XGB_model_std, lr_model_std, svc_model_std,knn_model_std])\n",
    "bl_ls.extend([XGB_avg_bl, lr_avg_bl, svc_avg_bl, knn_avg_bl])\n",
    "\n",
    "plt.errorbar(model_ls, avg_ls, std_ls, linestyle='None', marker='^')\n",
    "plt.plot(model_ls, bl_ls, color='black',marker='o')\n",
    "plt.ylabel('F1 Macro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ff4034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2448ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
